{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Download and Import Repository"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d171592340e28a7c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in e:\\python\\textmining\\bert\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in e:\\python\\textmining\\bert\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\python\\textmining\\bert\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\python\\textmining\\bert\\lib\\site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\python\\textmining\\bert\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\python\\textmining\\bert\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\python\\textmining\\bert\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\python\\textmining\\bert\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\python\\textmining\\bert\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\python\\textmining\\bert\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\python\\textmining\\bert\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\python\\textmining\\bert\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\python\\textmining\\bert\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\python\\textmining\\bert\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\python\\textmining\\bert\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in e:\\python\\textmining\\bert\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\python\\textmining\\bert\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\python\\textmining\\bert\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\python\\textmining\\bert\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in e:\\python\\textmining\\bert\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'relation'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'relation'],\n",
      "        num_rows: 2717\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T10:40:10.833857200Z",
     "start_time": "2025-02-23T10:40:06.089826700Z"
    }
   },
   "id": "3527dae28bc4ec15"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T10:40:13.954884100Z",
     "start_time": "2025-02-23T10:40:10.832857300Z"
    }
   },
   "id": "a5ec58b468e192cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the SemEval-2010 Task 8 dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52200f43d9afac99"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  relation\n",
      "0  The system as described above has its greatest...         3\n",
      "1  The <e1>child</e1> was carefully wrapped and b...        18\n",
      "2  The <e1>author</e1> of a keygen uses a <e2>dis...        11\n",
      "3  A misty <e1>ridge</e1> uprises from the <e2>su...        18\n",
      "4  The <e1>student</e1> <e2>association</e2> is t...        12\n",
      "Index(['sentence', 'relation'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "# 转换成 Pandas DataFrame\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()\n",
    "\n",
    "print(df_train.head())  # 确保数据加载正确\n",
    "print(df_train.columns)  # 查看列名"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T10:40:17.280156200Z",
     "start_time": "2025-02-23T10:40:15.326702600Z"
    }
   },
   "id": "9464017764b5b78e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据预处理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5a15970941c8551"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1) 处理关系标签"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db7fa86b509aa0d0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"Cause-Effect\", \"Instrument-Agency\", \"Product-Producer\",\n",
    "    \"Content-Container\", \"Entity-Origin\", \"Entity-Destination\",\n",
    "    \"Component-Whole\", \"Member-Collection\", \"Message-Topic\", \"Other\"\n",
    "]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T10:40:18.611004400Z",
     "start_time": "2025-02-23T10:40:18.604496200Z"
    }
   },
   "id": "62a921c28a5125cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (2) 处理句子"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ede2fb0edc468b66"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    sentence = sentence.replace(\"<e1>\", \"[E1]\").replace(\"</e1>\", \"[/E1]\")\n",
    "    sentence = sentence.replace(\"<e2>\", \"[E2]\").replace(\"</e2>\", \"[/E2]\")\n",
    "    return sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T10:40:19.667953700Z",
     "start_time": "2025-02-23T10:40:19.662044700Z"
    }
   },
   "id": "aedaca9d1ccda4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (3) 生成数据集格式"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3750cc1b3825b939"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique train label indices: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}\n",
      "Unique test label indices: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}\n"
     ]
    }
   ],
   "source": [
    "class REDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 计算实体 <e1> 和 <e2> 在原始文本中的索引\n",
    "        e1_start = text.find(\"[E1]\")\n",
    "        e1_end = text.find(\"[/E1]\") + 4  # 包含 \"[/E1]\"\n",
    "        e2_start = text.find(\"[E2]\")\n",
    "        e2_end = text.find(\"[/E2]\") + 4  # 包含 \"[/E2]\"\n",
    "\n",
    "        # 移除标记，使得输入 BERT 的文本不会包含 \"[E1]\" 和 \"[E2]\"\n",
    "        clean_text = text.replace(\"[E1]\", \"\").replace(\"[/E1]\", \"\").replace(\"[E2]\", \"\").replace(\"[/E2]\", \"\")\n",
    "\n",
    "        # Tokenizer 编码\n",
    "        encoding = self.tokenizer(\n",
    "            clean_text, padding=\"max_length\", truncation=True,\n",
    "            max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        item[\"e1_pos\"] = torch.tensor([e1_start, e1_end], dtype=torch.long)\n",
    "        item[\"e2_pos\"] = torch.tensor([e2_start, e2_end], dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "# 预处理数据\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_texts = [preprocess_text(sent) for sent in df_train[\"sentence\"]]\n",
    "test_texts = [preprocess_text(sent) for sent in df_test[\"sentence\"]]\n",
    "\n",
    "# # 确保标签是数字\n",
    "# if df_train[\"relation\"].dtype in [int, np.int32, np.int64]:\n",
    "#     train_labels = df_train[\"relation\"].tolist()\n",
    "#     test_labels = df_test[\"relation\"].tolist()\n",
    "# else:\n",
    "#     train_labels = [label2id.get(label, 0) for label in df_train[\"relation\"]]\n",
    "#     test_labels = [label2id.get(label, 0) for label in df_test[\"relation\"]]\n",
    "\n",
    "# 确保标签格式正确\n",
    "if df_train[\"relation\"].dtype in [int, np.int32, np.int64]:\n",
    "    train_labels = df_train[\"relation\"].tolist()\n",
    "    test_labels = df_test[\"relation\"].tolist()\n",
    "else:\n",
    "    train_labels = [label2id.get(label, 0) for label in df_train[\"relation\"]]\n",
    "    test_labels = [label2id.get(label, 0) for label in df_test[\"relation\"]]\n",
    "\n",
    "# 打印标签检查\n",
    "print(\"Unique train label indices:\", set(train_labels))\n",
    "print(\"Unique test label indices:\", set(test_labels))\n",
    "\n",
    "# 确保所有标签在范围内\n",
    "train_labels = [min(len(label_list) - 1, label) for label in train_labels]\n",
    "test_labels = [min(len(label_list) - 1, label) for label in test_labels]\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = REDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = REDataset(test_texts, test_labels, tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T10:40:20.675796200Z",
     "start_time": "2025-02-23T10:40:20.542392700Z"
    }
   },
   "id": "722e032d172bb64f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 加载BERT进行训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5eb1b29f141330ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用BERT进行关系分类"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c92df7dfe800a8"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=len(label_list)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T10:40:22.301298200Z",
     "start_time": "2025-02-23T10:40:22.072685700Z"
    }
   },
   "id": "81ec448357ddc27e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 训练参数设置"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3b52b35e72250d1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Count: 1\n",
      "Current GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Count:\", torch.cuda.device_count())\n",
    "print(\"Current GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T10:40:23.127659400Z",
     "start_time": "2025-02-23T10:40:23.105379300Z"
    }
   },
   "id": "f7df73929f09d67"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# RTX 4090 高性能优化设置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,  # RTX 4090 显存足够，可提高 batch_size\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_strategy=\"steps\",  # ✅ 确保 logging 按步数触发\n",
    "    logging_steps=10,  # ✅ 让日志更频繁输出，方便观察\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # ✅ 开启自动混合精度，加快训练\n",
    "    gradient_accumulation_steps=1,  # ✅ RTX 4090 显存足够，设为 1\n",
    "    report_to=\"none\",  # ✅ 不使用 TensorBoard\n",
    "    dataloader_num_workers=8,  # ✅ 增加数据加载线程，加速训练\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"adamw_torch\",  # ✅ 采用更稳定的 AdamW 优化器\n",
    "    lr_scheduler_type=\"linear\",  # ✅ 改用线性学习率衰减（可能更稳定）\n",
    "    warmup_ratio=0.06,  # ✅ 适当增加 warmup，提升稳定性\n",
    "    disable_tqdm=False,  # ✅ 确保 Jupyter Notebook 可以显示 tqdm 进度条\n",
    "    log_level=\"info\"  # ✅ 让 Trainer 输出更多日志信息\n",
    ")\n",
    "\n",
    "# 计算评估指标\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)  \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")  \n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-23T10:40:23.815905400Z",
     "start_time": "2025-02-23T10:40:23.543877500Z"
    }
   },
   "id": "b2b7149f6d7312b3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "***** Running training *****\n",
      "  Num examples = 8,000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 625\n",
      "  Number of trainable parameters = 109,489,930\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# progress_bar = tqdm(total=training_args.num_train_epochs, desc=\"Training Progress\", unit=\"epoch\")\n",
    "# for epoch in range(training_args.num_train_epochs):\n",
    "#     trainer.train()  # 运行 Trainer 训练\n",
    "#     progress_bar.update(1)  # 更新进度条\n",
    "# progress_bar.close()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 获取训练历史\n",
    "history = trainer.state.log_history\n",
    "\n",
    "# 转换成 DataFrame，方便分析\n",
    "df = pd.DataFrame(history)\n",
    "\n",
    "# 打印日志，查看所有记录\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-23T10:40:24.084928400Z"
    }
   },
   "id": "e12c214f5ac610a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练和评估"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d9ab9c0c8276317"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1) 训练模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "615a199c62b7d79d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-23T10:15:55.060351200Z"
    }
   },
   "id": "cd91c81c8633c069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (2) 在测试集上评估"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57b7adb0441f59bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79ca6f4a44f36939"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 运行推理（Inference）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d045c8da27058f58"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 对新句子进行预测"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e04ff5be0162ebe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_relation(sentence):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(preprocess_text(sentence), return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    pred_label = torch.argmax(logits, dim=-1).item()\n",
    "    return id2label[pred_label]\n",
    "\n",
    "test_sentence = \"The <e1>storm</e1> caused severe <e2>flooding</e2> in the city.\"\n",
    "predicted_relation = predict_relation(test_sentence)\n",
    "print(\"Predicted Relation:\", predicted_relation)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d30ed62cb0a9bd91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 保存和加载模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a1983ee5e141184"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1) 保存模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a84a3e51f65619ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./bert-relation-extraction\")\n",
    "tokenizer.save_pretrained(\"./bert-relation-extraction\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3accc7739fb8722"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (2) 加载模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18814047a9e8cf03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"./bert-relation-extraction\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert-relation-extraction\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e64a399dd10baddf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
