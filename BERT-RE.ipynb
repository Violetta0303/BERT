{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install dependencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16b3dcf3bccb330a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning\n",
    "!pip install neptune"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T10:24:01.623621200Z",
     "start_time": "2025-02-26T10:24:01.622942200Z"
    }
   },
   "id": "7bc8669afcfb9674"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import needed modules"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:04:06.658901700Z",
     "start_time": "2025-02-24T11:04:06.461758500Z"
    }
   },
   "id": "3241abfa0a66c9a4"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import OrderedDict\n",
    "from random import randint\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib.figure import Figure\n",
    "from pandas import DataFrame\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import column_or_1d\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    GenerationMixin, TFGenerationMixin\n",
    ")\n",
    "\n",
    "# 忽略 transformers 可能的 FutureWarning\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:08:53.013147900Z",
     "start_time": "2025-02-26T14:08:48.005823200Z"
    }
   },
   "id": "c17dd9f74259362c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define constants"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:04:09.819895700Z",
     "start_time": "2025-02-24T11:04:08.157419300Z"
    }
   },
   "id": "4f381dab5ddad5f4"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2025\n"
     ]
    }
   ],
   "source": [
    "# --- Random seed ---\n",
    "SEED = 2025\n",
    "seed_everything(SEED)\n",
    "\n",
    "# --- Directory ---\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, \"data\\\\processed\") \n",
    "METADATA_FILE_NAME = os.path.join(PROCESSED_DATA_DIR, \"metadata.json\")\n",
    "CHECKPOINT_DIR = os.path.join(ROOT_DIR, \"checkpoint\")\n",
    "\n",
    "KAGGLE_ENV = bool(os.getenv(\"KAGGLE_URL_BASE\"))\n",
    "if KAGGLE_ENV:\n",
    "    # in Kaggle environment\n",
    "    # 2 datasets should already been added to the notebook\n",
    "    RAW_DATA_DIR = os.path.join(ROOT_DIR, \"..\\\\input\")\n",
    "else:\n",
    "    # in local environment\n",
    "    RAW_DATA_DIR =  os.path.join(ROOT_DIR, \"data\\\\raw\")\n",
    "\n",
    "# --- Datasets ---\n",
    "DATASET_MAPPING = {\n",
    "    \"SemEval2010Task8\": {\n",
    "        \"dir\": os.path.join(RAW_DATA_DIR,\"semeval2010-task-8\"),\n",
    "        \"keep_test_order\": True,\n",
    "        \"precision_recall_curve_baseline_img\": None,\n",
    "    }\n",
    "}\n",
    "\n",
    "# change this variable to switch dataset in later tasks\n",
    "# DATASET_NAME = list(DATASET_MAPPING.keys())[0]\n",
    "DATASET_NAME = \"SemEval2010Task8\"\n",
    "\n",
    "# --- Subject & object markup ---\n",
    "SUB_START_CHAR = \"[\"\n",
    "SUB_END_CHAR = \"]\"\n",
    "OBJ_START_CHAR = \"{\"\n",
    "OBJ_END_CHAR = \"}\"\n",
    "\n",
    "# --- BERT variants ---\n",
    "# See https://huggingface.co/transformers/pretrained_models.html for the full list\n",
    "AVAILABLE_PRETRAINED_MODELS = [\n",
    "    \"distilbert-base-uncased\", # 0\n",
    "    \"distilbert-base-cased\",   # 1\n",
    "    \"bert-base-uncased\",       # 2\n",
    "    \"distilgpt2\",              # 3\n",
    "    \"gpt2\",                    # 4\n",
    "    \"distilroberta-base\",      # 5\n",
    "    \"roberta-base\",            # 6\n",
    "    \"albert-base-v1\",          # 7\n",
    "    \"albert-base-v2\",          # 8\n",
    "    \"bert-large-uncased\",      # 9\n",
    "]\n",
    "\n",
    "# change this variable to switch pretrained language model\n",
    "PRETRAINED_MODEL = AVAILABLE_PRETRAINED_MODELS[2]\n",
    "\n",
    "# if e1 is not related to e2, should \"e2 not related to e1\" be added to the training set\n",
    "ADD_REVERSE_RELATIONSHIP = True\n",
    "\n",
    "# --- Neptune logger ---\n",
    "# Create a free account at https://neptune.ai/,\n",
    "# then get the API token and create a project\n",
    "NEPTUNE_API_TOKEN = \" eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ== \"\n",
    "NEPTUNE_PROJECT_NAME = \" Violetta/BERT \""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:08:54.406374500Z",
     "start_time": "2025-02-26T14:08:54.398857300Z"
    }
   },
   "id": "42f341d5b622696e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:04:10.609278Z",
     "start_time": "2025-02-24T10:04:09.081836400Z"
    }
   },
   "id": "d12e7b5f952b50f5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class OrdinalLabelEncoder:\n",
    "    def __init__(self, init_labels=None):\n",
    "        if init_labels is None:\n",
    "            init_labels = []\n",
    "        self.mapping = OrderedDict({l: i for i, l in enumerate(init_labels)})\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return list(self.mapping.keys())\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        return self.fit(y).transform(y)\n",
    "\n",
    "    def fit(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        new_classes = pd.Series(y).unique()\n",
    "        for cls in new_classes:\n",
    "            if cls not in self.mapping:\n",
    "                self.mapping[cls] = len(self.mapping)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        return [self.mapping[value] for value in y]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:08:55.881125400Z",
     "start_time": "2025-02-26T14:08:55.849613800Z"
    }
   },
   "id": "6eeb36b2a27f25ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Abstract preprocessor class:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:26:53.166431700Z",
     "start_time": "2025-02-24T09:26:53.123829900Z"
    }
   },
   "id": "7f6fe7cf363af3b0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class AbstractPreprocessor(ABC):\n",
    "    DATASET_NAME = \"\"\n",
    "    VAL_DATA_PROPORTION = 0.2\n",
    "    NO_RELATION_LABEL = \"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.SUB_START_ID, self.SUB_END_ID, self.OBJ_START_ID, self.OBJ_END_ID \\\n",
    "            = tokenizer.convert_tokens_to_ids([SUB_START_CHAR, SUB_END_CHAR, OBJ_START_CHAR, OBJ_END_CHAR])\n",
    "        self.label_encoder = OrdinalLabelEncoder([self.NO_RELATION_LABEL])\n",
    "\n",
    "    def preprocess_data(self, reprocess: bool):\n",
    "        print(f\"\\n---> Preprocessing {self.DATASET_NAME} dataset <---\")\n",
    "        \n",
    "        # create processed data dir\n",
    "        if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "            print(\"Creating processed data directory \" + PROCESSED_DATA_DIR)\n",
    "            os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "        # stop preprocessing if file existed\n",
    "        json_file_names = [self.get_dataset_file_name(k) for k in (\"train\", \"val\", \"test\")]\n",
    "        existed_files = [fn for fn in json_file_names if os.path.exists(fn)]\n",
    "        if existed_files:\n",
    "            file_text = \"- \" + \"\\n- \".join(existed_files)\n",
    "            if not reprocess:\n",
    "                print(\"The following files already exist:\")\n",
    "                print(file_text)\n",
    "                print(\"Preprocessing is skipped. See option --reprocess.\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"The following files will be overwritten:\")\n",
    "                print(file_text)\n",
    "\n",
    "        train_data, val_data, test_data = self._preprocess_data()\n",
    "\n",
    "        print(\"Saving to json files\")\n",
    "        self._write_data_to_file(train_data, \"train\")\n",
    "        self._write_data_to_file(val_data, \"val\")\n",
    "        self._write_data_to_file(test_data, \"test\")\n",
    "\n",
    "        self._save_metadata({\n",
    "            \"train_size\": len(train_data),\n",
    "            \"val_size\": len(val_data),\n",
    "            \"test_size\": len(test_data),\n",
    "            \"no_relation_label\": self.NO_RELATION_LABEL,\n",
    "            **self._get_label_mapping()\n",
    "        })\n",
    "\n",
    "        self._create_secondary_data_files()\n",
    "\n",
    "        print(\"---> Done ! <---\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def _preprocess_data(self) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "        pass\n",
    "\n",
    "    def _create_secondary_data_files(self):\n",
    "        \"\"\"\n",
    "        From the primary data file, create a data file with binary labels\n",
    "        and a data file with only sentences classified as \"related\"\n",
    "        \"\"\"\n",
    "\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            root_metadata = json.load(f)\n",
    "            metadata = root_metadata[self.DATASET_NAME]\n",
    "\n",
    "        related_only_count = {\n",
    "            \"train\": 0,\n",
    "            \"val\": 0,\n",
    "            \"test\": 0,\n",
    "        }\n",
    "\n",
    "        for key in [\"train\", \"test\", \"val\"]:\n",
    "            print(f\"Creating secondary files for {key} data\")\n",
    "\n",
    "            origin_file = open(self.get_dataset_file_name(key))\n",
    "            bin_file = open(self.get_dataset_file_name(f\"{key}_binary\"), \"w\")\n",
    "            related_file = open(self.get_dataset_file_name(f\"{key}_related_only\"), \"w\")\n",
    "\n",
    "            total = metadata[f\"{key}_size\"]\n",
    "\n",
    "            for line in tqdm(origin_file, total=total):\n",
    "                data = json.loads(line)\n",
    "                if data[\"label\"] != 0:\n",
    "                    related_only_count[key] += 1\n",
    "                    data[\"label\"] -= 1 # label in \"related_only\" files is 1 less than the original label\n",
    "                    related_file.write(json.dumps(data) + \"\\n\")\n",
    "                    data[\"label\"] = 1 # in binary dataset, all \"related\" classes have label 1\n",
    "                    bin_file.write(json.dumps(data) + \"\\n\")\n",
    "                else:\n",
    "                    bin_file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "            origin_file.close()\n",
    "            bin_file.close()\n",
    "            related_file.close()\n",
    "\n",
    "        print(\"Updating metadata.json\")\n",
    "        for key in [\"train\", \"test\", \"val\"]:\n",
    "            metadata[f\"{key}_related_only_size\"] = related_only_count[key]\n",
    "        root_metadata[self.DATASET_NAME] = metadata\n",
    "        with open(METADATA_FILE_NAME, \"w\") as f:\n",
    "            json.dump(root_metadata, f, indent=4)\n",
    "\n",
    "    def _find_sub_obj_pos(self, input_ids_list: Iterable) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Find subject and object position in a sentence\n",
    "        \"\"\"\n",
    "        sub_start_pos = [self._index(s, self.SUB_START_ID) + 1 for s in input_ids_list]\n",
    "        sub_end_pos = [self._index(s, self.SUB_END_ID, sub_start_pos[i]) for i, s in enumerate(input_ids_list)]\n",
    "        obj_start_pos = [self._index(s, self.OBJ_START_ID) + 1 for s in input_ids_list]\n",
    "        obj_end_pos = [self._index(s, self.OBJ_END_ID, obj_start_pos[i]) for i, s in enumerate(input_ids_list)]\n",
    "        return DataFrame({\n",
    "            \"sub_start_pos\": sub_start_pos,\n",
    "            \"sub_end_pos\": sub_end_pos,\n",
    "            \"obj_start_pos\": obj_start_pos,\n",
    "            \"obj_end_pos\": obj_end_pos,\n",
    "        })\n",
    "\n",
    "    @staticmethod\n",
    "    def _index(lst: list, ele: int, start: int = 0) -> int:\n",
    "        \"\"\"\n",
    "        Find an element in a list. Returns -1 if not found instead of raising an exception.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return lst.index(ele, start)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "    def _clean_data(self, raw_sentences: list, labels: list) -> DataFrame:\n",
    "        if not raw_sentences:\n",
    "            return DataFrame()\n",
    "\n",
    "        tokens = self.tokenizer(raw_sentences, truncation=True, padding=\"max_length\")\n",
    "        data = DataFrame(tokens.data)\n",
    "        data[\"label\"] = self.label_encoder.fit_transform(labels)\n",
    "        sub_obj_position = self._find_sub_obj_pos(data[\"input_ids\"])\n",
    "        data = pd.concat([data, sub_obj_position], axis=1)\n",
    "        data = self._remove_invalid_sentences(data)\n",
    "        return data\n",
    "\n",
    "    def _remove_invalid_sentences(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Remove sentences without subject/object or whose subject/object\n",
    "        is beyond the maximum length the model supports\n",
    "        \"\"\"\n",
    "        seq_max_len = self.tokenizer.model_max_length\n",
    "        return data.loc[\n",
    "            (data[\"sub_end_pos\"] < seq_max_len)\n",
    "            & (data[\"obj_end_pos\"] < seq_max_len)\n",
    "            & (data[\"sub_end_pos\"] > -1)\n",
    "            & (data[\"obj_end_pos\"] > -1)\n",
    "        ]\n",
    "\n",
    "    def _get_label_mapping(self):\n",
    "        \"\"\"\n",
    "        Returns a mapping from id to label and vise versa from the label encoder\n",
    "        \"\"\"\n",
    "        # all labels\n",
    "        id_to_label = dict(enumerate(self.label_encoder.classes_))\n",
    "        label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "\n",
    "        # for the related_only dataset\n",
    "        # ignore id 0, which represent no relation\n",
    "        id_to_label_related_only = {k - 1: v for k, v in id_to_label.items() if k != 0}\n",
    "        label_to_id_related_only = {v: k for k, v in id_to_label_related_only.items()}\n",
    "\n",
    "        return {\n",
    "            \"id_to_label\": id_to_label,\n",
    "            \"label_to_id\": label_to_id,\n",
    "            \"id_to_label_related_only\": id_to_label_related_only,\n",
    "            \"label_to_id_related_only\": label_to_id_related_only,            \n",
    "        }\n",
    "\n",
    "    def _write_data_to_file(self, dataframe: DataFrame, subset: str):\n",
    "        \"\"\"Write data in a dataframe to train/val/test file\"\"\"\n",
    "        lines = \"\"\n",
    "        for _, row in dataframe.iterrows():\n",
    "            lines += row.to_json() + \"\\n\"\n",
    "        with open(self.get_dataset_file_name(subset), \"w\") as file:\n",
    "            file.write(lines)\n",
    "\n",
    "    def _save_metadata(self, metadata: dict):\n",
    "        \"\"\"Save metadata to metadata.json\"\"\"\n",
    "        # create metadata file\n",
    "        if not os.path.exists(METADATA_FILE_NAME):\n",
    "            print(f\"Create metadata file at {METADATA_FILE_NAME}\")\n",
    "            with open(METADATA_FILE_NAME, \"w\") as f:\n",
    "                f.write(\"{}\\n\")\n",
    "\n",
    "        # add metadata\n",
    "        print(\"Saving metadata\")\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            root_metadata = json.load(f)\n",
    "        with open(METADATA_FILE_NAME, \"w\") as f:\n",
    "            root_metadata[self.DATASET_NAME] = metadata\n",
    "            json.dump(root_metadata, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def get_dataset_file_name(cls, key: str) -> str:\n",
    "        return os.path.join(PROCESSED_DATA_DIR, f\"{cls.DATASET_NAME.lower()}_{key}.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:08:57.118242200Z",
     "start_time": "2025-02-26T14:08:57.106596800Z"
    }
   },
   "id": "bfb46534afa50955"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Concrete preprocessor for dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:26:56.657440800Z",
     "start_time": "2025-02-24T09:26:56.586044800Z"
    }
   },
   "id": "73a689bb553ac1d2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class SemEval2010Task8Preprocessor(AbstractPreprocessor):\n",
    "    DATASET_NAME = \"SemEval2010Task8\"\n",
    "    NO_RELATION_LABEL = \"Other\"\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"],\n",
    "                                       \"SemEval2010_task8_training\\\\TRAIN_FILE.TXT\")\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"],\n",
    "                                      \"SemEval2010_task8_testing_keys\\\\TEST_FILE_FULL.TXT\")\n",
    "    RAW_TRAIN_DATA_SIZE = 8000\n",
    "    RAW_TEST_DATA_SIZE = 2717\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        print(\"Processing training data\")\n",
    "        train_data = self._process_file(\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            self.RAW_TRAIN_DATA_SIZE,\n",
    "            ADD_REVERSE_RELATIONSHIP,\n",
    "        )\n",
    "\n",
    "        print(\"Processing test data\")\n",
    "        test_data = self._process_file(\n",
    "            self.RAW_TEST_FILE_NAME,\n",
    "            self.RAW_TEST_DATA_SIZE,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        print(\"Splitting train & validate data\")\n",
    "        train_data, val_data = train_test_split(train_data, shuffle=True, random_state=SEED)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def _process_file(self, file_name: str, dataset_size: int, add_reverse: bool) -> DataFrame:\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "        with open(file_name) as f:\n",
    "            for _ in tqdm(range(dataset_size)):\n",
    "                sent = f.readline()\n",
    "                label, sub, obj = self._process_label(f.readline())\n",
    "                labels.append(label)\n",
    "                raw_sentences.append(self._process_sentence(sent, sub, obj))\n",
    "                if label == \"Other\" and add_reverse:\n",
    "                    labels.append(label)\n",
    "                    raw_sentences.append(self._process_sentence(sent, obj, sub))\n",
    "                f.readline()\n",
    "                f.readline()\n",
    "\n",
    "        return self._clean_data(raw_sentences, labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_sentence(sentence: str, sub: int, obj: int) -> str:\n",
    "        return sentence.split(\"\\t\")[1][1:-2] \\\n",
    "            .replace(f\"<e{sub}>\", SUB_START_CHAR) \\\n",
    "            .replace(f\"</e{sub}>\", SUB_END_CHAR) \\\n",
    "            .replace(f\"<e{obj}>\", OBJ_START_CHAR) \\\n",
    "            .replace(f\"</e{obj}>\", OBJ_END_CHAR)\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_label(label: str) -> Tuple[str, int, int]:\n",
    "        label = label.strip()\n",
    "        if label == \"Other\":\n",
    "            return label, 1, 2\n",
    "        nums = list(filter(str.isdigit, label))\n",
    "        return label, int(nums[0]), int(nums[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:08:58.137762600Z",
     "start_time": "2025-02-26T14:08:58.133563200Z"
    }
   },
   "id": "94deaa87ce0aef7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Factory method to create preprocessors:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:26:57.999809300Z",
     "start_time": "2025-02-24T09:26:57.977150500Z"
    }
   },
   "id": "eba2b6256a8f9a36"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_preprocessor_class(dataset_name: str = DATASET_NAME):\n",
    "    return globals()[f\"{dataset_name}Preprocessor\"]\n",
    "        \n",
    "def get_preprocessor(dataset_name: str = DATASET_NAME)-> AbstractPreprocessor:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, use_fast=True)\n",
    "    # some tokenizer, like GPTTokenizer, doesn't have pad_token\n",
    "    # in this case, we use eos token as pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    preprocessors_class = get_preprocessor_class(dataset_name)\n",
    "    return preprocessors_class(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:08:59.278302100Z",
     "start_time": "2025-02-26T14:08:59.272029400Z"
    }
   },
   "id": "ce835924d958bacb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess data:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:04:21.960411400Z",
     "start_time": "2025-02-24T10:04:19.222127700Z"
    }
   },
   "id": "ca251d3d25a747fc"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Preprocessing SemEval2010Task8 dataset <---\n",
      "The following files will be overwritten:\n",
      "- E:\\Python\\TextMining\\BERT\\data\\processed\\semeval2010task8_train.json\n",
      "- E:\\Python\\TextMining\\BERT\\data\\processed\\semeval2010task8_val.json\n",
      "- E:\\Python\\TextMining\\BERT\\data\\processed\\semeval2010task8_test.json\n",
      "Processing training data\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/8000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d6f29e4c5aa4c7bb132816bc9fae095"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2717 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d39adb5dd764d6bb00b2506c418c576"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train & validate data\n",
      "Saving to json files\n",
      "Saving metadata\n",
      "Creating secondary files for train data\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7057 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4d243dee33549e59c5a7e127dc2c663"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating secondary files for test data\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2717 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32531f333dd94bf7bd4303d1d4f4675b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating secondary files for val data\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2353 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c711705896c548838f81515c5eb80c78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating metadata.json\n",
      "---> Done ! <---\n"
     ]
    }
   ],
   "source": [
    "preprocessor = get_preprocessor()\n",
    "preprocessor.preprocess_data(reprocess=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:09:17.644746Z",
     "start_time": "2025-02-26T14:09:00.169218600Z"
    }
   },
   "id": "56ce550891c1c750"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:53:59.890270400Z",
     "start_time": "2025-02-24T09:53:59.883216400Z"
    }
   },
   "id": "d18e0242ced35b81"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class GenericDataset(IterableDataset):\n",
    "    \"\"\"A generic dataset for train/val/test data for both SemEval and GIDS dataset\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_name: str, subset: str, batch_size: int, label_transform: str):\n",
    "        assert subset in [\"train\", \"val\", \"test\"]\n",
    "        assert label_transform in [\"none\", \"binary\", \"related_only\"]\n",
    "\n",
    "        file_name = subset if label_transform == \"none\" \\\n",
    "            else f\"{subset}_{label_transform}\"\n",
    "\n",
    "        preprocessor_class = get_preprocessor_class()\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            metadata = json.load(f)[dataset_name]\n",
    "\n",
    "        size = metadata[f\"{subset}_related_only_size\"] \\\n",
    "            if label_transform == \"related_only\" \\\n",
    "            else metadata[f\"{subset}_size\"]\n",
    "\n",
    "        self.subset = subset\n",
    "        self.batch_size = batch_size\n",
    "        self.length = math.ceil(size / batch_size)\n",
    "        self.file = open(preprocessor_class.get_dataset_file_name(file_name))\n",
    "        # self.file_path = preprocessor_class.get_dataset_file_name(file_name)\n",
    "        # \n",
    "        # # 立即读取数据并关闭文件\n",
    "        # with open(self.file_path, \"r\") as f:\n",
    "        #     self.data = f.readlines()  # 直接读取所有数据，存入 self.data\n",
    "        # self.file = None  # 关闭文件后，确保 self.file 不是打开的文件对象\n",
    "        \n",
    "        self.keep_test_order = self.subset == \"test\" and DATASET_MAPPING[dataset_name][\"keep_test_order\"]\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.file:\n",
    "            self.file.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Implement \"smart batching\"\n",
    "        \"\"\"\n",
    "\n",
    "        data = [json.loads(line) for line in self.file]\n",
    "        if not self.keep_test_order:\n",
    "            data = sorted(data, key=lambda x: sum(x[\"attention_mask\"]))\n",
    "\n",
    "        new_data = []\n",
    "\n",
    "        while len(data) > 0:\n",
    "            if self.keep_test_order or len(data) < self.batch_size:\n",
    "                idx = 0\n",
    "            else:\n",
    "                idx = randint(0, len(data) - self.batch_size)\n",
    "            batch = data[idx:idx + self.batch_size]\n",
    "            max_len = max([sum(b[\"attention_mask\"]) for b in batch])\n",
    "\n",
    "            for b in batch:\n",
    "                input_data = {}\n",
    "                for k, v in b.items():\n",
    "                    if k != \"label\":\n",
    "                        if isinstance(v, list):\n",
    "                            input_data[k] = torch.tensor(v[:max_len])\n",
    "                        else:\n",
    "                            input_data[k] = torch.tensor(v)\n",
    "                label = torch.tensor(b[\"label\"])\n",
    "                new_data.append((input_data, label))\n",
    "\n",
    "            del data[idx:idx + self.batch_size]\n",
    "\n",
    "        yield from new_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def as_batches(self):\n",
    "        input_data = []\n",
    "        label = []\n",
    "        \n",
    "        def create_batch():\n",
    "            return (\n",
    "                {k: torch.stack([x[k] for x in input_data]).cuda() for k in input_data[0].keys()},\n",
    "                torch.tensor(label).cuda()\n",
    "            )\n",
    "        \n",
    "        for ip, l in self:\n",
    "            input_data.append(ip)\n",
    "            label.append(l)\n",
    "            if len(input_data) == self.batch_size:\n",
    "                yield create_batch()\n",
    "                input_data.clear()\n",
    "                label.clear()\n",
    "\n",
    "        yield create_batch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:09:17.651676400Z",
     "start_time": "2025-02-26T14:09:17.643746400Z"
    }
   },
   "id": "92cf4767f2dd435"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:27:00.452556800Z",
     "start_time": "2025-02-24T09:27:00.434418500Z"
    }
   },
   "id": "cc5d6f5c45e7049"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Iterable\n",
    "\n",
    "class BaseClassifier(LightningModule, ABC):\n",
    "    \"\"\"\n",
    "    Base class of all classifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_label_transform = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, logits: Tensor, label: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the loss of the model\n",
    "        It MUST take care of the last activation layer\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor) -> dict:\n",
    "        pass\n",
    "\n",
    "    def __init__(self, pretrained_language_model, dataset_name, batch_size, learning_rate, decay_lr_speed, dropout_p, activation_function, weight_decay, linear_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.test_proposed_answer = None\n",
    "\n",
    "        self.language_model = AutoModel.from_pretrained(pretrained_language_model)\n",
    "        config = self.language_model.config\n",
    "        self.max_seq_len = config.max_position_embeddings\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_size, linear_size)\n",
    "        self.linear_output = nn.Linear(linear_size, self.num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.activation_function = getattr(nn, activation_function)()\n",
    "        \n",
    "        # 添加用于存储 epoch 端 `outputs`\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "    def forward(self, sub_start_pos, sub_end_pos,\n",
    "                obj_start_pos, obj_end_pos, *args, **kwargs) -> Tensor:\n",
    "        language_model_output = self.language_model(*args, **kwargs)\n",
    "        if isinstance(language_model_output, tuple):\n",
    "            language_model_output = language_model_output[0]\n",
    "\n",
    "        # x = torch.mean(language_model_output, dim=1)\n",
    "        x = torch.mean(language_model_output.last_hidden_state, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.activation_function(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear_output(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader(\"train\")\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader(\"val\")\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader(\"test\")\n",
    "\n",
    "    def __get_dataloader(self, subset: str) -> DataLoader:\n",
    "        batch_size = self.hparams.batch_size\n",
    "        dataset = GenericDataset(\n",
    "            self.hparams.dataset_name,\n",
    "            subset, \n",
    "            batch_size, \n",
    "            self.dataset_label_transform\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            # num_workers=1\n",
    "            num_workers=0,\n",
    "            persistent_workers=False\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            [p for p in self.parameters() if p.requires_grad],\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = LambdaLR(optimizer, lambda epoch: self.hparams.decay_lr_speed[epoch])\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    # def training_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n",
    "    #     input_data, label = batch\n",
    "    #     logits = self(**input_data)\n",
    "    # \n",
    "    #     loss = self.loss_function(logits, label)\n",
    "    #     log = {\"train_loss\": loss}\n",
    "    # \n",
    "    #     return {\"loss\": loss, \"log\": log}\n",
    "    \n",
    "    def training_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n",
    "        input_data, label = batch\n",
    "        logits = self(**input_data)\n",
    "\n",
    "        loss = self.loss_function(logits, label)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits.detach(), \"label\": label.detach()}\n",
    "\n",
    "    def __eval_step(self, batch:  Tuple[dict, Tensor]) -> dict:\n",
    "        input_data, label = batch\n",
    "        logits = self(**input_data)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"label\": label,\n",
    "        }\n",
    "    \n",
    "    # def validation_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n",
    "    #     return self.__eval_step(batch)\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[dict, Tensor], batch_nb: int):\n",
    "        input_data, label = batch\n",
    "        logits = self(**input_data)\n",
    "\n",
    "        loss = self.loss_function(logits, label)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        output = {\"logits\": logits.detach(), \"label\": label.detach()}\n",
    "        self.validation_step_outputs.append(output)  # 存储输出\n",
    "        return output\n",
    "    \n",
    "    # def test_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n",
    "    #     return self.__eval_step(batch)\n",
    "    \n",
    "    def test_step(self, batch: Tuple[dict, Tensor], batch_nb: int):\n",
    "        input_data, label = batch\n",
    "        logits = self(**input_data)\n",
    "\n",
    "        loss = self.loss_function(logits, label)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        output = {\"logits\": logits.detach(), \"label\": label.detach()}\n",
    "        self.test_step_outputs.append(output)  # 存储输出\n",
    "        return output\n",
    "\n",
    "\n",
    "    # def __eval_epoch_end(self, epoch_type: str, outputs: Iterable[dict]) -> dict:\n",
    "    #     assert epoch_type in [\"test\", \"val\"]\n",
    "    #     \n",
    "    #     logits = torch.cat([x[\"logits\"] for x in outputs]).cpu()\n",
    "    #     label = torch.cat([x[\"label\"] for x in outputs]).cpu()\n",
    "    #     \n",
    "    #     logs = self.log_metrics(epoch_type, logits, label)\n",
    "    #     \n",
    "    #     return {\"progress_bar\": logs}\n",
    "    \n",
    "    def __eval_epoch_end(self, epoch_type: str, outputs: List[dict]):\n",
    "        assert epoch_type in [\"test\", \"val\"]\n",
    "\n",
    "        logits = torch.cat([x[\"logits\"] for x in outputs]).cpu()\n",
    "        label = torch.cat([x[\"label\"] for x in outputs]).cpu()\n",
    "\n",
    "        logs = self.log_metrics(epoch_type, logits, label)\n",
    "\n",
    "        # 记录日志\n",
    "        for key, value in logs.items():\n",
    "            self.log(f\"{epoch_type}_{key}\", value)\n",
    "\n",
    "    # def on_validation_epoch_end(self, outputs: Iterable[dict]) -> dict:\n",
    "    #     return self.__eval_epoch_end(\"val\", outputs)\n",
    "\n",
    "    # def on_test_epoch_end(self, outputs: Iterable[dict]) -> dict:\n",
    "    #     return self.__eval_epoch_end(\"test\", outputs)\n",
    "    \n",
    "    # def on_validation_epoch_end(self, outputs):  # 正确使用 Lightning 传入的 outputs\n",
    "    #     if not outputs:  \n",
    "    #         print(\"[WARNING] No validation outputs found. Skipping eval.\")\n",
    "    #         return  \n",
    "    #     self.__eval_epoch_end(\"val\", outputs)\n",
    "    # \n",
    "    # def on_test_epoch_end(self, outputs):  # 正确使用 Lightning 传入的 outputs\n",
    "    #     if not outputs:  \n",
    "    #         print(\"[WARNING] No test outputs found. Skipping eval.\")\n",
    "    #         return  \n",
    "    #     self.__eval_epoch_end(\"test\", outputs)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Lightning 自动调用，无需传入 outputs\"\"\"\n",
    "        print(f\"[DEBUG] validation_step_outputs before eval: {self.validation_step_outputs}\")\n",
    "\n",
    "        if not self.validation_step_outputs:\n",
    "            print(\"[WARNING] No validation outputs found. Skipping eval.\")\n",
    "            return\n",
    "\n",
    "        self.__eval_epoch_end(\"val\", self.validation_step_outputs)\n",
    "        self.validation_step_outputs.clear()  # 清空，避免影响下个 epoch\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"Lightning 自动调用，无需传入 outputs\"\"\"\n",
    "        print(f\"[DEBUG] test_step_outputs before eval: {self.test_step_outputs}\")\n",
    "\n",
    "        if not self.test_step_outputs:\n",
    "            print(\"[WARNING] No test outputs found. Skipping eval.\")\n",
    "            return\n",
    "\n",
    "        self.__eval_epoch_end(\"test\", self.test_step_outputs)\n",
    "        self.test_step_outputs.clear()  # 清空，避免影响下个 epoch\n",
    "\n",
    "\n",
    "    def numeric_labels_to_text(self, label):\n",
    "        \"\"\"Revert labels from number to text\"\"\"\n",
    "        if self.dataset_label_transform == \"binary\":\n",
    "            label = [\"Positive\" if x else \"Negative\" for x in label]\n",
    "        else:\n",
    "            with open(METADATA_FILE_NAME) as f:\n",
    "                meta = json.load(f)[self.hparams.dataset_name]\n",
    "            if self.dataset_label_transform == \"none\":\n",
    "                mapping = meta[\"id_to_label\"]\n",
    "            else:\n",
    "                mapping = meta[\"id_to_label_related_only\"]\n",
    "            label = [mapping[str(int(x))] for x in label]\n",
    "        return label\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(predicted_label, label) -> Figure:\n",
    "        result = confusion_matrix(label, predicted_label)\n",
    "        display = ConfusionMatrixDisplay(result)\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        display.plot(cmap=plt.cm.get_cmap(\"Blues\"), ax=ax, xticks_rotation='vertical')\n",
    "        return fig\n",
    "\n",
    "    def log_confusion_matrix(self, prefix: str, predicted_label: Tensor, label: Tensor):\n",
    "        predicted_label = self.numeric_labels_to_text(predicted_label)\n",
    "        label = self.numeric_labels_to_text(label)\n",
    "        fig = self.plot_confusion_matrix(predicted_label, label)\n",
    "        self.logger.log_image(f\"{prefix}_confusion_matrix\", fig)\n",
    "\n",
    "\n",
    "class MulticlassClassifier(BaseClassifier, ABC):\n",
    "    \"\"\"\n",
    "    Base class for multiclass classifiers\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(self, logits: Tensor, label: Tensor)-> Tensor:\n",
    "        return F.cross_entropy(logits, label)\n",
    "\n",
    "    @staticmethod\n",
    "    def logits_to_label(logits: Tensor) -> Tensor:\n",
    "        return torch.argmax(logits, dim=-1)\n",
    "\n",
    "    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor) -> dict:\n",
    "        predicted_label = self.logits_to_label(logits)\n",
    "        self.log_confusion_matrix(epoch_type, predicted_label, label)\n",
    "\n",
    "        logs = {\n",
    "            f\"{epoch_type}_avg_loss\": float(self.loss_function(logits, label)),\n",
    "            f\"{epoch_type}_acc\": accuracy_score(label, predicted_label),\n",
    "            f\"{epoch_type}_pre_weighted\": precision_score(label, predicted_label, average=\"weighted\"),\n",
    "            f\"{epoch_type}_rec_weighted\": recall_score(label, predicted_label, average=\"weighted\"),\n",
    "            f\"{epoch_type}_f1_weighted\": f1_score(label, predicted_label, average=\"weighted\"),\n",
    "            f\"{epoch_type}_pre_macro\": precision_score(label, predicted_label, average=\"macro\"),\n",
    "            f\"{epoch_type}_rec_macro\": recall_score(label, predicted_label, average=\"macro\"),\n",
    "            f\"{epoch_type}_f1_macro\": f1_score(label, predicted_label, average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.logger.log_metric(k, v)\n",
    "\n",
    "        return logs\n",
    "\n",
    "\n",
    "class StandardClassifier(MulticlassClassifier):\n",
    "    \"\"\"\n",
    "    A classifier that can recognize the \"not related\" as well as other relations\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_label_transform = \"none\"\n",
    "\n",
    "    def __init__(self, dataset_name, **kwargs):\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            self.num_classes = len(json.load(f)[dataset_name][\"label_to_id\"])\n",
    "        self.test_proposed_answer = None\n",
    "        super().__init__(dataset_name=dataset_name, **kwargs)\n",
    "\n",
    "\n",
    "    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor)-> dict:\n",
    "        if epoch_type == \"test\":\n",
    "            self.test_proposed_answer = self.logits_to_label(logits).tolist()\n",
    "        self.__log_precision_recall_curve(epoch_type, logits, label)\n",
    "        return super().log_metrics(epoch_type, logits, label)\n",
    "\n",
    "    def __log_precision_recall_curve(self, epoch_type: str, logits: Tensor, label: Tensor):\n",
    "        \"\"\"\n",
    "        Log the micro-averaged precision recall curve\n",
    "        Ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "        \"\"\"\n",
    "\n",
    "        label = torch.tensor(label_binarize(label, classes=list(range(self.num_classes)))).flatten()\n",
    "        logits = logits.flatten()\n",
    "\n",
    "        pre, rec, thresholds = precision_recall_curve(label, logits)\n",
    "        f1s = 2 * pre * rec / (pre + rec)\n",
    "        ix = np.argmax(f1s)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "        # render the baseline curves as background for comparison\n",
    "        background = DATASET_MAPPING[self.hparams.dataset_name][\"precision_recall_curve_baseline_img\"]\n",
    "        if background:\n",
    "            img = plt.imread(background)\n",
    "            ax.imshow(img, extent=[0, 1, 0, 1])\n",
    "\n",
    "        no_skill = len(label[label == 1]) / len(label)\n",
    "        ax.plot(rec, pre, label=\"Our proposed model\", color=\"blue\")\n",
    "        ax.set_xlabel(\"Recall\")\n",
    "        ax.set_ylabel(\"Precision\")\n",
    "        ax.legend()\n",
    "\n",
    "        self.logger.log_image(f\"{epoch_type}_pre_rec_curve\", fig)\n",
    "        self.logger.log_metric(\n",
    "            f\"{epoch_type}_average_precision_score_micro\",\n",
    "            average_precision_score(label, logits, average=\"micro\")\n",
    "        )\n",
    "\n",
    "\n",
    "class BinaryClassifier(BaseClassifier):\n",
    "    \"\"\"\n",
    "    A binary classifier that picks out \"not-related\" sentences\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_label_transform = \"binary\"\n",
    "    num_classes = 1\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.thresholds = {}\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return super().forward(*args, **kwargs).flatten()\n",
    "\n",
    "    @staticmethod\n",
    "    def yhat_to_label(y_hat: Tensor, threshold: float) -> Tensor:\n",
    "        return (y_hat > threshold).long()\n",
    "\n",
    "    def loss_function(self, logits: Tensor, label: Tensor) -> Tensor:\n",
    "        return F.binary_cross_entropy_with_logits(logits, label.float())\n",
    "\n",
    "    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor) -> dict:\n",
    "        y_hat = torch.sigmoid(logits)\n",
    "\n",
    "        if epoch_type == \"val\":\n",
    "            self.__find_thresholds(y_hat, label)\n",
    "\n",
    "        self.__log_output_distribution(epoch_type, y_hat, label)\n",
    "\n",
    "        logs = {\n",
    "            f\"{epoch_type}_avg_loss\": float(self.loss_function(logits, label)),\n",
    "            f\"{epoch_type}_roc_auc\": self.__roc_auc_score(label, y_hat),\n",
    "        }\n",
    "\n",
    "        for criteria, threshold in self.thresholds.items():\n",
    "            prefix = f\"{epoch_type}_{criteria}\"\n",
    "            predicted_label = self.yhat_to_label(y_hat, threshold)\n",
    "            self.log_confusion_matrix(prefix, predicted_label, label)\n",
    "\n",
    "            logs[f\"{prefix}_acc\"] = accuracy_score(label, predicted_label)\n",
    "            logs[f\"{prefix}_pre\"] = precision_score(label, predicted_label, average=\"binary\")\n",
    "            logs[f\"{prefix}_rec\"] = recall_score(label, predicted_label, average=\"binary\")\n",
    "            logs[f\"{prefix}_f1\"] = f1_score(label, predicted_label, average=\"binary\")\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.logger.log_metric(k, v)\n",
    "\n",
    "        return logs\n",
    "\n",
    "    @staticmethod\n",
    "    def __roc_auc_score(label: Tensor, y_hat: Tensor) -> float:\n",
    "        try:\n",
    "            return roc_auc_score(label, y_hat)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    def __find_thresholds(self, y_hat: Tensor, label: Tensor):\n",
    "        \"\"\"\n",
    "        Find 3 classification thresholds based on 3 criteria:\n",
    "        - The one that yields highest accuracy\n",
    "        - The \"best point\" in the ROC curve\n",
    "        - The one that yields highest f1\n",
    "        The results are logged and stored in self.threshold\n",
    "        \"\"\"\n",
    "        # best accuracy\n",
    "        best_acc = 0\n",
    "        best_acc_threshold = None\n",
    "        for y in y_hat:\n",
    "            y_predicted = self.yhat_to_label(y_hat, threshold=y)\n",
    "            acc = accuracy_score(label, y_predicted)\n",
    "            if best_acc < acc:\n",
    "                best_acc = acc\n",
    "                best_acc_threshold = y\n",
    "        self.thresholds[\"best_acc\"] = best_acc_threshold\n",
    "\n",
    "        # ROC curve\n",
    "        # https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "        fpr, tpr, thresholds = roc_curve(label, y_hat)\n",
    "        gmeans = tpr * (1 - fpr)\n",
    "        ix = np.argmax(gmeans)\n",
    "        self.thresholds[\"best_roc\"] = thresholds[ix]\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        ax.plot([0,1], [0,1], linestyle=\"--\", label=\"No Skill\")\n",
    "        ax.plot(fpr, tpr, marker=\".\", label=\"Logistic\")\n",
    "        ax.scatter(fpr[ix], tpr[ix], marker=\"o\", color=\"black\", label=\"Best\")\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.legend()\n",
    "        self.logger.log_image(\"roc_curve\", fig)\n",
    "\n",
    "        # precision recall curve\n",
    "        # https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "        pre, rec, thresholds = precision_recall_curve(label, y_hat)\n",
    "        f1s = 2 * pre * rec / (pre + rec)\n",
    "        ix = np.argmax(f1s)\n",
    "        self.thresholds[\"best_f1\"] = thresholds[ix]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        no_skill = len(label[label == 1]) / len(label)\n",
    "        ax.plot([0, 1], [no_skill, no_skill], linestyle=\"--\", label=\"No Skill\")\n",
    "        ax.plot(rec, pre, marker=\".\", label=\"Logistic\")\n",
    "        ax.scatter(rec[ix], pre[ix], marker=\"o\", color=\"black\", label=\"Best F1\")\n",
    "        ax.set_xlabel(\"Recall\")\n",
    "        ax.set_ylabel(\"Precision\")\n",
    "        ax.legend()\n",
    "        self.logger.log_image(\"pre_rec_curve\", fig)\n",
    "\n",
    "        # log thresholds\n",
    "        for k, v in self.thresholds.items():\n",
    "            self.logger.log_metric(f\"threshold_{k}\", v)\n",
    "\n",
    "    def __log_output_distribution(self, epoch_type: str, y_hat: Tensor, label: Tensor):\n",
    "        \"\"\"\n",
    "        Log the distribution of the model output and 3 thresholds with log scale and linear scale\n",
    "        \"\"\"\n",
    "        y_neg = y_hat[label == 0].numpy()\n",
    "        y_pos = y_hat[label == 1].numpy()\n",
    "\n",
    "        for scale in [\"linear\", \"log\"]:\n",
    "            fig, ax = plt.subplots(figsize=(16, 12))\n",
    "            ax.set_yscale(scale)\n",
    "            ax.hist([y_neg, y_pos], stacked=True, bins=50, label=[\"No relation\", \"Related\"])\n",
    "            ylim = ax.get_ylim()\n",
    "            for k, v in self.thresholds.items():\n",
    "                ax.plot([v, v], ylim, linestyle=\"--\", label=f\"{k} threshold\")\n",
    "            ax.legend()\n",
    "            self.logger.log_image(f\"{epoch_type}_distribution_{scale}_scale\", fig)\n",
    "\n",
    "\n",
    "class RelationClassifier(MulticlassClassifier):\n",
    "    \"\"\"\n",
    "    A classifier that recognizes relations except for \"not-related\"\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_label_transform = \"related_only\"\n",
    "\n",
    "    def __init__(self, dataset_name, **kwargs):\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            self.num_classes = len(json.load(f)[dataset_name][\"label_to_id_related_only\"])\n",
    "        super().__init__(dataset_name=dataset_name, **kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:09:38.338461200Z",
     "start_time": "2025-02-26T14:09:38.316868500Z"
    }
   },
   "id": "1cb2cd26ae69f8a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The official scorer\n",
    "Some datasets comes with official scorers. We will define them in this session."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:27:01.743643300Z",
     "start_time": "2025-02-24T09:27:01.708091400Z"
    }
   },
   "id": "2175ae924f195629"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class AbstractScorer(ABC):\n",
    "    def __init__(self, experiment_no: int, logger):\n",
    "        self.experiment_no = experiment_no\n",
    "        self.logger = logger\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, proposed_answer: dict):\n",
    "        pass\n",
    "\n",
    "class SemEval2010Task8Scorer(AbstractScorer):\n",
    "    RESULT_FILE = \"semeval2010_task8_official_score_{}_{}.txt\"\n",
    "    PROPOSED_ANSWER_FILE = \"semeval2010_task8_proposed_answer.txt\"\n",
    "    SCORER = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"], \"SemEval2010_task8_scorer-v1.2/semeval2010_task8_scorer-v1.2.pl\")\n",
    "    FORMAT_CHECKER = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"], \"SemEval2010_task8_scorer-v1.2/semeval2010_task8_format_checker.pl\")\n",
    "    ANSWER_KEY = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"], \"SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT\")\n",
    "\n",
    "    def score(self, proposed_answer: dict):\n",
    "        # write test_result to file\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            metadata = json.load(f)\n",
    "            id_to_label = {int(k): v for k, v in metadata[DATASET_NAME][\"id_to_label\"].items()}\n",
    "\n",
    "        for criteria, answer in proposed_answer.items():\n",
    "            result_file = self.RESULT_FILE.format(self.experiment_no, criteria)\n",
    "            i = 8001\n",
    "            with open(self.PROPOSED_ANSWER_FILE, \"w\") as f:\n",
    "                for r in answer:\n",
    "                    f.write(f\"{i}\\t{id_to_label[r]}\\n\")\n",
    "                    i += 1\n",
    "\n",
    "            # call the official scorer\n",
    "            os.system(f\"perl {self.FORMAT_CHECKER} {self.PROPOSED_ANSWER_FILE}\")\n",
    "            os.system(f\"perl {self.SCORER} {self.PROPOSED_ANSWER_FILE} {self.ANSWER_KEY} > {result_file}\")\n",
    "\n",
    "            # log the official score\n",
    "            with open(result_file) as f:\n",
    "                result = f.read()\n",
    "                print(f\">>> Classifier with criteria: {criteria} <<<\")\n",
    "                print(result)\n",
    "                print(\"\\n\\n\")\n",
    "            self.logger.log_artifact(result_file)\n",
    "\n",
    "def get_official_scorer(experiment_no: int, logger, dataset_name: str = DATASET_NAME) -> AbstractScorer:\n",
    "    cls = globals().get(dataset_name + \"Scorer\")\n",
    "    if cls:\n",
    "        return cls(experiment_no, logger)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:09:57.158735Z",
     "start_time": "2025-02-26T14:09:57.144570400Z"
    }
   },
   "id": "2c57c2041949320a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Claiming back memory & disk space"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:27:02.800833700Z",
     "start_time": "2025-02-24T09:27:02.795439900Z"
    }
   },
   "id": "6d9c696748637c7a"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\n",
      "\u001B[1;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1 / 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:09:58.128128900Z",
     "start_time": "2025-02-26T14:09:58.112946200Z"
    }
   },
   "id": "e8510f29a2801f24"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "trainer = classifier = rel_trainer = rel_classifier = bin_trainer = bin_classifier = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:09:58.805181400Z",
     "start_time": "2025-02-26T14:09:58.645454300Z"
    }
   },
   "id": "8eedf6bb99abad86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training standard classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:01:37.967407100Z",
     "start_time": "2025-02-24T11:01:37.845711600Z"
    }
   },
   "id": "3838af25e45f82f0"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "GPUS = 1\n",
    "MIN_EPOCHS = MAX_EPOCHS = 3\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-05\n",
    "LEARNING_RATE_DECAY_SPEED = [1, 1, 0.75, 0.5, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01]\n",
    "\n",
    "LINEAR_SIZE = 1024\n",
    "\n",
    "DROPOUT_P = 0.2\n",
    "ACTIVATION_FUNCTION = \"PReLU\"\n",
    "WEIGHT_DECAY = 0.01 # default = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:09:59.961129900Z",
     "start_time": "2025-02-26T14:09:59.946293600Z"
    }
   },
   "id": "ebd80c501e4d65a3"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # 是否检测到 GPU\n",
    "print(torch.cuda.device_count())  # 有多少个 GPU\n",
    "print(torch.cuda.current_device())  # 当前使用的 GPU\n",
    "print(torch.cuda.get_device_name(0))  # GPU 设备名称"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:10:00.618738Z",
     "start_time": "2025-02-26T14:10:00.597085700Z"
    }
   },
   "id": "cad5344b5dcfc56e"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 0 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始训练\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/Violetta/BERT/e/BERTRE-46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\neptune\\internal\\utils\\git.py:71: UserWarning: GitPython could not be initialized\n",
      "  warnings.warn(\"GitPython could not be initialized\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type      | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | language_model      | BertModel | 109 M  | eval \n",
      "1 | linear              | Linear    | 787 K  | train\n",
      "2 | linear_output       | Linear    | 19.5 K | train\n",
      "3 | dropout             | Dropout   | 0      | train\n",
      "4 | activation_function | PReLU     | 1      | train\n",
      "----------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "441.157   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "228       Modules in eval mode\n",
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\utilities\\data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2cf9afeb5ea4a9da0bd725eea3c8de8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Validation: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f10ce185ce04f3aa29403fa3117f5e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] validation_step_outputs before eval: [{'logits': tensor([[ 1.8771e+00, -9.3274e-02,  2.1208e-01, -6.7348e-01,  1.7353e-03,\n",
      "          5.7600e-01,  1.7315e-02, -4.3198e-02, -2.9129e-01, -1.4107e-01,\n",
      "         -3.5069e-02,  1.3180e-01,  7.0417e-02, -2.3082e-01, -3.0910e-01,\n",
      "         -4.2436e-01, -3.9521e-01, -7.3797e-01, -7.7583e-01],\n",
      "        [ 1.8455e+00, -1.7358e-01,  2.1656e-01, -5.0853e-01, -8.7274e-02,\n",
      "          5.3009e-01,  9.6530e-03,  9.4820e-02, -2.0798e-01, -1.4702e-01,\n",
      "          2.9791e-02,  9.3860e-02,  1.6955e-01, -2.0648e-01, -2.8330e-01,\n",
      "         -4.4760e-01, -3.2131e-01, -7.2273e-01, -7.0514e-01],\n",
      "        [ 1.8723e+00, -1.3475e-01,  2.4145e-01, -5.3641e-01, -9.9998e-02,\n",
      "          4.8442e-01, -3.3494e-02,  2.6678e-02, -2.7058e-01, -1.0351e-01,\n",
      "          5.6159e-02,  8.5985e-02,  1.5865e-01, -2.4994e-01, -2.9560e-01,\n",
      "         -4.4620e-01, -3.4906e-01, -7.2880e-01, -8.1259e-01],\n",
      "        [ 1.8512e+00, -4.8279e-02,  2.0939e-01, -5.2992e-01,  1.3509e-02,\n",
      "          5.6029e-01, -4.6701e-02,  5.2760e-02, -2.7550e-01, -1.2179e-01,\n",
      "         -1.7426e-02,  8.9873e-02,  8.0601e-02, -2.3585e-01, -3.2934e-01,\n",
      "         -4.4797e-01, -3.5067e-01, -7.3401e-01, -7.3767e-01],\n",
      "        [ 1.8176e+00, -1.4089e-01,  2.0751e-01, -5.5205e-01, -3.1508e-02,\n",
      "          4.9109e-01,  1.5511e-02,  1.7175e-02, -2.0559e-01, -1.1865e-01,\n",
      "          1.1358e-02,  4.0498e-02,  1.1370e-01, -3.0001e-01, -2.3129e-01,\n",
      "         -3.9831e-01, -3.3527e-01, -7.8479e-01, -7.0499e-01],\n",
      "        [ 1.7485e+00, -1.2988e-01,  2.0198e-01, -5.4017e-01, -1.2085e-01,\n",
      "          4.9694e-01,  1.5474e-02,  1.5591e-02, -2.5702e-01, -7.3576e-02,\n",
      "         -1.9142e-02,  1.0328e-01,  7.9543e-02, -2.5410e-01, -2.2569e-01,\n",
      "         -4.6673e-01, -2.9070e-01, -8.0502e-01, -7.6083e-01],\n",
      "        [ 1.7816e+00, -6.6636e-02,  1.4867e-01, -5.3705e-01, -5.3601e-03,\n",
      "          5.0705e-01, -2.6847e-02, -5.8424e-03, -2.5511e-01, -1.4990e-01,\n",
      "         -9.7213e-03,  1.3358e-01,  1.1750e-01, -2.6825e-01, -2.6289e-01,\n",
      "         -4.2696e-01, -3.0602e-01, -7.3209e-01, -7.9089e-01],\n",
      "        [ 1.9276e+00, -4.4194e-02,  1.9512e-01, -5.7193e-01, -2.2964e-02,\n",
      "          5.3170e-01, -4.1049e-02, -3.2591e-02, -3.2463e-01, -1.1326e-01,\n",
      "         -1.0793e-02,  1.4849e-01,  5.4826e-02, -2.2648e-01, -2.9727e-01,\n",
      "         -4.9969e-01, -3.5304e-01, -6.9940e-01, -7.6893e-01],\n",
      "        [ 1.8693e+00, -7.2524e-02,  1.9881e-01, -5.2172e-01, -1.9974e-02,\n",
      "          5.7025e-01,  5.1660e-02,  5.6114e-02, -2.5760e-01, -1.1153e-01,\n",
      "          5.0491e-02,  1.0315e-01,  1.2516e-01, -2.8546e-01, -2.5627e-01,\n",
      "         -4.1965e-01, -2.9627e-01, -7.2704e-01, -8.0335e-01],\n",
      "        [ 1.8667e+00, -8.4646e-02,  1.9565e-01, -5.3684e-01,  6.9438e-02,\n",
      "          5.4868e-01, -9.2853e-02,  2.4666e-02, -2.2883e-01, -1.4735e-01,\n",
      "          4.0209e-02,  9.0469e-02,  6.1638e-02, -2.4196e-01, -4.1026e-01,\n",
      "         -4.2861e-01, -3.8362e-01, -6.9856e-01, -7.2973e-01],\n",
      "        [ 1.8598e+00, -6.9415e-02,  1.8155e-01, -5.6299e-01, -8.5951e-02,\n",
      "          4.9006e-01,  1.0497e-02,  6.6505e-02, -2.6419e-01, -1.1185e-01,\n",
      "         -2.2281e-02,  4.5775e-02,  8.2870e-02, -3.1388e-01, -3.5599e-01,\n",
      "         -4.3145e-01, -3.2310e-01, -7.6805e-01, -7.4554e-01],\n",
      "        [ 1.9537e+00, -5.2813e-02,  1.6451e-01, -6.0327e-01,  3.8972e-03,\n",
      "          5.6927e-01, -4.5336e-02,  5.2142e-02, -2.2109e-01, -6.6418e-02,\n",
      "          5.1449e-02,  6.3030e-02,  1.0755e-01, -2.3415e-01, -3.3634e-01,\n",
      "         -3.8220e-01, -3.9939e-01, -7.6312e-01, -7.3583e-01],\n",
      "        [ 1.9032e+00, -6.1804e-02,  2.1108e-01, -6.0123e-01, -2.0048e-02,\n",
      "          5.4078e-01, -1.8543e-02,  1.4414e-03, -2.8456e-01, -1.0191e-01,\n",
      "          6.2649e-02,  8.0428e-02,  4.8365e-02, -2.3749e-01, -2.9492e-01,\n",
      "         -4.1608e-01, -3.5509e-01, -7.1908e-01, -8.0641e-01],\n",
      "        [ 1.9114e+00, -7.1821e-02,  2.4792e-01, -6.3961e-01, -1.2813e-01,\n",
      "          5.6117e-01, -5.5597e-02, -1.0823e-02, -2.7905e-01, -4.5202e-02,\n",
      "         -3.6220e-02,  1.3712e-01,  1.5233e-01, -2.2484e-01, -3.4364e-01,\n",
      "         -3.3236e-01, -3.2690e-01, -7.6243e-01, -8.4755e-01],\n",
      "        [ 1.7884e+00, -9.1326e-02,  1.6604e-01, -5.8086e-01, -2.0132e-02,\n",
      "          5.4147e-01, -5.1600e-02, -1.5102e-02, -2.7767e-01, -1.0221e-01,\n",
      "         -7.5420e-03,  1.2205e-01,  1.5071e-01, -2.6170e-01, -2.9116e-01,\n",
      "         -4.2810e-01, -3.3951e-01, -8.0872e-01, -7.7995e-01],\n",
      "        [ 1.8225e+00, -1.1807e-01,  1.9727e-01, -6.1894e-01, -8.0087e-03,\n",
      "          5.5602e-01, -2.1083e-02,  5.4862e-02, -2.4649e-01, -2.2669e-02,\n",
      "         -3.0454e-02,  1.6747e-01,  1.6495e-01, -3.1378e-01, -3.0234e-01,\n",
      "         -4.4760e-01, -3.5048e-01, -7.8645e-01, -7.4264e-01]], device='cuda:0'), 'label': tensor([12,  8, 10,  0,  9, 12,  0, 11, 16,  4,  8,  7,  9,  0, 10,  0],\n",
      "       device='cuda:0')}, {'logits': tensor([[ 1.9109e+00, -4.0563e-02,  2.8878e-01, -5.0692e-01, -7.7338e-02,\n",
      "          5.6375e-01,  4.2518e-02,  4.3101e-02, -2.2789e-01, -5.7940e-02,\n",
      "          4.5414e-02,  7.4201e-02,  9.3062e-02, -3.1930e-01, -3.3923e-01,\n",
      "         -4.3421e-01, -3.7410e-01, -7.8171e-01, -7.5445e-01],\n",
      "        [ 1.8914e+00, -2.1162e-02,  2.0539e-01, -6.4354e-01, -4.4397e-02,\n",
      "          5.4374e-01, -9.1754e-02, -3.7409e-02, -3.3499e-01, -1.2060e-01,\n",
      "         -4.4553e-02,  1.2990e-01,  5.6014e-02, -1.8198e-01, -3.1640e-01,\n",
      "         -4.4963e-01, -3.7819e-01, -7.4715e-01, -7.5837e-01],\n",
      "        [ 1.8847e+00, -2.0272e-02,  1.0607e-01, -5.7206e-01,  1.0998e-03,\n",
      "          5.2024e-01, -3.2993e-02,  6.1056e-02, -2.8408e-01, -1.5342e-01,\n",
      "          1.1832e-02,  1.1719e-01,  5.2026e-02, -3.2292e-01, -3.5534e-01,\n",
      "         -5.1905e-01, -3.5595e-01, -6.9607e-01, -7.0368e-01],\n",
      "        [ 1.9830e+00, -5.2081e-02,  8.3583e-02, -5.3408e-01, -3.6307e-02,\n",
      "          5.5224e-01, -9.5923e-02,  7.5432e-03, -2.4064e-01, -5.6495e-02,\n",
      "          3.7897e-02,  1.4858e-01,  4.3466e-02, -2.7220e-01, -3.4891e-01,\n",
      "         -3.1089e-01, -3.8591e-01, -6.9059e-01, -6.8267e-01],\n",
      "        [ 1.8041e+00, -8.1971e-02,  1.6408e-01, -5.3649e-01, -3.9229e-02,\n",
      "          5.1518e-01,  3.2564e-02,  6.7389e-03, -3.1325e-01, -5.7000e-02,\n",
      "         -1.4466e-02,  1.3562e-01,  8.8455e-02, -2.4364e-01, -3.4272e-01,\n",
      "         -4.7554e-01, -2.6683e-01, -8.1580e-01, -7.4899e-01],\n",
      "        [ 1.8120e+00, -7.6982e-02,  1.7822e-01, -6.3609e-01, -5.5554e-02,\n",
      "          5.3265e-01, -5.5586e-02,  1.2240e-02, -2.9776e-01, -6.8631e-02,\n",
      "         -2.5018e-02,  1.2182e-01,  1.0073e-01, -1.5257e-01, -3.3816e-01,\n",
      "         -4.4870e-01, -3.0837e-01, -7.0419e-01, -7.7637e-01],\n",
      "        [ 1.8996e+00, -1.7356e-03,  1.1515e-01, -5.6715e-01, -1.3446e-01,\n",
      "          5.2782e-01, -3.1817e-02, -4.4358e-02, -1.2608e-01, -7.9376e-03,\n",
      "          8.7829e-02,  8.8054e-02,  6.4679e-02, -3.0712e-01, -3.4773e-01,\n",
      "         -3.5371e-01, -3.5599e-01, -7.6917e-01, -7.2789e-01],\n",
      "        [ 1.8153e+00, -1.7669e-02,  1.9203e-01, -5.7525e-01, -1.3937e-01,\n",
      "          5.0800e-01, -4.5022e-02,  7.0838e-02, -2.5570e-01, -5.4576e-02,\n",
      "          2.8670e-02,  1.1687e-01,  4.8628e-02, -2.6449e-01, -3.4023e-01,\n",
      "         -4.7141e-01, -3.3142e-01, -7.0563e-01, -7.6212e-01],\n",
      "        [ 1.9020e+00, -6.9887e-02,  2.5499e-01, -6.1124e-01, -1.3574e-01,\n",
      "          4.9501e-01, -1.9930e-02,  4.3055e-02, -1.8782e-01, -7.0547e-02,\n",
      "          1.1230e-02,  1.5194e-01,  1.4477e-01, -2.2852e-01, -3.1761e-01,\n",
      "         -3.9946e-01, -3.6600e-01, -7.4794e-01, -7.4665e-01],\n",
      "        [ 1.7903e+00, -1.7245e-01,  2.1859e-01, -5.0373e-01, -7.0692e-02,\n",
      "          5.2651e-01,  6.3382e-02, -3.8492e-02, -2.4947e-01, -1.0379e-01,\n",
      "          1.2095e-02,  1.8913e-01,  1.5350e-01, -2.9165e-01, -2.7561e-01,\n",
      "         -3.9503e-01, -3.0719e-01, -7.6542e-01, -7.5472e-01],\n",
      "        [ 1.8649e+00, -6.7540e-02,  1.6832e-01, -7.1025e-01, -6.4362e-02,\n",
      "          6.0702e-01, -3.8122e-02, -8.4066e-02, -2.7622e-01, -1.2372e-01,\n",
      "         -1.8105e-02,  1.9795e-01,  8.0470e-02, -2.4747e-01, -4.0687e-01,\n",
      "         -4.8873e-01, -3.9830e-01, -7.7905e-01, -7.0576e-01],\n",
      "        [ 1.9000e+00, -2.6585e-02,  1.7176e-01, -5.7662e-01, -3.7510e-02,\n",
      "          5.1505e-01, -3.8676e-02,  4.1888e-02, -2.8623e-01, -7.4374e-02,\n",
      "         -2.0272e-02,  8.8998e-02,  1.2053e-01, -2.7902e-01, -3.4231e-01,\n",
      "         -4.5413e-01, -3.5820e-01, -7.4951e-01, -7.6264e-01],\n",
      "        [ 1.8156e+00, -9.2736e-02,  1.8440e-01, -5.7836e-01, -3.0545e-02,\n",
      "          5.0257e-01,  2.0000e-02,  3.1898e-02, -2.6481e-01, -9.8727e-02,\n",
      "          6.0864e-04,  1.0606e-01,  1.6200e-01, -2.2400e-01, -3.1609e-01,\n",
      "         -4.9821e-01, -2.9485e-01, -7.4376e-01, -7.4969e-01],\n",
      "        [ 1.7984e+00, -1.0229e-01,  2.2236e-01, -5.6613e-01, -2.4609e-02,\n",
      "          5.4760e-01,  1.5436e-02, -1.2593e-03, -2.3208e-01, -9.2167e-02,\n",
      "         -1.3173e-03,  1.4579e-01,  1.1622e-01, -2.0231e-01, -2.7687e-01,\n",
      "         -4.2040e-01, -3.0177e-01, -7.3368e-01, -7.5905e-01],\n",
      "        [ 1.9453e+00, -4.6841e-02,  1.9652e-01, -6.7155e-01, -2.7911e-03,\n",
      "          5.8852e-01, -6.3382e-02, -3.3865e-02, -3.1776e-01, -5.6688e-02,\n",
      "         -2.7802e-02,  1.4724e-01,  8.5260e-02, -2.6776e-01, -3.2026e-01,\n",
      "         -4.3612e-01, -4.1926e-01, -7.4977e-01, -7.4532e-01],\n",
      "        [ 1.8744e+00, -4.2754e-02,  1.6671e-01, -6.3307e-01,  4.8660e-02,\n",
      "          5.0167e-01, -5.3628e-02, -3.4365e-02, -3.0431e-01, -1.7106e-01,\n",
      "         -6.8965e-02,  1.5301e-01,  1.5186e-01, -2.1141e-01, -3.4392e-01,\n",
      "         -5.0916e-01, -3.9832e-01, -7.2947e-01, -7.6001e-01]], device='cuda:0'), 'label': tensor([ 9, 11,  4, 11, 16,  0,  0,  0,  0,  6,  0,  0, 16,  4,  4,  4],\n",
      "       device='cuda:0')}, {'logits': tensor([[ 1.8051e+00, -6.2082e-02,  1.7714e-01, -5.2926e-01, -2.7240e-02,\n",
      "          5.8011e-01,  3.8358e-02,  3.9761e-02, -2.5942e-01, -7.7352e-02,\n",
      "         -2.9462e-02,  1.6341e-01,  1.8895e-01, -2.8723e-01, -2.9807e-01,\n",
      "         -4.5165e-01, -3.0540e-01, -8.0220e-01, -7.7839e-01],\n",
      "        [ 1.7851e+00, -6.6123e-02,  1.2618e-01, -5.6923e-01, -5.2326e-02,\n",
      "          5.1296e-01,  1.2688e-02, -8.1294e-03, -2.1583e-01, -6.7677e-02,\n",
      "          2.4061e-02,  1.2016e-01,  1.0269e-01, -2.4698e-01, -3.5328e-01,\n",
      "         -4.2770e-01, -3.3000e-01, -7.6026e-01, -7.8629e-01],\n",
      "        [ 1.8525e+00, -6.8293e-02,  1.8463e-01, -6.0481e-01, -8.0111e-02,\n",
      "          5.3461e-01, -3.8735e-02,  8.3560e-02, -2.9512e-01, -8.4823e-02,\n",
      "          3.3507e-02,  1.0981e-01,  1.0496e-01, -1.7825e-01, -3.0987e-01,\n",
      "         -4.8128e-01, -3.2333e-01, -7.0864e-01, -7.8081e-01],\n",
      "        [ 1.9118e+00, -8.2120e-02,  2.4326e-01, -5.2339e-01, -1.5709e-01,\n",
      "          5.5549e-01,  5.4248e-04,  1.4220e-03, -1.8562e-01,  7.1628e-03,\n",
      "          4.2060e-02,  1.2181e-01,  1.1126e-01, -2.9022e-01, -3.9495e-01,\n",
      "         -3.7524e-01, -3.3816e-01, -8.2021e-01, -7.9936e-01],\n",
      "        [ 1.8000e+00, -7.4041e-02,  1.4617e-01, -5.1735e-01,  3.3845e-02,\n",
      "          5.1622e-01,  8.3193e-02,  3.1941e-02, -2.0943e-01, -1.0435e-01,\n",
      "         -4.6033e-02,  1.5017e-01,  1.2216e-01, -3.2686e-01, -2.6993e-01,\n",
      "         -4.5556e-01, -2.4315e-01, -7.6018e-01, -7.5648e-01],\n",
      "        [ 1.8984e+00, -1.2088e-01,  2.7744e-01, -5.4081e-01, -3.0321e-02,\n",
      "          6.1195e-01,  2.2003e-02, -4.7710e-02, -2.6847e-01, -1.0463e-01,\n",
      "         -1.9338e-03,  8.7992e-02,  8.8862e-02, -2.2078e-01, -3.6026e-01,\n",
      "         -4.0527e-01, -3.6937e-01, -7.9715e-01, -7.7495e-01],\n",
      "        [ 1.9253e+00, -1.1963e-01,  1.5768e-01, -5.4161e-01,  2.1563e-02,\n",
      "          5.5299e-01, -9.3609e-03, -3.9698e-03, -2.7954e-01, -1.6154e-01,\n",
      "         -3.4284e-02,  1.1264e-01,  1.3943e-01, -3.1392e-01, -3.0097e-01,\n",
      "         -4.2190e-01, -3.9064e-01, -7.2230e-01, -8.0732e-01],\n",
      "        [ 1.9227e+00, -7.7072e-02,  2.2912e-01, -5.4165e-01, -6.0547e-02,\n",
      "          6.0655e-01, -2.4427e-02, -2.9236e-02, -2.3943e-01, -1.1256e-01,\n",
      "          2.7474e-02,  7.5559e-02,  5.7471e-02, -3.1466e-01, -3.7913e-01,\n",
      "         -4.6520e-01, -4.5344e-01, -7.8881e-01, -7.5117e-01],\n",
      "        [ 2.0056e+00, -1.0148e-01,  2.4660e-01, -5.5572e-01, -1.3007e-01,\n",
      "          5.3876e-01,  1.9614e-02,  8.0519e-02, -2.3235e-01, -1.2841e-01,\n",
      "         -2.0407e-02,  1.1095e-01,  1.1216e-01, -2.5586e-01, -2.8912e-01,\n",
      "         -2.9343e-01, -4.3407e-01, -7.3455e-01, -7.9045e-01],\n",
      "        [ 1.8396e+00, -1.0332e-01,  1.9318e-01, -5.3265e-01, -9.1949e-02,\n",
      "          5.2772e-01,  1.7214e-02,  3.6073e-02, -2.0339e-01, -7.4094e-02,\n",
      "          3.6440e-02,  1.0845e-01,  1.5842e-01, -2.6800e-01, -2.7493e-01,\n",
      "         -4.0762e-01, -2.7465e-01, -8.2176e-01, -7.8462e-01],\n",
      "        [ 1.8826e+00, -3.9344e-02,  1.8889e-01, -5.4667e-01, -8.0377e-02,\n",
      "          6.3154e-01, -4.1840e-02,  5.1970e-02, -3.1920e-01, -2.3758e-02,\n",
      "          4.6595e-02,  6.8853e-02,  1.1007e-01, -2.7442e-01, -3.7769e-01,\n",
      "         -4.5982e-01, -3.1795e-01, -7.0096e-01, -7.7145e-01],\n",
      "        [ 1.8467e+00, -5.5174e-02,  1.1349e-01, -5.6220e-01, -5.6963e-02,\n",
      "          6.0088e-01,  3.0588e-02,  1.1666e-02, -2.2204e-01, -1.1528e-01,\n",
      "         -2.2682e-02,  1.5135e-01,  9.4405e-02, -2.8237e-01, -3.2095e-01,\n",
      "         -4.4966e-01, -3.5523e-01, -7.9379e-01, -8.1408e-01],\n",
      "        [ 1.8661e+00, -8.3629e-02,  2.4407e-01, -6.9102e-01, -2.6568e-02,\n",
      "          5.2206e-01, -4.9764e-03,  7.9352e-03, -2.6336e-01, -8.0528e-02,\n",
      "          1.5930e-02,  1.4026e-01,  1.0863e-01, -2.1911e-01, -3.1181e-01,\n",
      "         -3.6687e-01, -4.8651e-01, -7.0697e-01, -7.7860e-01],\n",
      "        [ 1.9166e+00, -1.7456e-01,  1.8517e-01, -5.6615e-01, -1.2570e-01,\n",
      "          5.9137e-01, -6.6518e-02,  8.0740e-02, -2.2789e-01,  9.1734e-03,\n",
      "          5.0194e-02,  1.2425e-01,  1.2107e-01, -2.7537e-01, -4.1168e-01,\n",
      "         -4.1190e-01, -3.1362e-01, -7.8557e-01, -7.4378e-01],\n",
      "        [ 1.9366e+00, -3.4206e-02,  1.9919e-01, -6.7157e-01, -3.3348e-03,\n",
      "          5.6576e-01,  8.7427e-03, -1.6022e-02, -3.0744e-01, -9.2852e-02,\n",
      "         -2.0545e-02,  9.4656e-02,  4.9420e-02, -2.8124e-01, -2.9906e-01,\n",
      "         -4.3624e-01, -4.2735e-01, -7.9149e-01, -7.7087e-01],\n",
      "        [ 1.9301e+00, -4.0847e-02,  1.8778e-01, -6.1490e-01, -2.3012e-02,\n",
      "          5.4423e-01, -5.9096e-02,  3.1048e-02, -2.2755e-01, -8.4552e-02,\n",
      "          9.6683e-02,  7.0046e-02,  6.5529e-02, -2.6599e-01, -3.5646e-01,\n",
      "         -4.1113e-01, -3.8371e-01, -7.7284e-01, -7.2995e-01]], device='cuda:0'), 'label': tensor([16, 12,  7,  9, 16,  5,  5,  5,  0,  0,  0,  6, 10,  9,  0, 10],\n",
      "       device='cuda:0')}, {'logits': tensor([[ 1.8895e+00, -5.2718e-02,  1.8870e-01, -5.4262e-01, -1.0704e-02,\n",
      "          5.2165e-01, -3.9467e-02,  3.8219e-02, -2.3831e-01, -1.5373e-01,\n",
      "          4.0003e-02,  1.2122e-01,  1.0002e-01, -2.3047e-01, -3.3318e-01,\n",
      "         -5.1956e-01, -2.8459e-01, -7.3081e-01, -7.2221e-01],\n",
      "        [ 1.8295e+00, -9.8572e-02,  1.4921e-01, -5.4486e-01, -6.4602e-02,\n",
      "          5.0359e-01,  5.3414e-02,  4.1175e-02, -2.9686e-01, -6.6724e-02,\n",
      "          6.7337e-04,  1.1220e-01,  1.6806e-01, -2.4966e-01, -2.9825e-01,\n",
      "         -4.4935e-01, -2.2915e-01, -8.0875e-01, -7.4242e-01],\n",
      "        [ 1.8150e+00, -8.9760e-02,  1.1370e-01, -6.4252e-01, -3.7498e-02,\n",
      "          5.8163e-01, -8.5292e-02,  1.8456e-03, -2.2902e-01, -1.4608e-01,\n",
      "         -2.9172e-02,  1.8183e-01, -2.5601e-03, -2.6843e-01, -4.2178e-01,\n",
      "         -4.4896e-01, -4.0120e-01, -6.6558e-01, -6.7521e-01],\n",
      "        [ 1.8832e+00, -2.8309e-02,  2.2018e-01, -6.5794e-01, -5.8770e-02,\n",
      "          5.2149e-01, -3.6822e-02, -1.4183e-02, -2.4180e-01, -1.1786e-01,\n",
      "          2.1075e-02,  1.2931e-01,  2.8542e-02, -2.2076e-01, -3.7378e-01,\n",
      "         -4.1069e-01, -3.5658e-01, -7.6183e-01, -6.9241e-01],\n",
      "        [ 1.8423e+00, -5.4311e-02,  1.9394e-01, -5.1720e-01,  1.8328e-02,\n",
      "          5.3090e-01,  5.5147e-02,  2.7785e-02, -2.7534e-01, -1.9823e-01,\n",
      "         -1.1893e-02,  9.2809e-02,  3.7533e-02, -1.5764e-01, -2.5581e-01,\n",
      "         -4.2024e-01, -3.2287e-01, -6.6335e-01, -7.1830e-01],\n",
      "        [ 1.9109e+00, -1.2568e-01,  2.0973e-01, -5.8510e-01, -1.5331e-01,\n",
      "          5.5125e-01, -2.9813e-02,  3.6275e-02, -2.2394e-01, -1.0166e-01,\n",
      "         -3.4508e-02,  1.2096e-01,  1.2844e-01, -1.5907e-01, -3.4792e-01,\n",
      "         -3.8658e-01, -3.7954e-01, -7.1221e-01, -7.0116e-01],\n",
      "        [ 1.7692e+00, -1.3032e-01,  2.8645e-01, -4.9871e-01, -6.9874e-02,\n",
      "          5.0717e-01, -2.5470e-02,  8.5444e-02, -2.4751e-01, -1.2286e-01,\n",
      "         -8.7841e-03,  2.9237e-02,  5.7936e-02, -3.1915e-01, -2.5686e-01,\n",
      "         -3.8369e-01, -3.4365e-01, -7.4448e-01, -7.1838e-01],\n",
      "        [ 1.7789e+00, -1.3007e-01,  1.8914e-01, -4.4000e-01, -1.0954e-01,\n",
      "          4.5121e-01,  2.2375e-02, -2.3990e-02, -2.2663e-01, -6.1440e-02,\n",
      "         -7.9198e-02,  8.4904e-02,  1.2805e-01, -3.2820e-01, -2.9274e-01,\n",
      "         -4.2928e-01, -2.1979e-01, -9.0120e-01, -6.4680e-01],\n",
      "        [ 1.9150e+00, -1.0361e-01,  2.2656e-01, -5.8787e-01, -1.2176e-02,\n",
      "          5.5214e-01,  9.5943e-03, -7.8683e-02, -2.2212e-01, -1.4916e-01,\n",
      "         -5.3297e-03,  1.1337e-01,  4.2024e-02, -2.9098e-01, -3.0268e-01,\n",
      "         -4.0948e-01, -3.6919e-01, -7.7537e-01, -7.4577e-01],\n",
      "        [ 1.7818e+00, -1.5562e-01,  2.4984e-01, -5.8900e-01, -1.9983e-01,\n",
      "          4.4841e-01, -1.1413e-02,  9.1766e-03, -1.8763e-01, -1.0760e-01,\n",
      "         -4.1898e-02,  1.2474e-01,  2.0497e-01, -2.0339e-01, -3.4737e-01,\n",
      "         -3.7958e-01, -3.1326e-01, -7.2303e-01, -7.9599e-01],\n",
      "        [ 1.8714e+00, -1.1758e-01,  2.4780e-01, -6.2257e-01, -3.8438e-02,\n",
      "          5.0778e-01, -1.8798e-02, -7.0510e-03, -3.3166e-01, -7.9775e-02,\n",
      "          1.9033e-02,  1.0645e-01,  9.4998e-02, -2.0896e-01, -2.9010e-01,\n",
      "         -3.6808e-01, -3.2806e-01, -7.7661e-01, -7.8439e-01],\n",
      "        [ 1.8451e+00, -1.7348e-01,  2.0007e-01, -5.9269e-01, -6.6043e-02,\n",
      "          4.5571e-01,  2.1425e-02,  2.4085e-02, -2.5022e-01, -1.5244e-01,\n",
      "         -8.3521e-03,  8.2183e-02,  1.1634e-01, -2.3029e-01, -2.9495e-01,\n",
      "         -3.7388e-01, -3.1702e-01, -6.8284e-01, -7.0658e-01],\n",
      "        [ 1.6818e+00, -1.1159e-01,  2.1901e-01, -4.9621e-01, -5.7671e-02,\n",
      "          4.5300e-01,  1.2563e-02, -1.5981e-02, -2.1243e-01, -1.5782e-01,\n",
      "          1.4766e-02, -1.4171e-02, -3.1791e-02, -2.2539e-01, -2.8858e-01,\n",
      "         -2.8417e-01, -3.0627e-01, -8.0281e-01, -7.2218e-01],\n",
      "        [ 1.8601e+00, -1.0198e-01,  2.3699e-01, -5.0666e-01, -5.9534e-02,\n",
      "          4.8204e-01, -6.0897e-02,  3.1139e-02, -2.7315e-01, -1.9186e-01,\n",
      "         -2.1961e-02,  2.7847e-02,  5.3250e-02, -2.6368e-01, -2.9673e-01,\n",
      "         -2.5506e-01, -4.9954e-01, -6.3312e-01, -7.1509e-01],\n",
      "        [ 1.7323e+00, -1.6318e-01,  1.9031e-01, -4.8566e-01,  3.6092e-03,\n",
      "          4.6808e-01,  1.0183e-02,  1.4115e-02, -2.4522e-01, -1.3454e-01,\n",
      "         -3.5514e-02,  4.5027e-02,  1.3418e-01, -2.4401e-01, -3.2900e-01,\n",
      "         -4.7071e-01, -2.7365e-01, -7.5705e-01, -7.3313e-01],\n",
      "        [ 1.8011e+00, -1.6237e-01,  1.7779e-01, -4.8314e-01, -8.4125e-02,\n",
      "          4.7768e-01,  1.0182e-01, -2.3318e-03, -2.3976e-01, -1.0494e-01,\n",
      "          2.3189e-02,  9.5507e-02,  1.3478e-01, -2.8789e-01, -3.1981e-01,\n",
      "         -4.4348e-01, -2.3580e-01, -7.1175e-01, -6.8967e-01]], device='cuda:0'), 'label': tensor([14, 12,  4, 11,  8,  2,  9,  0,  0,  1,  4,  0, 15,  0, 10,  6],\n",
      "       device='cuda:0')}, {'logits': tensor([[ 1.9118e+00, -9.1544e-02,  2.2304e-01, -5.8402e-01, -1.0843e-01,\n",
      "          5.5175e-01,  5.5020e-02, -6.2879e-02, -2.3235e-01, -1.0653e-01,\n",
      "         -4.1572e-02,  1.7991e-01,  8.5082e-02, -3.0834e-01, -3.2570e-01,\n",
      "         -4.5047e-01, -3.6314e-01, -8.1238e-01, -7.5052e-01],\n",
      "        [ 1.8915e+00, -9.0858e-02,  2.3257e-01, -5.9336e-01, -8.8606e-02,\n",
      "          4.5594e-01, -3.5005e-02, -2.0488e-02, -1.9938e-01, -5.3529e-02,\n",
      "          1.0759e-02,  3.8443e-02,  9.0930e-02, -3.0697e-01, -3.6681e-01,\n",
      "         -4.5245e-01, -3.5139e-01, -8.3543e-01, -7.5110e-01],\n",
      "        [ 1.8811e+00, -1.0442e-01,  1.7426e-01, -4.8745e-01, -8.0267e-02,\n",
      "          5.7389e-01, -1.8870e-02,  2.3011e-02, -2.6517e-01, -6.0384e-02,\n",
      "          2.3472e-02,  1.0410e-01,  1.1712e-01, -2.3529e-01, -3.6091e-01,\n",
      "         -4.5772e-01, -2.9328e-01, -8.1014e-01, -7.6059e-01],\n",
      "        [ 1.9679e+00, -1.0644e-01,  1.8788e-01, -6.1122e-01, -5.3436e-02,\n",
      "          6.0652e-01,  1.0186e-02, -4.4509e-02, -2.7898e-01, -6.3493e-02,\n",
      "          8.8984e-03,  1.7450e-01,  1.3964e-01, -2.9396e-01, -4.1470e-01,\n",
      "         -4.1136e-01, -4.1890e-01, -8.1099e-01, -7.5020e-01],\n",
      "        [ 1.9309e+00, -4.8875e-02,  1.2377e-01, -5.8900e-01, -1.4261e-02,\n",
      "          5.2830e-01,  3.5852e-02,  1.9016e-02, -2.6116e-01, -1.2634e-01,\n",
      "          4.5303e-02,  3.9590e-02,  9.8833e-02, -2.4883e-01, -3.3085e-01,\n",
      "         -4.6703e-01, -3.2873e-01, -7.2106e-01, -7.8721e-01],\n",
      "        [ 1.7244e+00, -5.3404e-02,  1.3406e-01, -3.7548e-01, -3.4395e-02,\n",
      "          4.4349e-01,  2.0099e-02,  7.9767e-02, -2.5384e-01, -2.0007e-01,\n",
      "          5.4638e-02,  6.6270e-02,  2.0977e-02, -1.7685e-01, -2.4357e-01,\n",
      "         -3.6263e-01, -2.8317e-01, -5.0269e-01, -6.2216e-01],\n",
      "        [ 1.8258e+00, -4.7131e-02,  2.1005e-01, -5.7824e-01, -1.1103e-01,\n",
      "          5.6342e-01,  1.6096e-02,  4.3797e-02, -3.0553e-01, -3.8993e-02,\n",
      "         -5.3064e-03,  1.0316e-01,  1.2062e-01, -2.5690e-01, -3.1907e-01,\n",
      "         -4.5729e-01, -3.4221e-01, -7.9214e-01, -7.8511e-01],\n",
      "        [ 1.8620e+00, -1.2569e-01,  1.8663e-01, -5.8264e-01, -9.6194e-02,\n",
      "          5.9991e-01, -2.9122e-02, -5.4330e-03, -2.0275e-01, -5.9564e-02,\n",
      "         -3.1980e-03,  1.1791e-01,  1.0507e-01, -2.7349e-01, -3.4371e-01,\n",
      "         -3.9810e-01, -3.0739e-01, -8.6857e-01, -7.6264e-01],\n",
      "        [ 1.9129e+00, -7.2338e-02,  2.8705e-01, -5.7308e-01, -5.3777e-02,\n",
      "          5.5533e-01, -3.1601e-02, -3.8278e-02, -2.7816e-01, -1.3636e-01,\n",
      "          5.8188e-02,  8.1374e-02,  2.7567e-02, -2.6346e-01, -3.3148e-01,\n",
      "         -4.3242e-01, -4.5533e-01, -7.1646e-01, -7.6006e-01],\n",
      "        [ 1.8967e+00, -6.4885e-02,  2.0254e-01, -5.5040e-01, -3.0985e-02,\n",
      "          5.3326e-01, -8.3658e-03, -1.6275e-02, -3.2495e-01, -1.2770e-01,\n",
      "         -1.4998e-03,  6.8061e-02,  1.5576e-01, -2.2249e-01, -3.2925e-01,\n",
      "         -4.9628e-01, -3.3484e-01, -7.8541e-01, -7.8556e-01],\n",
      "        [ 1.9621e+00, -1.1961e-01,  3.0151e-01, -6.1994e-01, -1.0291e-01,\n",
      "          6.2415e-01, -4.0145e-02,  3.7102e-02, -2.7743e-01, -2.3472e-02,\n",
      "          6.8916e-02,  8.3595e-02,  7.1240e-02, -2.2673e-01, -3.7189e-01,\n",
      "         -3.8687e-01, -4.0966e-01, -8.0900e-01, -8.0316e-01],\n",
      "        [ 1.8693e+00, -8.5961e-02,  2.0983e-01, -5.7789e-01, -4.0153e-02,\n",
      "          5.2589e-01, -5.4078e-02,  4.5656e-02, -2.8715e-01, -1.2640e-01,\n",
      "          3.3472e-02,  1.0313e-01,  9.6192e-02, -2.2756e-01, -2.9516e-01,\n",
      "         -4.0994e-01, -3.5007e-01, -7.4855e-01, -7.3797e-01],\n",
      "        [ 1.8710e+00, -5.8623e-02,  1.7476e-01, -5.9828e-01, -4.3055e-02,\n",
      "          5.4170e-01, -2.3427e-02,  9.2161e-03, -2.7492e-01, -1.2138e-01,\n",
      "         -1.5958e-02,  1.2075e-01,  1.1434e-01, -2.4941e-01, -3.0702e-01,\n",
      "         -3.8373e-01, -2.8905e-01, -7.2448e-01, -8.0691e-01],\n",
      "        [ 1.9014e+00,  4.4794e-02,  2.5142e-01, -6.0362e-01, -4.2337e-02,\n",
      "          5.4760e-01, -6.9274e-02,  7.7312e-03, -2.8257e-01, -1.1998e-01,\n",
      "         -6.8984e-02,  4.8310e-02,  7.2265e-02, -2.6411e-01, -3.7852e-01,\n",
      "         -4.4793e-01, -4.3358e-01, -7.6931e-01, -7.5301e-01],\n",
      "        [ 1.8672e+00, -1.3161e-01,  2.7023e-01, -5.4114e-01, -4.0872e-02,\n",
      "          5.1080e-01, -6.9693e-03,  1.1820e-02, -1.9037e-01, -8.1097e-02,\n",
      "          8.0011e-03,  7.5811e-02,  1.3711e-01, -2.7775e-01, -3.0523e-01,\n",
      "         -3.8418e-01, -3.6492e-01, -7.5724e-01, -7.1070e-01],\n",
      "        [ 1.7639e+00, -7.3867e-02,  5.4171e-02, -5.5571e-01, -2.4076e-02,\n",
      "          5.6134e-01, -2.4799e-02, -8.0841e-03, -2.4519e-01, -1.1004e-01,\n",
      "         -1.1262e-02,  6.1972e-02,  8.6975e-02, -2.9180e-01, -3.2931e-01,\n",
      "         -4.6815e-01, -3.2989e-01, -7.8532e-01, -8.0384e-01]], device='cuda:0'), 'label': tensor([ 0,  9, 13,  2,  0,  0,  2, 16,  0, 10,  2, 10,  4,  4,  0, 10],\n",
      "       device='cuda:0')}, {'logits': tensor([[ 1.7859e+00, -8.4841e-02,  2.0634e-01, -6.2428e-01,  1.8828e-02,\n",
      "          4.3837e-01,  1.0232e-02, -6.0751e-02, -2.0045e-01, -1.2360e-01,\n",
      "          1.6269e-02,  1.1940e-01,  9.3217e-02, -2.0152e-01, -2.8719e-01,\n",
      "         -4.0020e-01, -3.5803e-01, -7.0764e-01, -7.1029e-01],\n",
      "        [ 1.8582e+00, -1.6221e-01,  2.3085e-01, -5.0723e-01, -1.0301e-01,\n",
      "          5.6593e-01,  2.8315e-02,  6.1263e-02, -2.6945e-01, -9.9876e-02,\n",
      "          5.4744e-02,  4.0739e-02,  1.1361e-01, -2.5785e-01, -3.0065e-01,\n",
      "         -4.1931e-01, -3.4205e-01, -7.2552e-01, -7.2237e-01],\n",
      "        [ 1.7558e+00, -1.5916e-01,  2.5311e-01, -5.4375e-01, -5.2460e-03,\n",
      "          4.9727e-01,  7.6009e-02, -1.0362e-03, -2.0334e-01, -1.5278e-01,\n",
      "         -3.3653e-02,  1.0127e-01,  1.8841e-01, -1.8192e-01, -3.0174e-01,\n",
      "         -3.8169e-01, -3.5167e-01, -7.0182e-01, -7.2709e-01],\n",
      "        [ 1.7170e+00, -7.5645e-02,  1.5912e-01, -4.1169e-01, -6.5699e-03,\n",
      "          4.9290e-01,  4.0645e-02,  4.5057e-02, -2.3497e-01, -6.8847e-02,\n",
      "          8.2620e-02,  9.0617e-02,  9.4091e-02, -2.3353e-01, -2.1630e-01,\n",
      "         -3.9098e-01, -2.1715e-01, -7.1751e-01, -7.1410e-01],\n",
      "        [ 1.8385e+00, -1.8801e-02,  2.0081e-01, -5.5888e-01, -4.5304e-03,\n",
      "          5.1504e-01, -1.2451e-02,  3.7006e-02, -2.7622e-01, -9.5290e-02,\n",
      "          5.6640e-02,  4.0524e-02,  7.2939e-02, -2.5680e-01, -2.9589e-01,\n",
      "         -4.1634e-01, -3.4362e-01, -8.0085e-01, -7.1756e-01],\n",
      "        [ 1.8779e+00,  1.4803e-02,  1.7744e-01, -6.4458e-01,  5.0017e-02,\n",
      "          4.8899e-01, -4.3526e-02,  1.3063e-02, -2.4643e-01, -1.1148e-01,\n",
      "          1.2263e-02,  1.1888e-01,  6.0946e-02, -2.8567e-01, -3.5610e-01,\n",
      "         -4.7829e-01, -3.7972e-01, -6.7379e-01, -7.5975e-01],\n",
      "        [ 1.8668e+00, -1.5423e-01,  2.5852e-01, -5.0930e-01, -1.6104e-02,\n",
      "          4.9438e-01,  4.5121e-03,  3.1890e-04, -2.7534e-01, -8.5511e-02,\n",
      "          2.8788e-02,  7.4111e-02,  1.2942e-01, -2.1640e-01, -3.0479e-01,\n",
      "         -3.7575e-01, -2.9197e-01, -8.2844e-01, -7.5985e-01],\n",
      "        [ 1.7566e+00, -1.1908e-01,  1.0605e-01, -5.3860e-01, -5.6497e-02,\n",
      "          4.6234e-01,  3.8921e-02, -2.5956e-02, -2.4266e-01, -6.9191e-02,\n",
      "         -5.1988e-02,  1.0273e-01,  5.8927e-02, -2.2467e-01, -3.0605e-01,\n",
      "         -4.7713e-01, -2.5254e-01, -7.8314e-01, -7.1158e-01],\n",
      "        [ 1.8709e+00, -1.5500e-01,  1.4045e-01, -5.4463e-01, -9.1491e-02,\n",
      "          5.4305e-01,  1.7376e-02,  1.1342e-02, -2.0664e-01, -8.0942e-02,\n",
      "          4.1359e-02,  1.4062e-01,  9.4335e-02, -3.1155e-01, -3.5568e-01,\n",
      "         -4.3805e-01, -2.5648e-01, -7.4015e-01, -6.7287e-01],\n",
      "        [ 1.8299e+00, -8.5921e-02,  1.7712e-01, -5.6075e-01, -1.6652e-03,\n",
      "          5.5248e-01,  2.0338e-02,  8.0670e-03, -3.0195e-01, -1.4206e-01,\n",
      "         -2.5576e-02,  1.6056e-01,  1.2206e-01, -2.5876e-01, -2.7498e-01,\n",
      "         -4.5635e-01, -2.6950e-01, -6.6452e-01, -7.6043e-01],\n",
      "        [ 1.8628e+00, -1.0154e-01,  2.7073e-01, -5.9001e-01, -9.4089e-03,\n",
      "          5.3765e-01,  3.6108e-03,  9.3707e-02, -2.2276e-01, -9.7200e-02,\n",
      "          7.2034e-02,  1.1634e-01,  4.1587e-02, -2.3496e-01, -3.0887e-01,\n",
      "         -4.1426e-01, -3.0035e-01, -6.8479e-01, -7.0863e-01],\n",
      "        [ 1.7887e+00, -1.3734e-01,  2.3683e-01, -5.4550e-01, -3.5895e-02,\n",
      "          5.4361e-01,  5.3254e-02, -1.7288e-02, -2.3656e-01, -9.5549e-02,\n",
      "         -1.8121e-02,  9.7321e-02,  1.1587e-01, -1.9695e-01, -2.6577e-01,\n",
      "         -3.8451e-01, -3.3458e-01, -7.7789e-01, -7.1063e-01],\n",
      "        [ 1.8339e+00, -1.0517e-01,  1.5589e-01, -4.5322e-01, -2.5346e-02,\n",
      "          4.9546e-01,  5.3849e-02,  2.6156e-02, -2.4951e-01, -8.3849e-02,\n",
      "         -1.0904e-02,  7.8214e-02,  1.2295e-01, -2.9220e-01, -3.0860e-01,\n",
      "         -4.1762e-01, -2.4306e-01, -7.4294e-01, -7.8055e-01],\n",
      "        [ 1.7585e+00, -1.1100e-01,  1.1096e-01, -5.3976e-01, -5.6439e-02,\n",
      "          4.5175e-01, -5.0695e-02,  6.1858e-02, -2.8010e-01, -8.7344e-02,\n",
      "          4.4708e-03,  4.5512e-02,  8.0383e-02, -2.0546e-01, -3.0104e-01,\n",
      "         -4.1840e-01, -2.5647e-01, -7.5354e-01, -7.3928e-01],\n",
      "        [ 1.7639e+00, -1.9176e-01,  2.2118e-01, -5.9537e-01, -3.2868e-02,\n",
      "          4.6144e-01,  3.2290e-03,  5.1017e-02, -2.0735e-01, -1.1830e-01,\n",
      "         -3.6515e-02,  1.3249e-01,  1.4360e-01, -2.0694e-01, -2.4849e-01,\n",
      "         -3.7257e-01, -2.9254e-01, -6.8443e-01, -7.1014e-01],\n",
      "        [ 1.8458e+00, -1.7590e-01,  2.8520e-01, -5.1259e-01, -2.8221e-02,\n",
      "          5.3153e-01, -7.5278e-03, -1.6780e-02, -3.1574e-01, -1.0227e-01,\n",
      "          3.2874e-02,  6.8570e-02,  3.8101e-02, -1.7789e-01, -3.0980e-01,\n",
      "         -4.1758e-01, -3.2854e-01, -7.8301e-01, -7.3860e-01]], device='cuda:0'), 'label': tensor([12, 10, 14,  1,  9,  4, 10, 12, 11,  4,  4,  5,  0,  9,  4,  6],\n",
      "       device='cuda:0')}, {'logits': tensor([[ 1.9230, -0.1399,  0.1817, -0.5706, -0.0060,  0.5246, -0.0363,  0.0339,\n",
      "         -0.3082, -0.0721, -0.0169,  0.1711,  0.0346, -0.3217, -0.2473, -0.3946,\n",
      "         -0.3801, -0.7683, -0.7312],\n",
      "        [ 1.8320, -0.1102,  0.2524, -0.5834, -0.1338,  0.5847, -0.1088,  0.0226,\n",
      "         -0.2822, -0.0659,  0.0400,  0.1032,  0.1118, -0.2508, -0.2901, -0.4530,\n",
      "         -0.3375, -0.7446, -0.7546],\n",
      "        [ 1.8639, -0.0909,  0.1869, -0.5566, -0.0739,  0.5865, -0.1007,  0.0076,\n",
      "         -0.2594, -0.0458,  0.0380,  0.0880,  0.0564, -0.2480, -0.3196, -0.4151,\n",
      "         -0.3673, -0.7867, -0.7484],\n",
      "        [ 1.7950, -0.1027,  0.2261, -0.4806, -0.0425,  0.5061, -0.0049, -0.0390,\n",
      "         -0.2861, -0.1653,  0.0199,  0.0677,  0.1477, -0.2211, -0.3736, -0.3876,\n",
      "         -0.3556, -0.7138, -0.7258],\n",
      "        [ 1.9179, -0.0249,  0.2571, -0.5969, -0.0453,  0.5297, -0.0516, -0.0194,\n",
      "         -0.2602, -0.0564, -0.0040,  0.0471,  0.0822, -0.2750, -0.3797, -0.4768,\n",
      "         -0.3670, -0.7660, -0.7803],\n",
      "        [ 1.8915, -0.1041,  0.1704, -0.6467, -0.0861,  0.4968, -0.0549,  0.0611,\n",
      "         -0.2526, -0.0682,  0.0162,  0.1138,  0.0834, -0.2418, -0.4203, -0.4333,\n",
      "         -0.3134, -0.7042, -0.8009],\n",
      "        [ 1.8540, -0.1001,  0.1884, -0.5668, -0.0293,  0.5367, -0.0128, -0.0431,\n",
      "         -0.3017, -0.1160,  0.0331,  0.0678,  0.0602, -0.2053, -0.3409, -0.4623,\n",
      "         -0.2808, -0.6832, -0.7087],\n",
      "        [ 1.8934, -0.0686,  0.2433, -0.5673, -0.0070,  0.5329,  0.0316,  0.0527,\n",
      "         -0.2868, -0.1295,  0.0123,  0.0708,  0.0387, -0.2167, -0.2729, -0.4496,\n",
      "         -0.3609, -0.7207, -0.7135],\n",
      "        [ 1.8624, -0.0806,  0.2060, -0.6717, -0.0636,  0.5227,  0.0030, -0.0631,\n",
      "         -0.2435, -0.0691,  0.0116,  0.1164,  0.0548, -0.2757, -0.3775, -0.4642,\n",
      "         -0.3710, -0.7687, -0.7553],\n",
      "        [ 1.8678, -0.0372,  0.2242, -0.4759, -0.0681,  0.5129, -0.0376,  0.0087,\n",
      "         -0.3014, -0.1264,  0.0538,  0.0488,  0.0469, -0.2420, -0.3448, -0.3902,\n",
      "         -0.4187, -0.6986, -0.7241],\n",
      "        [ 1.8262, -0.1334,  0.1802, -0.5092, -0.0527,  0.5628, -0.0095,  0.0292,\n",
      "         -0.2809, -0.1387,  0.0025,  0.0938,  0.1221, -0.2097, -0.3257, -0.4799,\n",
      "         -0.3324, -0.7168, -0.7150],\n",
      "        [ 1.9076, -0.1810,  0.2782, -0.5603, -0.0562,  0.5126,  0.0243, -0.0641,\n",
      "         -0.2354, -0.1134, -0.0301,  0.1031,  0.1009, -0.2472, -0.3314, -0.3618,\n",
      "         -0.3145, -0.8683, -0.7698],\n",
      "        [ 1.8766, -0.0977,  0.2621, -0.5712, -0.0284,  0.5593, -0.0315,  0.0549,\n",
      "         -0.2305, -0.1136,  0.0072,  0.0562,  0.1178, -0.2612, -0.2922, -0.4207,\n",
      "         -0.3709, -0.7791, -0.7286],\n",
      "        [ 1.8944, -0.1145,  0.2688, -0.5534, -0.0460,  0.5474, -0.0178,  0.0156,\n",
      "         -0.2544, -0.1341,  0.0405,  0.0452,  0.0847, -0.2761, -0.3075, -0.4124,\n",
      "         -0.4477, -0.7635, -0.7044],\n",
      "        [ 1.7096, -0.0896,  0.1558, -0.5286,  0.0132,  0.5260,  0.0305,  0.0194,\n",
      "         -0.2820, -0.1268,  0.0190,  0.1657,  0.1282, -0.2358, -0.2718, -0.4595,\n",
      "         -0.2659, -0.6993, -0.7723],\n",
      "        [ 1.8776, -0.0744,  0.2207, -0.5734, -0.0337,  0.5270, -0.0024, -0.0486,\n",
      "         -0.2364, -0.1457,  0.0087,  0.1222,  0.1145, -0.2407, -0.3404, -0.4967,\n",
      "         -0.3022, -0.7320, -0.7409]], device='cuda:0'), 'label': tensor([ 1, 14, 14, 10, 11,  0,  4,  0,  2,  0,  7,  8,  9,  9, 12, 14],\n",
      "       device='cuda:0')}, {'logits': tensor([[ 1.7835e+00, -2.7969e-02,  2.0299e-01, -6.2594e-01, -4.6990e-02,\n",
      "          4.7436e-01,  4.6887e-02, -8.6563e-02, -2.3739e-01, -1.9131e-01,\n",
      "         -2.0058e-02,  9.5939e-02,  2.1766e-02, -2.4047e-01, -2.8406e-01,\n",
      "         -3.6132e-01, -3.1072e-01, -7.3863e-01, -6.9186e-01],\n",
      "        [ 1.6843e+00, -9.7511e-02,  2.4964e-01, -4.3617e-01, -3.8423e-02,\n",
      "          5.2078e-01,  1.0125e-02,  1.0723e-01, -2.1775e-01, -1.6367e-01,\n",
      "          5.4539e-02,  6.5160e-02,  1.6336e-02, -2.7888e-01, -2.2443e-01,\n",
      "         -4.0546e-01, -2.8031e-01, -5.5754e-01, -6.6541e-01],\n",
      "        [ 1.8380e+00, -7.2223e-02,  2.1171e-01, -6.1207e-01, -5.2549e-02,\n",
      "          5.1335e-01, -2.0300e-02,  3.7607e-03, -3.2251e-01, -9.9906e-02,\n",
      "          5.9784e-02,  1.1794e-01,  1.0296e-01, -2.0404e-01, -3.2219e-01,\n",
      "         -4.0040e-01, -3.4670e-01, -6.9933e-01, -7.2944e-01],\n",
      "        [ 1.7905e+00, -5.8012e-02,  2.0704e-01, -5.3764e-01, -6.3427e-02,\n",
      "          4.8603e-01, -4.8504e-02,  8.4551e-02, -2.7553e-01, -1.1700e-01,\n",
      "          4.8146e-02,  2.1021e-02,  9.9108e-02, -2.6937e-01, -2.7414e-01,\n",
      "         -4.0205e-01, -3.7587e-01, -6.5738e-01, -7.2322e-01],\n",
      "        [ 1.8443e+00, -1.2789e-01,  2.2563e-01, -6.1639e-01, -7.7074e-02,\n",
      "          6.4673e-01, -5.0164e-02, -1.3802e-02, -2.4687e-01, -1.1886e-01,\n",
      "          7.6982e-03,  1.3151e-01,  6.6459e-02, -3.1516e-01, -3.8057e-01,\n",
      "         -4.5572e-01, -3.8246e-01, -7.8065e-01, -6.1397e-01],\n",
      "        [ 1.8462e+00, -5.5317e-02,  1.6165e-01, -6.0973e-01, -5.4058e-02,\n",
      "          5.6905e-01, -1.1100e-02,  1.1646e-02, -2.4558e-01, -1.0998e-01,\n",
      "          5.5104e-02,  1.9182e-01,  1.2611e-01, -2.7480e-01, -3.5778e-01,\n",
      "         -4.1763e-01, -3.3390e-01, -7.5537e-01, -7.8524e-01],\n",
      "        [ 1.9210e+00, -1.0488e-01,  1.5858e-01, -6.4413e-01, -1.1478e-02,\n",
      "          5.4736e-01,  2.2719e-02,  1.2975e-02, -2.9117e-01, -1.6013e-01,\n",
      "         -3.0306e-02,  1.0748e-01,  1.3643e-01, -2.5659e-01, -3.1110e-01,\n",
      "         -4.3058e-01, -3.9290e-01, -6.9514e-01, -7.6888e-01],\n",
      "        [ 1.7687e+00, -1.0981e-01,  1.7747e-01, -5.5741e-01, -1.0980e-02,\n",
      "          4.6289e-01,  2.5676e-02, -9.1824e-04, -2.1501e-01, -5.8963e-02,\n",
      "          7.6283e-02,  1.5321e-01,  1.0916e-01, -2.3033e-01, -2.8042e-01,\n",
      "         -4.1116e-01, -2.5696e-01, -7.5442e-01, -7.5885e-01],\n",
      "        [ 1.8188e+00, -6.0239e-02,  1.5030e-01, -5.8707e-01, -7.1527e-02,\n",
      "          4.8816e-01,  5.8061e-02,  1.5703e-02, -3.4292e-01, -1.2161e-01,\n",
      "          5.7874e-03,  1.3663e-01,  8.2558e-02, -2.6372e-01, -2.9236e-01,\n",
      "         -4.5240e-01, -2.8016e-01, -7.2699e-01, -7.5632e-01],\n",
      "        [ 1.8334e+00, -1.0292e-01,  2.0887e-01, -5.5754e-01, -1.3336e-01,\n",
      "          5.3470e-01, -1.9486e-02,  4.9096e-02, -2.9203e-01, -1.5000e-01,\n",
      "          2.7706e-03,  7.2054e-02,  1.8776e-02, -2.9521e-01, -3.1027e-01,\n",
      "         -4.1245e-01, -3.3983e-01, -7.0880e-01, -6.8397e-01],\n",
      "        [ 1.8536e+00, -6.8754e-02,  2.3394e-01, -6.0352e-01, -1.0541e-02,\n",
      "          5.8257e-01,  2.2427e-02, -4.3223e-02, -2.4164e-01, -8.8141e-02,\n",
      "         -5.6441e-02,  1.3825e-01,  2.4279e-02, -2.5747e-01, -3.3997e-01,\n",
      "         -4.1447e-01, -3.8382e-01, -7.6651e-01, -7.4104e-01],\n",
      "        [ 1.8511e+00, -5.8735e-02,  1.6190e-01, -5.0425e-01,  1.1273e-02,\n",
      "          5.3406e-01, -1.1572e-02, -3.0774e-02, -3.0366e-01, -1.6551e-01,\n",
      "         -1.3435e-02,  1.4418e-01,  7.9872e-02, -2.2399e-01, -3.2492e-01,\n",
      "         -4.9031e-01, -3.5875e-01, -7.5503e-01, -7.6408e-01],\n",
      "        [ 1.8535e+00, -1.0214e-01,  2.5641e-01, -5.9303e-01, -1.2414e-02,\n",
      "          5.5036e-01, -4.4883e-02, -6.3308e-02, -3.2204e-01, -1.4676e-01,\n",
      "         -1.6586e-02,  1.2429e-01,  1.2670e-01, -3.0909e-01, -3.7340e-01,\n",
      "         -4.2875e-01, -3.5665e-01, -7.6164e-01, -7.4468e-01],\n",
      "        [ 1.8349e+00, -5.9149e-02,  1.0704e-01, -6.4254e-01, -4.2921e-02,\n",
      "          4.6213e-01, -4.5821e-02,  4.6439e-02, -2.7623e-01, -1.1027e-01,\n",
      "         -6.6701e-02,  7.6167e-02,  9.7588e-02, -2.6251e-01, -3.1554e-01,\n",
      "         -4.8349e-01, -2.8935e-01, -7.4070e-01, -7.0629e-01],\n",
      "        [ 1.7666e+00, -1.1549e-01,  1.9993e-01, -4.2168e-01, -3.2895e-02,\n",
      "          5.1775e-01,  5.5869e-02,  1.9912e-02, -1.8605e-01, -6.8431e-02,\n",
      "          3.6077e-03,  9.2922e-02,  1.1153e-01, -2.7893e-01, -2.9500e-01,\n",
      "         -4.0544e-01, -3.1642e-01, -7.4866e-01, -7.1898e-01],\n",
      "        [ 1.8832e+00, -1.2759e-01,  2.0577e-01, -5.4348e-01, -8.5979e-02,\n",
      "          5.6415e-01, -1.6859e-02,  3.0347e-02, -3.0700e-01, -2.6751e-02,\n",
      "         -2.9801e-03,  1.4711e-01,  1.5344e-01, -2.2309e-01, -3.0384e-01,\n",
      "         -4.0450e-01, -2.7269e-01, -7.6340e-01, -8.0049e-01]], device='cuda:0'), 'label': tensor([ 1,  9,  0,  7,  0,  6,  0,  9,  1, 14, 14,  1,  4,  0,  9, 11],\n",
      "       device='cuda:0')}, {'logits': tensor([[ 1.7903e+00, -1.0895e-01,  1.6276e-01, -6.1708e-01, -2.4288e-02,\n",
      "          5.2136e-01,  4.2539e-02, -1.7599e-02, -2.3485e-01, -1.3317e-01,\n",
      "         -4.7574e-02,  1.1990e-01,  1.4270e-01, -2.5806e-01, -3.2672e-01,\n",
      "         -4.9570e-01, -2.9190e-01, -7.6373e-01, -7.8436e-01],\n",
      "        [ 1.8989e+00, -7.4195e-02,  1.5510e-01, -5.5875e-01, -2.0658e-01,\n",
      "          5.5131e-01, -1.0388e-01,  5.5411e-02, -2.9160e-01, -3.4434e-02,\n",
      "          6.5548e-03,  6.1364e-02,  8.7127e-02, -2.6692e-01, -3.6669e-01,\n",
      "         -4.7671e-01, -3.1390e-01, -7.4438e-01, -7.5172e-01],\n",
      "        [ 1.8472e+00, -3.5070e-02,  1.7749e-01, -5.9091e-01, -3.7136e-02,\n",
      "          5.0286e-01,  1.3142e-02,  4.5532e-02, -2.4890e-01, -1.1667e-01,\n",
      "          2.3641e-02,  1.0187e-01,  1.7993e-01, -2.8606e-01, -3.8664e-01,\n",
      "         -5.0220e-01, -3.2929e-01, -7.8879e-01, -7.4086e-01],\n",
      "        [ 1.8923e+00, -5.7120e-02,  1.7232e-01, -5.8090e-01,  2.0187e-02,\n",
      "          5.7147e-01, -1.5514e-02,  4.1148e-03, -2.5283e-01, -1.0311e-01,\n",
      "         -4.7264e-02,  1.6429e-01,  8.5819e-02, -2.8636e-01, -3.8962e-01,\n",
      "         -4.3010e-01, -4.0926e-01, -7.7774e-01, -7.6506e-01],\n",
      "        [ 1.8866e+00, -2.2991e-02,  2.1736e-01, -6.2918e-01, -4.1832e-02,\n",
      "          4.9377e-01, -1.8762e-02,  1.1747e-02, -2.9756e-01, -8.3835e-02,\n",
      "          3.3804e-02,  1.1801e-01,  1.0842e-01, -2.3093e-01, -2.3920e-01,\n",
      "         -4.3163e-01, -3.8598e-01, -7.1522e-01, -7.5545e-01],\n",
      "        [ 1.8266e+00, -8.5649e-02,  1.5193e-01, -6.3472e-01, -1.4815e-01,\n",
      "          5.3461e-01,  1.6300e-02,  8.3624e-02, -2.4647e-01, -5.4825e-02,\n",
      "          3.3539e-02,  1.3467e-01,  1.8015e-01, -2.6873e-01, -3.7210e-01,\n",
      "         -4.1945e-01, -2.8370e-01, -7.8091e-01, -7.8079e-01],\n",
      "        [ 1.8284e+00, -1.3274e-01,  1.7946e-01, -5.8065e-01, -1.4342e-01,\n",
      "          5.0444e-01, -2.9354e-02,  4.4716e-02, -2.7119e-01, -6.2196e-02,\n",
      "         -6.2197e-03,  1.0532e-01,  1.4764e-01, -2.7452e-01, -3.2747e-01,\n",
      "         -4.5651e-01, -2.9163e-01, -7.3725e-01, -7.8555e-01],\n",
      "        [ 1.8658e+00, -6.8692e-02,  1.7731e-01, -6.4385e-01, -4.9523e-02,\n",
      "          5.1170e-01, -3.2035e-02,  1.0243e-02, -2.7392e-01, -1.0724e-01,\n",
      "         -1.6917e-02,  7.4985e-02,  1.5640e-01, -2.5871e-01, -3.0409e-01,\n",
      "         -4.4083e-01, -3.3442e-01, -7.8886e-01, -7.1688e-01],\n",
      "        [ 1.8355e+00, -7.0936e-02,  2.0551e-01, -5.6692e-01, -6.3043e-03,\n",
      "          5.4431e-01, -3.5781e-03,  5.9243e-02, -2.6268e-01, -1.3512e-01,\n",
      "          1.1839e-02,  7.7771e-02,  6.1909e-02, -2.9978e-01, -3.2094e-01,\n",
      "         -4.3115e-01, -3.3490e-01, -7.3276e-01, -7.9307e-01],\n",
      "        [ 1.8832e+00, -2.4972e-02,  2.0012e-01, -6.2007e-01, -7.4969e-02,\n",
      "          6.3128e-01, -3.4829e-04, -6.1633e-02, -3.1428e-01, -1.1806e-01,\n",
      "         -5.5411e-02,  1.6613e-01,  7.8169e-02, -2.8453e-01, -4.0184e-01,\n",
      "         -4.5111e-01, -4.1991e-01, -7.6998e-01, -7.3101e-01],\n",
      "        [ 1.9322e+00, -3.2318e-02,  1.1863e-01, -5.8886e-01,  8.9472e-05,\n",
      "          5.3401e-01, -1.9845e-02,  5.5979e-02, -3.0876e-01, -1.3111e-01,\n",
      "         -1.2875e-02,  1.2264e-01,  8.8892e-02, -2.2282e-01, -3.4869e-01,\n",
      "         -4.7979e-01, -3.1896e-01, -7.1626e-01, -7.3873e-01],\n",
      "        [ 1.8624e+00, -3.1993e-02,  1.7771e-01, -5.8369e-01, -2.1025e-02,\n",
      "          5.8604e-01, -2.0705e-02,  3.1654e-02, -2.6105e-01, -1.2689e-01,\n",
      "         -1.0712e-03,  1.2015e-01,  7.5264e-02, -2.5872e-01, -3.2048e-01,\n",
      "         -4.9685e-01, -3.0692e-01, -6.5297e-01, -6.8259e-01],\n",
      "        [ 1.8940e+00, -6.9539e-02,  1.8956e-01, -5.1732e-01, -9.3813e-02,\n",
      "          5.6132e-01, -4.7540e-02,  6.9473e-02, -2.1128e-01, -7.0537e-02,\n",
      "          8.5530e-02,  1.0381e-01,  1.3566e-01, -2.7169e-01, -3.7758e-01,\n",
      "         -4.6628e-01, -4.0815e-01, -8.1401e-01, -7.8874e-01],\n",
      "        [ 1.8602e+00, -6.0699e-02,  1.5727e-01, -6.2358e-01, -2.8259e-03,\n",
      "          5.2448e-01, -5.1799e-02, -2.2967e-03, -2.5834e-01, -1.2305e-01,\n",
      "         -5.1661e-02,  1.4734e-01,  4.0915e-02, -2.8901e-01, -3.6886e-01,\n",
      "         -5.3344e-01, -3.9746e-01, -7.5163e-01, -7.2866e-01],\n",
      "        [ 1.9664e+00, -7.4112e-02,  2.2725e-01, -6.5795e-01, -5.1325e-02,\n",
      "          6.3773e-01, -5.7296e-02, -2.8058e-03, -2.5529e-01, -1.5267e-02,\n",
      "         -1.5274e-02,  1.6677e-01,  7.1330e-02, -3.0279e-01, -3.7376e-01,\n",
      "         -4.2822e-01, -4.2869e-01, -8.0923e-01, -7.5428e-01],\n",
      "        [ 1.8314e+00, -1.2130e-01,  1.5210e-01, -5.8658e-01, -1.0018e-01,\n",
      "          4.5879e-01,  2.2338e-02, -1.4123e-02, -2.7196e-01, -1.1219e-01,\n",
      "         -1.7360e-02,  1.3929e-01,  1.1038e-01, -2.2717e-01, -3.1907e-01,\n",
      "         -4.9150e-01, -2.7526e-01, -7.2687e-01, -7.6576e-01]], device='cuda:0'), 'label': tensor([ 6,  1,  0,  5,  0,  1,  1, 11,  0,  0,  7,  1,  0, 11,  0, 12],\n",
      "       device='cuda:0')}, {'logits': tensor([[ 1.8631e+00, -1.6617e-01,  1.9975e-01, -5.6701e-01, -9.0845e-03,\n",
      "          5.5725e-01,  2.2718e-02,  2.4680e-02, -2.8389e-01, -8.2011e-02,\n",
      "          3.3576e-02,  1.4963e-01,  1.0995e-01, -2.3261e-01, -3.0066e-01,\n",
      "         -4.5653e-01, -3.3917e-01, -6.8610e-01, -7.4629e-01],\n",
      "        [ 2.0182e+00, -6.4741e-02,  2.6147e-01, -5.9291e-01, -6.5507e-02,\n",
      "          5.5518e-01,  2.2826e-02,  5.6410e-02, -2.8647e-01, -6.2817e-02,\n",
      "          1.1942e-02,  1.4831e-01,  4.3702e-02, -2.3463e-01, -3.8010e-01,\n",
      "         -3.9390e-01, -4.1193e-01, -7.3838e-01, -7.8925e-01],\n",
      "        [ 1.8714e+00, -8.3564e-02,  1.8155e-01, -5.5878e-01, -6.7745e-02,\n",
      "          5.3232e-01,  3.8190e-03,  2.8802e-03, -2.5833e-01, -1.0247e-01,\n",
      "          2.4814e-02,  5.0155e-02,  1.2552e-01, -2.6232e-01, -3.3599e-01,\n",
      "         -4.5344e-01, -3.1772e-01, -6.8875e-01, -7.6786e-01],\n",
      "        [ 1.7914e+00, -8.0179e-02,  1.7473e-01, -5.8104e-01, -6.7155e-02,\n",
      "          5.0637e-01, -3.0105e-02, -2.5732e-02, -2.9824e-01, -9.5727e-02,\n",
      "         -1.5858e-02,  9.0065e-02,  1.0918e-01, -2.1629e-01, -3.8630e-01,\n",
      "         -4.7433e-01, -3.4371e-01, -7.8543e-01, -7.5690e-01],\n",
      "        [ 1.9330e+00, -9.4669e-02,  2.1542e-01, -5.9883e-01, -3.1022e-02,\n",
      "          5.6239e-01, -1.0877e-02, -1.7287e-02, -2.2826e-01, -2.6203e-02,\n",
      "         -4.7530e-02,  1.5395e-01,  1.1486e-01, -2.2490e-01, -4.0334e-01,\n",
      "         -4.1166e-01, -3.3033e-01, -8.2573e-01, -7.6115e-01],\n",
      "        [ 1.8625e+00, -1.6709e-01,  1.7867e-01, -5.7966e-01, -9.1344e-02,\n",
      "          6.0387e-01, -7.5065e-03, -1.7638e-03, -2.9753e-01, -1.0078e-01,\n",
      "          1.4739e-02,  1.4697e-01,  1.2577e-01, -2.8607e-01, -3.8100e-01,\n",
      "         -4.2258e-01, -3.1944e-01, -7.2777e-01, -7.6961e-01],\n",
      "        [ 1.8997e+00, -9.7207e-02,  1.8396e-01, -5.9590e-01, -5.9093e-02,\n",
      "          4.8237e-01, -4.3270e-02, -8.3904e-03, -2.8725e-01, -1.1252e-01,\n",
      "          2.3207e-02,  1.5129e-01,  1.0318e-01, -2.2338e-01, -3.4756e-01,\n",
      "         -4.4136e-01, -3.4988e-01, -7.0387e-01, -7.9132e-01],\n",
      "        [ 1.8688e+00, -3.4989e-02,  1.5977e-01, -5.8479e-01, -1.0759e-01,\n",
      "          6.3972e-01, -7.4993e-02, -3.6481e-03, -2.2665e-01, -9.2363e-02,\n",
      "          9.0826e-03,  1.8252e-01,  3.6668e-02, -2.5881e-01, -4.2706e-01,\n",
      "         -4.2575e-01, -3.9546e-01, -7.5824e-01, -7.5672e-01],\n",
      "        [ 1.9255e+00, -1.5854e-01,  2.2432e-01, -6.2147e-01, -1.2971e-01,\n",
      "          5.6066e-01, -1.4526e-02,  3.8486e-02, -2.7175e-01, -8.4680e-02,\n",
      "          1.5086e-02,  1.0776e-01,  1.1094e-01, -3.0548e-01, -3.8531e-01,\n",
      "         -3.5900e-01, -3.7320e-01, -7.5894e-01, -7.9675e-01],\n",
      "        [ 1.8820e+00, -4.7673e-02,  2.3877e-01, -5.2944e-01, -1.1027e-01,\n",
      "          5.2092e-01, -2.5555e-02, -3.4726e-02, -2.6866e-01, -1.0702e-01,\n",
      "         -3.9559e-02,  5.0976e-02,  7.8418e-02, -2.6076e-01, -3.3226e-01,\n",
      "         -4.0687e-01, -3.4623e-01, -8.3547e-01, -7.4953e-01],\n",
      "        [ 1.8525e+00, -8.1538e-02,  1.9628e-01, -6.3440e-01, -4.3867e-02,\n",
      "          5.7199e-01,  7.2178e-03, -3.5908e-02, -1.9832e-01, -1.1847e-01,\n",
      "         -4.3494e-02,  1.1984e-01,  6.5407e-02, -3.2668e-01, -3.4110e-01,\n",
      "         -4.3735e-01, -3.9478e-01, -8.0608e-01, -7.2380e-01],\n",
      "        [ 1.7708e+00,  1.3852e-02,  1.3492e-01, -6.3111e-01, -4.3682e-02,\n",
      "          5.2697e-01, -1.8675e-02,  1.6311e-03, -2.9660e-01, -1.3087e-01,\n",
      "         -2.1804e-02,  1.4318e-01,  1.1732e-01, -2.1542e-01, -3.7029e-01,\n",
      "         -5.1514e-01, -3.3935e-01, -7.3445e-01, -7.5459e-01],\n",
      "        [ 1.7755e+00, -6.7736e-02,  1.9943e-01, -4.8059e-01,  1.2450e-02,\n",
      "          5.6364e-01,  9.3799e-02, -3.5373e-02, -2.8243e-01, -1.1520e-01,\n",
      "         -5.2712e-02,  1.3690e-01,  1.9260e-01, -2.4169e-01, -3.3382e-01,\n",
      "         -4.3526e-01, -2.8062e-01, -7.2594e-01, -7.8619e-01],\n",
      "        [ 1.7997e+00, -2.0084e-02,  1.6157e-01, -5.9064e-01, -3.6925e-03,\n",
      "          5.0724e-01,  4.5507e-02,  3.9973e-02, -2.8095e-01, -1.1759e-01,\n",
      "         -5.4514e-02,  9.4003e-02,  6.8910e-02, -2.3507e-01, -3.2085e-01,\n",
      "         -5.1963e-01, -2.9282e-01, -7.8821e-01, -7.3028e-01],\n",
      "        [ 1.8595e+00, -1.1038e-01,  1.7887e-01, -5.7469e-01, -4.7163e-02,\n",
      "          5.2671e-01, -1.1649e-03, -2.3707e-02, -2.6333e-01, -1.0974e-01,\n",
      "          1.5083e-02,  5.3892e-02,  1.3472e-01, -2.6232e-01, -3.2754e-01,\n",
      "         -4.3468e-01, -3.2248e-01, -6.9045e-01, -7.7714e-01],\n",
      "        [ 1.9092e+00, -8.1178e-02,  2.2033e-01, -5.6047e-01, -4.0984e-02,\n",
      "          5.5186e-01, -8.4138e-03, -4.4404e-02, -2.6878e-01, -5.5801e-02,\n",
      "          9.4201e-03,  5.2204e-02,  1.1444e-01, -2.7631e-01, -3.9262e-01,\n",
      "         -4.7227e-01, -3.8513e-01, -8.4060e-01, -7.2041e-01]], device='cuda:0'), 'label': tensor([ 4,  0,  0, 12,  0, 11,  0,  0,  9, 12,  6,  0, 16, 16,  0,  7],\n",
      "       device='cuda:0')}]\n"
     ]
    },
    {
     "ename": "NeptunePossibleLegacyUsageException",
     "evalue": "\n\n----NeptunePossibleLegacyUsageException----------------------------------------------------------------\n\nIt seems you are trying to use the legacy API, but you imported the new one.\n\nSimply update your import statement to:\n    import neptune\n\nYou may want to check the legacy API docs:\n    - https://docs-legacy.neptune.ai\n\nIf you want to update your code with the new API, we prepared a handy migration guide:\n    - https://docs.neptune.ai/about/legacy/#migrating-to-neptunenew\n\nYou can read more about neptune.new in the release blog post:\n    - https://neptune.ai/blog/neptune-new\n\nYou may also want to check the following docs page:\n    - https://docs-legacy.neptune.ai/getting-started/integrate-neptune-into-your-codebase.html\n\nNeed help?-> https://docs.neptune.ai/getting_help\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNeptunePossibleLegacyUsageException\u001B[0m       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 63\u001B[0m\n\u001B[0;32m     59\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo official scorer found\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;66;03m# logger.experiment.stop()\u001B[39;00m\n\u001B[1;32m---> 63\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# else:\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m#     logger.experiment.stop()\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[19], line 49\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# 训练 & 测试\u001B[39;00m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m🚀 开始训练\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 49\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ 训练完成\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     51\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtest(classifier)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 539\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[1;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[0;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    569\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[0;32m    571\u001B[0m     ckpt_path,\n\u001B[0;32m    572\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    573\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    574\u001B[0m )\n\u001B[1;32m--> 575\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    577\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    578\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\trainer\\trainer.py:982\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m    977\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[0;32m    979\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[0;32m    981\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m--> 982\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    985\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[0;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m    987\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1026\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1024\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[0;32m   1025\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m-> 1026\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1027\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1028\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[1;32m--> 216\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    454\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 455\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:151\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(data_fetcher)\n\u001B[1;32m--> 151\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_advance_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    153\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:370\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.on_advance_end\u001B[1;34m(self, data_fetcher)\u001B[0m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_accumulate():\n\u001B[0;32m    367\u001B[0m     \u001B[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001B[39;00m\n\u001B[0;32m    368\u001B[0m     call\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_validation_model_zero_grad\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 370\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    371\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    372\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_logger_connector\u001B[38;5;241m.\u001B[39m_first_loop_iter \u001B[38;5;241m=\u001B[39m first_loop_iter\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\loops\\utilities.py:179\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    177\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[1;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:151\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    149\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_iteration_done()\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_store_dataloader_outputs()\n\u001B[1;32m--> 151\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_run_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:291\u001B[0m, in \u001B[0;36m_EvaluationLoop.on_run_end\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_logger_connector\u001B[38;5;241m.\u001B[39m_evaluation_epoch_end()\n\u001B[0;32m    290\u001B[0m \u001B[38;5;66;03m# hook\u001B[39;00m\n\u001B[1;32m--> 291\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_on_evaluation_epoch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    293\u001B[0m logged_outputs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logged_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logged_outputs, []  \u001B[38;5;66;03m# free memory\u001B[39;00m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;66;03m# include any logged outputs on epoch_end\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:371\u001B[0m, in \u001B[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    369\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_test_epoch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_validation_epoch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    370\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(trainer, hook_name)\n\u001B[1;32m--> 371\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    373\u001B[0m trainer\u001B[38;5;241m.\u001B[39m_logger_connector\u001B[38;5;241m.\u001B[39mon_epoch_end()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\trainer\\call.py:171\u001B[0m, in \u001B[0;36m_call_lightning_module_hook\u001B[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    168\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 171\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m    174\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "Cell \u001B[1;32mIn[10], line 196\u001B[0m, in \u001B[0;36mBaseClassifier.on_validation_epoch_end\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    193\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[WARNING] No validation outputs found. Skipping eval.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 196\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__eval_epoch_end\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mval\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidation_step_outputs\u001B[38;5;241m.\u001B[39mclear()\n",
      "Cell \u001B[1;32mIn[10], line 164\u001B[0m, in \u001B[0;36mBaseClassifier.__eval_epoch_end\u001B[1;34m(self, epoch_type, outputs)\u001B[0m\n\u001B[0;32m    161\u001B[0m logits \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m outputs])\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[0;32m    162\u001B[0m label \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m outputs])\u001B[38;5;241m.\u001B[39mcpu()\n\u001B[1;32m--> 164\u001B[0m logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;66;03m# 记录日志\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m logs\u001B[38;5;241m.\u001B[39mitems():\n",
      "Cell \u001B[1;32mIn[10], line 290\u001B[0m, in \u001B[0;36mStandardClassifier.log_metrics\u001B[1;34m(self, epoch_type, logits, label)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_proposed_answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogits_to_label(logits)\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m--> 290\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__log_precision_recall_curve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mlog_metrics(epoch_type, logits, label)\n",
      "Cell \u001B[1;32mIn[10], line 320\u001B[0m, in \u001B[0;36mStandardClassifier.__log_precision_recall_curve\u001B[1;34m(self, epoch_type, logits, label)\u001B[0m\n\u001B[0;32m    317\u001B[0m ax\u001B[38;5;241m.\u001B[39mset_ylabel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrecision\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    318\u001B[0m ax\u001B[38;5;241m.\u001B[39mlegend()\n\u001B[1;32m--> 320\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexperiment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_image\u001B[49m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_pre_rec_curve\u001B[39m\u001B[38;5;124m\"\u001B[39m, fig)\n\u001B[0;32m    321\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mexperiment\u001B[38;5;241m.\u001B[39mlog_metric(\n\u001B[0;32m    322\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_average_precision_score_micro\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    323\u001B[0m     average_precision_score(label, logits, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmicro\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    324\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\neptune\\metadata_containers\\metadata_container.py:324\u001B[0m, in \u001B[0;36mMetadataContainer.__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getattr__\u001B[39m(\u001B[38;5;28mself\u001B[39m, item):\n\u001B[0;32m    323\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLEGACY_METHODS:\n\u001B[1;32m--> 324\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NeptunePossibleLegacyUsageException()\n\u001B[0;32m    325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNeptunePossibleLegacyUsageException\u001B[0m: \n\n----NeptunePossibleLegacyUsageException----------------------------------------------------------------\n\nIt seems you are trying to use the legacy API, but you imported the new one.\n\nSimply update your import statement to:\n    import neptune\n\nYou may want to check the legacy API docs:\n    - https://docs-legacy.neptune.ai\n\nIf you want to update your code with the new API, we prepared a handy migration guide:\n    - https://docs.neptune.ai/about/legacy/#migrating-to-neptunenew\n\nYou can read more about neptune.new in the release blog post:\n    - https://neptune.ai/blog/neptune-new\n\nYou may also want to check the following docs page:\n    - https://docs-legacy.neptune.ai/getting-started/integrate-neptune-into-your-codebase.html\n\nNeed help?-> https://docs.neptune.ai/getting_help\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x1000 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning import Trainer as LightningTrainer, LightningModule\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "\n",
    "logger = NeptuneLogger(\n",
    "    api_key= \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ==\",\n",
    "    project=\"Violetta/BERT\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    for i in range(4):\n",
    "        print(f\"--------- EXPERIMENT {i} ---------\")\n",
    "\n",
    "        # 释放显存，防止 OOM\n",
    "        classifier = trainer = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # 创建 Trainer\n",
    "        trainer = LightningTrainer(\n",
    "            accelerator=\"gpu\",\n",
    "            devices=[0],\n",
    "            max_epochs=3,\n",
    "            default_root_dir=CHECKPOINT_DIR,\n",
    "            deterministic=False,\n",
    "            enable_checkpointing=True,\n",
    "            logger=logger,\n",
    "            check_val_every_n_epoch=1,\n",
    "            log_every_n_steps=1,  # 确保 step 递增，防止 Neptune step 乱序\n",
    "            num_sanity_val_steps=0, # 禁用 Sanity Check 直接进入训练\n",
    "        )\n",
    "\n",
    "        # 直接调用 StandardClassifier\n",
    "        classifier = StandardClassifier(\n",
    "            # pretrained_language_model=\"bert-base-uncased\",\n",
    "            pretrained_language_model=PRETRAINED_MODEL,\n",
    "            dataset_name=DATASET_NAME,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            decay_lr_speed=LEARNING_RATE_DECAY_SPEED,\n",
    "            dropout_p=DROPOUT_P,\n",
    "            activation_function=ACTIVATION_FUNCTION,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            linear_size=LINEAR_SIZE,\n",
    "        )\n",
    "\n",
    "        # 训练 & 测试\n",
    "        print(\"🚀 开始训练\")\n",
    "        trainer.fit(classifier)\n",
    "        print(\"✅ 训练完成\")\n",
    "        trainer.test(classifier)\n",
    "        \n",
    "        scorer = get_official_scorer(i, logger)\n",
    "        if scorer:\n",
    "            scorer.score({\n",
    "                \"standard\": classifier.test_proposed_answer,\n",
    "            })\n",
    "        else:\n",
    "            print(\"No official scorer found\")\n",
    "\n",
    "except Exception as e:\n",
    "    # logger.experiment.stop()\n",
    "    raise e\n",
    "\n",
    "# else:\n",
    "#     logger.experiment.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T14:19:20.368235100Z",
     "start_time": "2025-02-26T14:19:16.533742500Z"
    }
   },
   "id": "71c199a7a71853e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training binary classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:01:44.039562500Z",
     "start_time": "2025-02-24T11:01:44.015594500Z"
    }
   },
   "id": "bf76454b093fd153"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GPUS = 1\n",
    "\n",
    "BIN_MIN_EPOCHS = BIN_MAX_EPOCHS = 2\n",
    "\n",
    "BIN_BATCH_SIZE = 32\n",
    "BIN_LEARNING_RATE = 2e-05\n",
    "BIN_LEARNING_RATE_DECAY_SPEED = [1, 1, 0.75, 0.5, 0.5, 0.25, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01]\n",
    "\n",
    "BIN_LINEAR_SIZE = 256\n",
    "\n",
    "BIN_DROPOUT_P = 0.2\n",
    "BIN_ACTIVATION_FUNCTION = \"PReLU\"\n",
    "BIN_WEIGHT_DECAY = 0.01 # default = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T10:24:01.632633400Z"
    }
   },
   "id": "f8f3bd74a80f772c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bin_logger = NeptuneLogger(\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ==\",\n",
    "    project=\"Violetta/BERT\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    for i in range(4):\n",
    "        print(f\"--------- EXPERIMENT {i} ---------\")\n",
    "    \n",
    "        bin_classifier = bin_trainer = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        bin_trainer = LightningTrainer(\n",
    "            gpus=GPUS,\n",
    "            min_epochs=BIN_MIN_EPOCHS,\n",
    "            max_epochs=BIN_MAX_EPOCHS,\n",
    "            default_root_dir=CHECKPOINT_DIR,\n",
    "            reload_dataloaders_every_epoch=True, # needed as we loop over a file,\n",
    "            deterministic=False,\n",
    "            checkpoint_callback=False,\n",
    "            logger=bin_logger,\n",
    "        )\n",
    "    \n",
    "        bin_classifier = BinaryClassifier(\n",
    "            pretrained_language_model=PRETRAINED_MODEL,\n",
    "            dataset_name=DATASET_NAME,\n",
    "            batch_size=BIN_BATCH_SIZE,\n",
    "            learning_rate=BIN_LEARNING_RATE,\n",
    "            decay_lr_speed=BIN_LEARNING_RATE_DECAY_SPEED,\n",
    "            linear_size=BIN_LINEAR_SIZE,\n",
    "            dropout_p=BIN_DROPOUT_P,\n",
    "            activation_function=BIN_ACTIVATION_FUNCTION,\n",
    "            weight_decay=BIN_WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        bin_trainer.fit(bin_classifier)\n",
    "        bin_trainer.test(bin_classifier)\n",
    "\n",
    "except Exception as e:\n",
    "    # bin_logger.experiment.stop(str(e))\n",
    "    raise e\n",
    "\n",
    "# else:\n",
    "#     bin_logger.experiment.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-26T10:24:01.640631800Z",
     "start_time": "2025-02-26T10:24:01.633633400Z"
    }
   },
   "id": "d29044fe3ebfa944"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train relation classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:01:54.557382800Z",
     "start_time": "2025-02-24T11:01:54.548478Z"
    }
   },
   "id": "b21cfd3eb95469b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GPUS = 1\n",
    "REL_MIN_EPOCHS = REL_MAX_EPOCHS = 4\n",
    "\n",
    "REL_BATCH_SIZE = 32\n",
    "REL_LEARNING_RATE = 2e-05\n",
    "REL_LEARNING_RATE_DECAY_SPEED = [1, 1, 0.75, 0.5, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01]\n",
    "\n",
    "REL_LINEAR_SIZE = 512\n",
    "\n",
    "REL_DROPOUT_P = 0.1\n",
    "REL_ACTIVATION_FUNCTION = \"PReLU\"\n",
    "REL_WEIGHT_DECAY = 0.01 # default = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T10:24:01.634633300Z"
    }
   },
   "id": "4b3adafc852e475f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "rel_logger = NeptuneLogger(\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ==\",\n",
    "    project=\"Violetta/BERT\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    for i in range(4):\n",
    "        print(f\"--------- EXPERIMENT {i} ---------\")\n",
    "    \n",
    "        rel_classifier = rel_trainer = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        rel_trainer = LightningTrainer(\n",
    "            gpus=GPUS,\n",
    "            min_epochs=REL_MIN_EPOCHS,\n",
    "            max_epochs=REL_MAX_EPOCHS,\n",
    "            default_root_dir=CHECKPOINT_DIR,\n",
    "            reload_dataloaders_every_epoch=True, # needed as we loop over a file,\n",
    "            deterministic=False,\n",
    "            checkpoint_callback=False,\n",
    "            logger=rel_logger\n",
    "        )\n",
    "    \n",
    "        rel_classifier = RelationClassifier(\n",
    "            pretrained_language_model=PRETRAINED_MODEL,\n",
    "            dataset_name=DATASET_NAME,\n",
    "            batch_size=REL_BATCH_SIZE,\n",
    "            learning_rate=REL_LEARNING_RATE,\n",
    "            decay_lr_speed=REL_LEARNING_RATE_DECAY_SPEED,\n",
    "            dropout_p=REL_DROPOUT_P,\n",
    "            activation_function=REL_ACTIVATION_FUNCTION,\n",
    "            weight_decay=REL_WEIGHT_DECAY,\n",
    "            linear_size=REL_LINEAR_SIZE,\n",
    "        )\n",
    "    \n",
    "        rel_trainer.fit(rel_classifier)\n",
    "        rel_trainer.test(rel_classifier)\n",
    "\n",
    "except Exception as e:\n",
    "    # rel_logger.experiment.stop(str(e))\n",
    "    raise e\n",
    "\n",
    "# else:\n",
    "#     rel_logger.experiment.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T10:24:01.634633300Z"
    }
   },
   "id": "d700c80f4cb4dd2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train 2 classifiers independently then test together"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:01:59.637872600Z",
     "start_time": "2025-02-24T11:01:59.633331100Z"
    }
   },
   "id": "dbacc8bcfec24408"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_together(experiment_no: int, logger, b_classifier: BinaryClassifier, r_classifier: RelationClassifier, dataset_name: str = DATASET_NAME,\n",
    "                  bin_batch_size = BIN_BATCH_SIZE, batch_size: int = REL_BATCH_SIZE):\n",
    "    \n",
    "    b_classifier.freeze()\n",
    "    r_classifier.freeze()\n",
    "\n",
    "    true_answer = []\n",
    "\n",
    "    # run binary classifier\n",
    "    print(\"Running binary classifier\")\n",
    "    dataset = GenericDataset(dataset_name, subset=\"test\", batch_size=bin_batch_size, label_transform=\"none\")\n",
    "    binary_classify_results = { criteria: [] for criteria in b_classifier.thresholds.keys() }\n",
    "\n",
    "    for input_data, true_label  in tqdm(dataset.as_batches(), total=len(dataset)):\n",
    "        # append true answers\n",
    "        true_answer += true_label.tolist()\n",
    "\n",
    "        # run bin classifier\n",
    "        logits = b_classifier(**input_data)\n",
    "        y_hat = torch.sigmoid(logits)\n",
    "        for criteria, threshold in b_classifier.thresholds.items():\n",
    "            label = b_classifier.yhat_to_label(y_hat, threshold)\n",
    "            binary_classify_results[criteria] += label.tolist()\n",
    "\n",
    "    # run relation classifier\n",
    "    print(\"Running relation classifier\")\n",
    "    dataset = GenericDataset(dataset_name, subset=\"test\", batch_size=batch_size, label_transform=\"none\")\n",
    "    relation_classify_result = []\n",
    "\n",
    "    for input_data, true_label  in tqdm(dataset.as_batches(), total=len(dataset)):\n",
    "        logits = r_classifier(**input_data)\n",
    "        label = r_classifier.logits_to_label(logits) + 1\n",
    "        relation_classify_result += label.tolist()\n",
    "\n",
    "    # combine results\n",
    "    print(\"Combining results\")\n",
    "    proposed_answer = {}\n",
    "    for criteria in b_classifier.thresholds.keys():\n",
    "        results = zip(relation_classify_result, binary_classify_results[criteria])\n",
    "        final_label = [relation_result if bin_result else 0 for relation_result, bin_result in results]\n",
    "        proposed_answer[criteria] = final_label\n",
    "\n",
    "    # log metric\n",
    "    final_metrics = {}\n",
    "    for criteria in b_classifier.thresholds.keys():\n",
    "        pa = proposed_answer[criteria]\n",
    "        \n",
    "        final_metrics.update({\n",
    "            f\"test_combined_{criteria}_acc\": accuracy_score(true_answer, pa),\n",
    "            f\"test_combined_{criteria}_pre_micro\": precision_score(true_answer, pa, average=\"micro\"),\n",
    "            f\"test_combined_{criteria}_rec_micro\": recall_score(true_answer, pa, average=\"micro\"),\n",
    "            f\"test_combined_{criteria}_f1_micro\": f1_score(true_answer, pa, average=\"micro\"),\n",
    "            f\"test_combined_{criteria}_pre_macro\": precision_score(true_answer, pa, average=\"macro\"),\n",
    "            f\"test_combined_{criteria}_rec_macro\": recall_score(true_answer, pa, average=\"macro\"),\n",
    "            f\"test_combined_{criteria}_f1_macro\": f1_score(true_answer, pa, average=\"macro\"),\n",
    "        })\n",
    "        \n",
    "        fig = BaseClassifier.plot_confusion_matrix(pa, true_answer)\n",
    "        logger.experiment.log_image(f\"test_combined_{criteria}_confusion_matrix\", fig)\n",
    "\n",
    "    for k, v in final_metrics.items():\n",
    "        print(f\"{k}: {v * 100}\")\n",
    "\n",
    "    for k, v in final_metrics.items():\n",
    "        logger.experiment.log_metric(k, v)\n",
    "    \n",
    "    # run the offical scorer\n",
    "    scorer = get_official_scorer(experiment_no, logger)\n",
    "    if scorer:\n",
    "        scorer.score(proposed_answer)\n",
    "    else:\n",
    "        print(\"No official scorer found\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T10:24:01.635634100Z"
    }
   },
   "id": "e0375b55597dc9d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combine_logger = NeptuneLogger(\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ==\",\n",
    "    project=\"Violetta/BERT\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    for i in range(4):\n",
    "        print(f\"--------- EXPERIMENT {i} ---------\")\n",
    "    \n",
    "        # clean up\n",
    "        bin_classifier = bin_trainer = rel_classifier = rel_trainer = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        # relation classifier\n",
    "        rel_trainer = LightningTrainer(\n",
    "            gpus=GPUS,\n",
    "            min_epochs=REL_MIN_EPOCHS,\n",
    "            max_epochs=REL_MAX_EPOCHS,\n",
    "            default_root_dir=CHECKPOINT_DIR,\n",
    "            reload_dataloaders_every_epoch=True, # needed as we loop over a file,\n",
    "            deterministic=False,\n",
    "            checkpoint_callback=False,\n",
    "            logger=combine_logger\n",
    "        )\n",
    "        rel_classifier = RelationClassifier(\n",
    "            pretrained_language_model=PRETRAINED_MODEL,\n",
    "            dataset_name=DATASET_NAME,\n",
    "            batch_size=REL_BATCH_SIZE,\n",
    "            learning_rate=REL_LEARNING_RATE,\n",
    "            decay_lr_speed=REL_LEARNING_RATE_DECAY_SPEED,\n",
    "            dropout_p=REL_DROPOUT_P,\n",
    "            activation_function=REL_ACTIVATION_FUNCTION,\n",
    "            weight_decay=REL_WEIGHT_DECAY,\n",
    "            linear_size=REL_LINEAR_SIZE,\n",
    "        )\n",
    "        rel_trainer.fit(rel_classifier)\n",
    "    \n",
    "        # binary classifier\n",
    "        bin_trainer = LightningTrainer(\n",
    "            gpus=GPUS,\n",
    "            min_epochs=BIN_MIN_EPOCHS,\n",
    "            max_epochs=BIN_MAX_EPOCHS,\n",
    "            default_root_dir=CHECKPOINT_DIR,\n",
    "            reload_dataloaders_every_epoch=True, # needed as we loop over a file,\n",
    "            deterministic=False,\n",
    "            checkpoint_callback=False,\n",
    "            logger=combine_logger,\n",
    "        )\n",
    "        bin_classifier = BinaryClassifier(\n",
    "            pretrained_language_model=PRETRAINED_MODEL,\n",
    "            dataset_name=DATASET_NAME,\n",
    "            batch_size=BIN_BATCH_SIZE,\n",
    "            learning_rate=BIN_LEARNING_RATE,\n",
    "            decay_lr_speed=BIN_LEARNING_RATE_DECAY_SPEED,\n",
    "            dropout_p=BIN_DROPOUT_P,\n",
    "            activation_function=BIN_ACTIVATION_FUNCTION,\n",
    "            weight_decay=BIN_WEIGHT_DECAY,\n",
    "            linear_size=BIN_LINEAR_SIZE,\n",
    "        )\n",
    "        bin_trainer.fit(bin_classifier)\n",
    "        \n",
    "        # test together\n",
    "        test_together(i, combine_logger, bin_classifier, rel_classifier)\n",
    "\n",
    "except Exception as e:\n",
    "    # combine_logger.experiment.stop(str(e))\n",
    "    raise e\n",
    "\n",
    "# else:\n",
    "#     combine_logger.experiment.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T10:24:01.635634100Z"
    }
   },
   "id": "907d4c40564b3da9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-26T10:24:01.636634Z"
    }
   },
   "id": "313598277cc928a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
