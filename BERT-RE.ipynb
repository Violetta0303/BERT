{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install dependencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16b3dcf3bccb330a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests==2.23.0\n",
      "  Using cached requests-2.23.0-py2.py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting numpy==1.18.5\n",
      "  Using cached numpy-1.18.5.zip (5.4 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [26 lines of output]\n",
      "  Running from numpy source directory.\n",
      "  <string>:461: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates\n",
      "  Traceback (most recent call last):\n",
      "    File \"E:\\Python\\Python3.11.5\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "      main()\n",
      "    File \"E:\\Python\\Python3.11.5\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "      json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"E:\\Python\\Python3.11.5\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 175, in prepare_metadata_for_build_wheel\n",
      "      return hook(metadata_directory, config_settings)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\pc\\AppData\\Local\\Temp\\pip-build-env-aqsvm4fk\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 377, in prepare_metadata_for_build_wheel\n",
      "      self.run_setup()\n",
      "    File \"C:\\Users\\pc\\AppData\\Local\\Temp\\pip-build-env-aqsvm4fk\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\n",
      "      super().run_setup(setup_script=setup_script)\n",
      "    File \"C:\\Users\\pc\\AppData\\Local\\Temp\\pip-build-env-aqsvm4fk\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\n",
      "      exec(code, locals())\n",
      "    File \"<string>\", line 488, in <module>\n",
      "    File \"<string>\", line 465, in setup_package\n",
      "    File \"C:\\Users\\pc\\AppData\\Local\\Temp\\pip-install-cg293q3p\\numpy_bfadb00d1b834fa0a1f02684474936c4\\numpy\\distutils\\core.py\", line 26, in <module>\n",
      "      from numpy.distutils.command import config, config_compiler, \\\n",
      "    File \"C:\\Users\\pc\\AppData\\Local\\Temp\\pip-install-cg293q3p\\numpy_bfadb00d1b834fa0a1f02684474936c4\\numpy\\distutils\\command\\config.py\", line 20, in <module>\n",
      "      from numpy.distutils.mingw32ccompiler import generate_manifest\n",
      "    File \"C:\\Users\\pc\\AppData\\Local\\Temp\\pip-install-cg293q3p\\numpy_bfadb00d1b834fa0a1f02684474936c4\\numpy\\distutils\\mingw32ccompiler.py\", line 34, in <module>\n",
      "      from distutils.msvccompiler import get_build_version as get_build_msvc_version\n",
      "  ModuleNotFoundError: No module named 'distutils.msvccompiler'\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests==2.23.0 numpy==1.18.5 pandas==1.0.3 \\\n",
    "    scikit-learn==0.23.1 pytorch-lightning==0.8.4 torch==1.5.1 \\\n",
    "    transformers==3.0.2 sklearn==0.0 tqdm==4.45.0 neptune-client==0.4.119 \\\n",
    "    matplotlib==3.1.0 scikit-plot==0.3.7"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:17:39.640104100Z",
     "start_time": "2025-02-24T11:17:32.319746200Z"
    }
   },
   "id": "2a1375485af72057"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (2.5.0.post0)\n",
      "Requirement already satisfied: torch>=2.1.0 in e:\\python\\python3.11.5\\lib\\site-packages (from pytorch-lightning) (2.1.0+cu121)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in e:\\python\\python3.11.5\\lib\\site-packages (from pytorch-lightning) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in e:\\python\\python3.11.5\\lib\\site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in e:\\python\\python3.11.5\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.3.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from pytorch-lightning) (1.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\python\\python3.11.5\\lib\\site-packages (from pytorch-lightning) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from pytorch-lightning) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from pytorch-lightning) (0.12.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in e:\\python\\python3.11.5\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.12)\n",
      "Requirement already satisfied: setuptools in e:\\python\\python3.11.5\\lib\\site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (65.5.0)\n",
      "Requirement already satisfied: filelock in e:\\python\\python3.11.5\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
      "Requirement already satisfied: networkx in e:\\python\\python3.11.5\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.0)\n",
      "Requirement already satisfied: jinja2 in e:\\python\\python3.11.5\\lib\\site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in e:\\python\\python3.11.5\\lib\\site-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.0)\n",
      "Requirement already satisfied: colorama in e:\\python\\python3.11.5\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in e:\\python\\python3.11.5\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in e:\\python\\python3.11.5\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in e:\\python\\python3.11.5\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in e:\\python\\python3.11.5\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in e:\\python\\python3.11.5\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in e:\\python\\python3.11.5\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in e:\\python\\python3.11.5\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\python\\python3.11.5\\lib\\site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\python\\python3.11.5\\lib\\site-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.0 in e:\\python\\python3.11.5\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: neptune in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (1.13.0)\n",
      "Requirement already satisfied: GitPython>=2.0.8 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from neptune) (3.1.44)\n",
      "Requirement already satisfied: Pillow>=1.1.6 in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (10.0.1)\n",
      "Requirement already satisfied: PyJWT in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (2.10.1)\n",
      "Requirement already satisfied: boto3>=1.28.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from neptune) (1.36.26)\n",
      "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from neptune) (11.1.0)\n",
      "Requirement already satisfied: click>=7.0 in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (8.1.7)\n",
      "Requirement already satisfied: future>=0.17.1 in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (1.0.0)\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from neptune) (3.2.2)\n",
      "Requirement already satisfied: packaging in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (23.2)\n",
      "Requirement already satisfied: pandas in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (2.1.2)\n",
      "Requirement already satisfied: psutil in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (6.1.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from neptune) (2.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (1.16.0)\n",
      "Requirement already satisfied: swagger-spec-validator>=2.7.4 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from neptune) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from neptune) (4.12.2)\n",
      "Requirement already satisfied: urllib3 in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (1.26.13)\n",
      "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in e:\\python\\python3.11.5\\lib\\site-packages (from neptune) (1.8.0)\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.26 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from boto3>=1.28.0->neptune) (1.36.26)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in e:\\python\\python3.11.5\\lib\\site-packages (from boto3>=1.28.0->neptune) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from boto3>=1.28.0->neptune) (0.11.2)\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.1.1)\n",
      "Requirement already satisfied: monotonic in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.6)\n",
      "Requirement already satisfied: msgpack in e:\\python\\python3.11.5\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil in e:\\python\\python3.11.5\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in e:\\python\\python3.11.5\\lib\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.1)\n",
      "Requirement already satisfied: simplejson in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from bravado<12.0.0,>=11.0.0->neptune) (3.20.1)\n",
      "Requirement already satisfied: colorama in e:\\python\\python3.11.5\\lib\\site-packages (from click>=7.0->neptune) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from GitPython>=2.0.8->neptune) (4.0.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\python\\python3.11.5\\lib\\site-packages (from requests>=2.20.0->neptune) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\python\\python3.11.5\\lib\\site-packages (from requests>=2.20.0->neptune) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\python\\python3.11.5\\lib\\site-packages (from requests>=2.20.0->neptune) (2022.12.7)\n",
      "Requirement already satisfied: jsonschema in e:\\python\\python3.11.5\\lib\\site-packages (from swagger-spec-validator>=2.7.4->neptune) (4.23.0)\n",
      "Requirement already satisfied: importlib-resources>=1.3 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from swagger-spec-validator>=2.7.4->neptune) (6.5.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in e:\\python\\python3.11.5\\lib\\site-packages (from pandas->neptune) (1.26.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\python\\python3.11.5\\lib\\site-packages (from pandas->neptune) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in e:\\python\\python3.11.5\\lib\\site-packages (from pandas->neptune) (2023.3)\n",
      "Requirement already satisfied: jsonref in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in e:\\python\\python3.11.5\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in e:\\python\\python3.11.5\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in e:\\python\\python3.11.5\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in e:\\python\\python3.11.5\\lib\\site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.21.0)\n",
      "Requirement already satisfied: fqdn in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in e:\\python\\python3.11.5\\lib\\site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (3.0.0)\n",
      "Requirement already satisfied: rfc3339-validator in e:\\python\\python3.11.5\\lib\\site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>0.1.0 in e:\\python\\python3.11.5\\lib\\site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (0.1.1)\n",
      "Requirement already satisfied: uri-template in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (24.11.1)\n",
      "Requirement already satisfied: arrow>=0.15.0 in e:\\python\\python3.11.5\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in e:\\python\\python3.11.5\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (2.9.0.20241003)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning\n",
    "!pip install neptune"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T12:43:33.582886600Z",
     "start_time": "2025-02-24T12:43:30.639906Z"
    }
   },
   "id": "7bc8669afcfb9674"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import needed modules"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:04:06.658901700Z",
     "start_time": "2025-02-24T11:04:06.461758500Z"
    }
   },
   "id": "3241abfa0a66c9a4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import OrderedDict\n",
    "from random import randint\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib.figure import Figure\n",
    "from pandas import DataFrame\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from pytorch_lightning import Trainer as LightningTrainer\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.utils import column_or_1d\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    GenerationMixin, TFGenerationMixin\n",
    ")\n",
    "\n",
    "# 忽略 transformers 可能的 FutureWarning\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T12:46:40.308667Z",
     "start_time": "2025-02-24T12:46:40.291991700Z"
    }
   },
   "id": "c17dd9f74259362c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define constants"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:04:09.819895700Z",
     "start_time": "2025-02-24T11:04:08.157419300Z"
    }
   },
   "id": "4f381dab5ddad5f4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2020\n"
     ]
    }
   ],
   "source": [
    "# --- Random seed ---\n",
    "SEED = 2020\n",
    "seed_everything(SEED)\n",
    "\n",
    "# --- Directory ---\n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, \"data/processed\") \n",
    "METADATA_FILE_NAME = os.path.join(PROCESSED_DATA_DIR, \"metadata.json\")\n",
    "CHECKPOINT_DIR = os.path.join(ROOT_DIR, \"checkpoint\")\n",
    "\n",
    "KAGGLE_ENV = bool(os.getenv(\"KAGGLE_URL_BASE\"))\n",
    "if KAGGLE_ENV:\n",
    "    # in Kaggle environment\n",
    "    # 2 datasets should already been added to the notebook\n",
    "    RAW_DATA_DIR = os.path.join(ROOT_DIR, \"../input\")\n",
    "else:\n",
    "    # in local environment\n",
    "    RAW_DATA_DIR =  os.path.join(ROOT_DIR, \"data/raw\")\n",
    "\n",
    "# --- Datasets ---\n",
    "DATASET_MAPPING = {\n",
    "    \"SemEval2010Task8\": {\n",
    "        \"dir\": os.path.join(RAW_DATA_DIR,\"semeval2010-task-8\"),\n",
    "        \"keep_test_order\": True,\n",
    "        \"precision_recall_curve_baseline_img\": None,\n",
    "    },\n",
    "    \"GIDS\": {\n",
    "        \"dir\": os.path.join(RAW_DATA_DIR,\"gids-dataset\"),\n",
    "        \"keep_test_order\": False,\n",
    "        \"precision_recall_curve_baseline_img\": os.path.join(RAW_DATA_DIR,\"gids-dataset/GIDS_precision_recall_curve.png\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "# change this variable to switch dataset in later tasks\n",
    "DATASET_NAME = list(DATASET_MAPPING.keys())[1]\n",
    "\n",
    "# --- Subject & object markup ---\n",
    "SUB_START_CHAR = \"[\"\n",
    "SUB_END_CHAR = \"]\"\n",
    "OBJ_START_CHAR = \"{\"\n",
    "OBJ_END_CHAR = \"}\"\n",
    "\n",
    "# --- BERT variants ---\n",
    "# See https://huggingface.co/transformers/pretrained_models.html for the full list\n",
    "AVAILABLE_PRETRAINED_MODELS = [\n",
    "    \"distilbert-base-uncased\", # 0\n",
    "    \"distilbert-base-cased\",   # 1\n",
    "    \"bert-base-uncased\",       # 2\n",
    "    \"distilgpt2\",              # 3\n",
    "    \"gpt2\",                    # 4\n",
    "    \"distilroberta-base\",      # 5\n",
    "    \"roberta-base\",            # 6\n",
    "    \"albert-base-v1\",          # 7\n",
    "    \"albert-base-v2\",          # 8\n",
    "    \"bert-large-uncased\",      # 9\n",
    "]\n",
    "\n",
    "# change this variable to switch pretrained language model\n",
    "PRETRAINED_MODEL = AVAILABLE_PRETRAINED_MODELS[2]\n",
    "\n",
    "# if e1 is not related to e2, should \"e2 not related to e1\" be added to the training set\n",
    "ADD_REVERSE_RELATIONSHIP = True\n",
    "\n",
    "# --- Neptune logger ---\n",
    "# Create a free account at https://neptune.ai/,\n",
    "# then get the API token and create a project\n",
    "NEPTUNE_API_TOKEN = \" eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ== \"\n",
    "NEPTUNE_PROJECT_NAME = \" Violetta/BERT \""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T12:46:59.412917900Z",
     "start_time": "2025-02-24T12:46:59.392895300Z"
    }
   },
   "id": "42f341d5b622696e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddc10e8d4a44f81d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GIDS train set from https://raw.githubusercontent.com/SharmisthaJat/RE-DS-Word-Attention-Models/master/Data/GIDS/train.tsv...\n",
      "train.tsv downloaded successfully.\n",
      "Downloading GIDS validation set from https://raw.githubusercontent.com/SharmisthaJat/RE-DS-Word-Attention-Models/master/Data/GIDS/dev.tsv...\n",
      "val.tsv downloaded successfully.\n",
      "Downloading GIDS test set from https://raw.githubusercontent.com/SharmisthaJat/RE-DS-Word-Attention-Models/master/Data/GIDS/test.tsv...\n",
      "test.tsv downloaded successfully.\n",
      "Downloading SemEval2010 Task 8 dataset from Hugging Face...\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'relation'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'relation'],\n",
      "        num_rows: 2717\n",
      "    })\n",
      "})\n",
      "Train set size: 8000\n",
      "Test set size: 2717\n",
      "DEBUG: Example: sentence\n",
      "DEBUG: Example: relation\n",
      "Training set saved to datasets/semeval2010\\SemEval2010_task8_training\\TRAIN_FILE.TXT\n",
      "Test set saved to datasets/semeval2010\\SemEval2010_task8_testing_keys\\TEST_FILE_FULL.TXT\n",
      "\n",
      "Dataset storage locations:\n",
      "SemEval 2010 Task 8 directory: datasets/semeval2010\n",
      "GIDS directory: datasets/gids\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 定义数据集存储路径\n",
    "DATASET_MAPPING = {\n",
    "    \"SemEval2010Task8\": {\n",
    "        \"dir\": \"datasets/semeval2010\"\n",
    "    },\n",
    "    \"GIDS\": {\n",
    "        \"dir\": \"datasets/gids\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# GIDS 数据集的下载 URL\n",
    "GIDS_URLs = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/SharmisthaJat/RE-DS-Word-Attention-Models/master/Data/GIDS/train.tsv\",\n",
    "    \"validation\": \"https://raw.githubusercontent.com/SharmisthaJat/RE-DS-Word-Attention-Models/master/Data/GIDS/dev.tsv\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/SharmisthaJat/RE-DS-Word-Attention-Models/master/Data/GIDS/test.tsv\",\n",
    "}\n",
    "\n",
    "# 确保所有数据集目录存在\n",
    "for dataset, paths in DATASET_MAPPING.items():\n",
    "    os.makedirs(paths[\"dir\"], exist_ok=True)\n",
    "\n",
    "# 下载 GIDS 数据集\n",
    "def download_gids():\n",
    "    gids_dir = DATASET_MAPPING[\"GIDS\"][\"dir\"]\n",
    "\n",
    "    for split, url in GIDS_URLs.items():\n",
    "        file_name = \"val.tsv\" if split == \"validation\" else f\"{split}.tsv\"\n",
    "        file_path = os.path.join(gids_dir, file_name)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Downloading GIDS {split} set from {url}...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                print(f\"{file_name} downloaded successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to download {file_name}. Please check the URL.\")\n",
    "        else:\n",
    "            print(f\"{file_name} already exists. Skipping download.\")\n",
    "\n",
    "# 下载并保存 SemEval 2010 Task 8 数据集\n",
    "def download_semeval():\n",
    "    semeval_dir = DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"]\n",
    "    train_dir = os.path.join(semeval_dir, \"SemEval2010_task8_training\")\n",
    "    test_dir = os.path.join(semeval_dir, \"SemEval2010_task8_testing_keys\")\n",
    "\n",
    "    train_path = os.path.join(train_dir, \"TRAIN_FILE.TXT\")\n",
    "    test_path = os.path.join(test_dir, \"TEST_FILE_FULL.TXT\")\n",
    "\n",
    "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "        print(\"Downloading SemEval2010 Task 8 dataset from Hugging Face...\")\n",
    "        dataset = load_dataset(\"SemEvalWorkshop/sem_eval_2010_task_8\")\n",
    "\n",
    "        # **检查数据集是否成功加载**\n",
    "        print(\"Dataset structure:\", dataset)\n",
    "        print(\"Train set size:\", len(dataset[\"train\"]))\n",
    "        print(\"Test set size:\", len(dataset[\"test\"]))\n",
    "\n",
    "        # 确保目录存在\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "        os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "        # **检查数据格式**\n",
    "        for example in dataset[\"train\"][:5]:  \n",
    "            print(f\"DEBUG: Example: {example}\")\n",
    "\n",
    "        # **保存训练集**\n",
    "        with open(train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for example in dataset[\"train\"]:\n",
    "                if not isinstance(example, dict):  \n",
    "                    print(f\"Unexpected data format in train set: {example}\")\n",
    "                    continue  # 跳过格式错误的样本\n",
    "                \n",
    "                sentence = example.get(\"sentence\", \"\").strip()\n",
    "                relation = str(example.get(\"relation\", \"Other\")).strip()\n",
    "                \n",
    "                if not sentence or not relation:\n",
    "                    print(f\"Skipping empty entry: {example}\")\n",
    "                    continue  # 跳过空值数据\n",
    "                \n",
    "                f.write(f\"{sentence}\\t{relation}\\n\")\n",
    "        print(f\"Training set saved to {train_path}\")\n",
    "\n",
    "        # **保存测试集**\n",
    "        with open(test_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for example in dataset[\"test\"]:\n",
    "                if not isinstance(example, dict):  \n",
    "                    print(f\"Unexpected data format in test set: {example}\")\n",
    "                    continue  # 跳过格式错误的样本\n",
    "\n",
    "                sentence = example.get(\"sentence\", \"\").strip()\n",
    "                relation = str(example.get(\"relation\", \"Other\")).strip()\n",
    "\n",
    "                if not sentence or not relation:\n",
    "                    print(f\"Skipping empty entry: {example}\")\n",
    "                    continue  # 跳过空值数据\n",
    "\n",
    "                f.write(f\"{sentence}\\t{relation}\\n\")\n",
    "        print(f\"Test set saved to {test_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"SemEval2010 Task 8 dataset already exists.\")\n",
    "\n",
    "# 执行数据下载\n",
    "download_gids()\n",
    "download_semeval()\n",
    "\n",
    "# 验证数据集路径\n",
    "print(\"\\nDataset storage locations:\")\n",
    "print(f\"SemEval 2010 Task 8 directory: {DATASET_MAPPING['SemEval2010Task8']['dir']}\")\n",
    "print(f\"GIDS directory: {DATASET_MAPPING['GIDS']['dir']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:04:08.936900Z",
     "start_time": "2025-02-24T13:04:03.862564900Z"
    }
   },
   "id": "ee05f7f83844633b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:04:10.609278Z",
     "start_time": "2025-02-24T10:04:09.081836400Z"
    }
   },
   "id": "d12e7b5f952b50f5"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class OrdinalLabelEncoder:\n",
    "    def __init__(self, init_labels=None):\n",
    "        if init_labels is None:\n",
    "            init_labels = []\n",
    "        self.mapping = OrderedDict({l: i for i, l in enumerate(init_labels)})\n",
    "\n",
    "    @property\n",
    "    def classes_(self):\n",
    "        return list(self.mapping.keys())\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        return self.fit(y).transform(y)\n",
    "\n",
    "    def fit(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        new_classes = pd.Series(y).unique()\n",
    "        for cls in new_classes:\n",
    "            if cls not in self.mapping:\n",
    "                self.mapping[cls] = len(self.mapping)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        return [self.mapping[value] for value in y]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:09:04.309484400Z",
     "start_time": "2025-02-24T13:09:04.306996800Z"
    }
   },
   "id": "6eeb36b2a27f25ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Abstract preprocessor class:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:26:53.166431700Z",
     "start_time": "2025-02-24T09:26:53.123829900Z"
    }
   },
   "id": "7f6fe7cf363af3b0"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class AbstractPreprocessor(ABC):\n",
    "    DATASET_NAME = \"\"\n",
    "    VAL_DATA_PROPORTION = 0.2\n",
    "    NO_RELATION_LABEL = \"\"\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.SUB_START_ID, self.SUB_END_ID, self.OBJ_START_ID, self.OBJ_END_ID \\\n",
    "            = tokenizer.convert_tokens_to_ids([SUB_START_CHAR, SUB_END_CHAR, OBJ_START_CHAR, OBJ_END_CHAR])\n",
    "        self.label_encoder = OrdinalLabelEncoder([self.NO_RELATION_LABEL])\n",
    "\n",
    "    def preprocess_data(self, reprocess: bool):\n",
    "        print(f\"\\n---> Preprocessing {self.DATASET_NAME} dataset <---\")\n",
    "        \n",
    "        # create processed data dir\n",
    "        if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "            print(\"Creating processed data directory \" + PROCESSED_DATA_DIR)\n",
    "            os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "        # stop preprocessing if file existed\n",
    "        json_file_names = [self.get_dataset_file_name(k) for k in (\"train\", \"val\", \"test\")]\n",
    "        existed_files = [fn for fn in json_file_names if os.path.exists(fn)]\n",
    "        if existed_files:\n",
    "            file_text = \"- \" + \"\\n- \".join(existed_files)\n",
    "            if not reprocess:\n",
    "                print(\"The following files already exist:\")\n",
    "                print(file_text)\n",
    "                print(\"Preprocessing is skipped. See option --reprocess.\")\n",
    "                return\n",
    "            else:\n",
    "                print(\"The following files will be overwritten:\")\n",
    "                print(file_text)\n",
    "\n",
    "        train_data, val_data, test_data = self._preprocess_data()\n",
    "\n",
    "        print(\"Saving to json files\")\n",
    "        self._write_data_to_file(train_data, \"train\")\n",
    "        self._write_data_to_file(val_data, \"val\")\n",
    "        self._write_data_to_file(test_data, \"test\")\n",
    "\n",
    "        self._save_metadata({\n",
    "            \"train_size\": len(train_data),\n",
    "            \"val_size\": len(val_data),\n",
    "            \"test_size\": len(test_data),\n",
    "            \"no_relation_label\": self.NO_RELATION_LABEL,\n",
    "            **self._get_label_mapping()\n",
    "        })\n",
    "\n",
    "        self._create_secondary_data_files()\n",
    "\n",
    "        print(\"---> Done ! <---\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def _preprocess_data(self) -> Tuple[DataFrame, DataFrame, DataFrame]:\n",
    "        pass\n",
    "\n",
    "    def _create_secondary_data_files(self):\n",
    "        \"\"\"\n",
    "        From the primary data file, create a data file with binary labels\n",
    "        and a data file with only sentences classified as \"related\"\n",
    "        \"\"\"\n",
    "\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            root_metadata = json.load(f)\n",
    "            metadata = root_metadata[self.DATASET_NAME]\n",
    "\n",
    "        related_only_count = {\n",
    "            \"train\": 0,\n",
    "            \"val\": 0,\n",
    "            \"test\": 0,\n",
    "        }\n",
    "\n",
    "        for key in [\"train\", \"test\", \"val\"]:\n",
    "            print(f\"Creating secondary files for {key} data\")\n",
    "\n",
    "            origin_file = open(self.get_dataset_file_name(key))\n",
    "            bin_file = open(self.get_dataset_file_name(f\"{key}_binary\"), \"w\")\n",
    "            related_file = open(self.get_dataset_file_name(f\"{key}_related_only\"), \"w\")\n",
    "\n",
    "            total = metadata[f\"{key}_size\"]\n",
    "\n",
    "            for line in tqdm(origin_file, total=total):\n",
    "                data = json.loads(line)\n",
    "                if data[\"label\"] != 0:\n",
    "                    related_only_count[key] += 1\n",
    "                    data[\"label\"] -= 1 # label in \"related_only\" files is 1 less than the original label\n",
    "                    related_file.write(json.dumps(data) + \"\\n\")\n",
    "                    data[\"label\"] = 1 # in binary dataset, all \"related\" classes have label 1\n",
    "                    bin_file.write(json.dumps(data) + \"\\n\")\n",
    "                else:\n",
    "                    bin_file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "            origin_file.close()\n",
    "            bin_file.close()\n",
    "            related_file.close()\n",
    "\n",
    "        print(\"Updating metadata.json\")\n",
    "        for key in [\"train\", \"test\", \"val\"]:\n",
    "            metadata[f\"{key}_related_only_size\"] = related_only_count[key]\n",
    "        root_metadata[self.DATASET_NAME] = metadata\n",
    "        with open(METADATA_FILE_NAME, \"w\") as f:\n",
    "            json.dump(root_metadata, f, indent=4)\n",
    "\n",
    "    def _find_sub_obj_pos(self, input_ids_list: Iterable) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Find subject and object position in a sentence\n",
    "        \"\"\"\n",
    "        sub_start_pos = [self._index(s, self.SUB_START_ID) + 1 for s in input_ids_list]\n",
    "        sub_end_pos = [self._index(s, self.SUB_END_ID, sub_start_pos[i]) for i, s in enumerate(input_ids_list)]\n",
    "        obj_start_pos = [self._index(s, self.OBJ_START_ID) + 1 for s in input_ids_list]\n",
    "        obj_end_pos = [self._index(s, self.OBJ_END_ID, obj_start_pos[i]) for i, s in enumerate(input_ids_list)]\n",
    "        return DataFrame({\n",
    "            \"sub_start_pos\": sub_start_pos,\n",
    "            \"sub_end_pos\": sub_end_pos,\n",
    "            \"obj_start_pos\": obj_start_pos,\n",
    "            \"obj_end_pos\": obj_end_pos,\n",
    "        })\n",
    "\n",
    "    @staticmethod\n",
    "    def _index(lst: list, ele: int, start: int = 0) -> int:\n",
    "        \"\"\"\n",
    "        Find an element in a list. Returns -1 if not found instead of raising an exception.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return lst.index(ele, start)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "\n",
    "    def _clean_data(self, raw_sentences: list, labels: list) -> DataFrame:\n",
    "        if not raw_sentences:\n",
    "            return DataFrame()\n",
    "\n",
    "        tokens = self.tokenizer(raw_sentences, truncation=True, padding=\"max_length\")\n",
    "        data = DataFrame(tokens.data)\n",
    "        data[\"label\"] = self.label_encoder.fit_transform(labels)\n",
    "        sub_obj_position = self._find_sub_obj_pos(data[\"input_ids\"])\n",
    "        data = pd.concat([data, sub_obj_position], axis=1)\n",
    "        data = self._remove_invalid_sentences(data)\n",
    "        return data\n",
    "\n",
    "    def _remove_invalid_sentences(self, data: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Remove sentences without subject/object or whose subject/object\n",
    "        is beyond the maximum length the model supports\n",
    "        \"\"\"\n",
    "        seq_max_len = self.tokenizer.model_max_length\n",
    "        return data.loc[\n",
    "            (data[\"sub_end_pos\"] < seq_max_len)\n",
    "            & (data[\"obj_end_pos\"] < seq_max_len)\n",
    "            & (data[\"sub_end_pos\"] > -1)\n",
    "            & (data[\"obj_end_pos\"] > -1)\n",
    "        ]\n",
    "\n",
    "    def _get_label_mapping(self):\n",
    "        \"\"\"\n",
    "        Returns a mapping from id to label and vise versa from the label encoder\n",
    "        \"\"\"\n",
    "        # all labels\n",
    "        id_to_label = dict(enumerate(self.label_encoder.classes_))\n",
    "        label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "\n",
    "        # for the related_only dataset\n",
    "        # ignore id 0, which represent no relation\n",
    "        id_to_label_related_only = {k - 1: v for k, v in id_to_label.items() if k != 0}\n",
    "        label_to_id_related_only = {v: k for k, v in id_to_label_related_only.items()}\n",
    "\n",
    "        return {\n",
    "            \"id_to_label\": id_to_label,\n",
    "            \"label_to_id\": label_to_id,\n",
    "            \"id_to_label_related_only\": id_to_label_related_only,\n",
    "            \"label_to_id_related_only\": label_to_id_related_only,            \n",
    "        }\n",
    "\n",
    "    def _write_data_to_file(self, dataframe: DataFrame, subset: str):\n",
    "        \"\"\"Write data in a dataframe to train/val/test file\"\"\"\n",
    "        lines = \"\"\n",
    "        for _, row in dataframe.iterrows():\n",
    "            lines += row.to_json() + \"\\n\"\n",
    "        with open(self.get_dataset_file_name(subset), \"w\") as file:\n",
    "            file.write(lines)\n",
    "\n",
    "    def _save_metadata(self, metadata: dict):\n",
    "        \"\"\"Save metadata to metadata.json\"\"\"\n",
    "        # create metadata file\n",
    "        if not os.path.exists(METADATA_FILE_NAME):\n",
    "            print(f\"Create metadata file at {METADATA_FILE_NAME}\")\n",
    "            with open(METADATA_FILE_NAME, \"w\") as f:\n",
    "                f.write(\"{}\\n\")\n",
    "\n",
    "        # add metadata\n",
    "        print(\"Saving metadata\")\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            root_metadata = json.load(f)\n",
    "        with open(METADATA_FILE_NAME, \"w\") as f:\n",
    "            root_metadata[self.DATASET_NAME] = metadata\n",
    "            json.dump(root_metadata, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def get_dataset_file_name(cls, key: str) -> str:\n",
    "        return os.path.join(PROCESSED_DATA_DIR, f\"{cls.DATASET_NAME.lower()}_{key}.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:09:09.266049600Z",
     "start_time": "2025-02-24T13:09:09.258493900Z"
    }
   },
   "id": "bfb46534afa50955"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Concrete preprocessor for each dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:26:56.657440800Z",
     "start_time": "2025-02-24T09:26:56.586044800Z"
    }
   },
   "id": "73a689bb553ac1d2"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class SemEval2010Task8Preprocessor(AbstractPreprocessor):\n",
    "    DATASET_NAME = \"SemEval2010Task8\"\n",
    "    NO_RELATION_LABEL = \"Other\"\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"],\n",
    "                                       \"SemEval2010_task8_training/TRAIN_FILE.TXT\")\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"],\n",
    "                                      \"SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT\")\n",
    "    RAW_TRAIN_DATA_SIZE = 8000\n",
    "    RAW_TEST_DATA_SIZE = 2717\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        print(\"Processing training data\")\n",
    "        train_data = self._process_file(\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            self.RAW_TRAIN_DATA_SIZE,\n",
    "            ADD_REVERSE_RELATIONSHIP,\n",
    "        )\n",
    "\n",
    "        print(\"Processing test data\")\n",
    "        test_data = self._process_file(\n",
    "            self.RAW_TEST_FILE_NAME,\n",
    "            self.RAW_TEST_DATA_SIZE,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        print(\"Splitting train & validate data\")\n",
    "        train_data, val_data = train_test_split(train_data, shuffle=True, random_state=SEED)\n",
    "\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def _process_file(self, file_name: str, dataset_size: int, add_reverse: bool) -> DataFrame:\n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "        with open(file_name) as f:\n",
    "            for _ in tqdm(range(dataset_size)):\n",
    "                sent = f.readline()\n",
    "                label, sub, obj = self._process_label(f.readline())\n",
    "                labels.append(label)\n",
    "                raw_sentences.append(self._process_sentence(sent, sub, obj))\n",
    "                if label == \"Other\" and add_reverse:\n",
    "                    labels.append(label)\n",
    "                    raw_sentences.append(self._process_sentence(sent, obj, sub))\n",
    "                f.readline()\n",
    "                f.readline()\n",
    "\n",
    "        return self._clean_data(raw_sentences, labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_sentence(sentence: str, sub: int, obj: int) -> str:\n",
    "        return sentence.split(\"\\t\")[1][1:-2] \\\n",
    "            .replace(f\"<e{sub}>\", SUB_START_CHAR) \\\n",
    "            .replace(f\"</e{sub}>\", SUB_END_CHAR) \\\n",
    "            .replace(f\"<e{obj}>\", OBJ_START_CHAR) \\\n",
    "            .replace(f\"</e{obj}>\", OBJ_END_CHAR)\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_label(label: str) -> Tuple[str, int, int]:\n",
    "        label = label.strip()\n",
    "        if label == \"Other\":\n",
    "            return label, 1, 2\n",
    "        nums = list(filter(str.isdigit, label))\n",
    "        return label, int(nums[0]), int(nums[1])\n",
    "\n",
    "\n",
    "class GIDSPreprocessor(AbstractPreprocessor):\n",
    "    DATASET_NAME = \"GIDS\"\n",
    "    RAW_TRAIN_FILE_NAME = os.path.join(DATASET_MAPPING[\"GIDS\"][\"dir\"], \"train.tsv\")\n",
    "    RAW_VAL_FILE_NAME = os.path.join(DATASET_MAPPING[\"GIDS\"][\"dir\"], \"val.tsv\")\n",
    "    RAW_TEST_FILE_NAME = os.path.join(DATASET_MAPPING[\"GIDS\"][\"dir\"], \"test.tsv\")\n",
    "    TRAIN_SIZE = 11297\n",
    "    VAL_SIZE = 1864\n",
    "    TEST_SIZE = 5663\n",
    "    NO_RELATION_LABEL = \"NA\"\n",
    "\n",
    "    def _process_file(self, file_name: str, add_reverse: bool) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Process a file in batches\n",
    "        Return the total data size\n",
    "        \"\"\"\n",
    "        with open(file_name) as in_file:\n",
    "            lines = in_file.readlines()\n",
    "            \n",
    "        raw_sentences = []\n",
    "        labels = []\n",
    "\n",
    "        for line in tqdm(lines):\n",
    "            _, _, sub, obj, label, sent = line.split(\"\\t\")\n",
    "            sent = sent.replace(\"###END###\", \"\")\n",
    "\n",
    "            # add subject markup\n",
    "            new_sub = SUB_START_CHAR + \" \" + sub.replace(\"_\", \" \") + \" \" + SUB_END_CHAR\n",
    "            new_obj = OBJ_START_CHAR + \" \" +  obj.replace(\"_\", \" \") + \" \" + OBJ_END_CHAR\n",
    "            sent = sent.replace(sub, new_sub).replace(obj, new_obj)\n",
    "            raw_sentences.append(sent)\n",
    "            labels.append(label)\n",
    "\n",
    "            if add_reverse and label == self.NO_RELATION_LABEL:\n",
    "                new_sub = OBJ_START_CHAR + \" \" + sub.replace(\"_\", \" \") + \" \" + OBJ_END_CHAR\n",
    "                new_obj = SUB_START_CHAR + \" \" + obj.replace(\"_\", \" \") + \" \" + SUB_END_CHAR\n",
    "                sent = sent.replace(sub, new_sub).replace(obj, new_obj)\n",
    "                raw_sentences.append(sent)\n",
    "                labels.append(label)\n",
    "\n",
    "        return self._clean_data(raw_sentences, labels)\n",
    "\n",
    "    def _preprocess_data(self):\n",
    "        \n",
    "        print(\"Process train dataset\")\n",
    "        train_data = self._process_file(\n",
    "            self.RAW_TRAIN_FILE_NAME,\n",
    "            ADD_REVERSE_RELATIONSHIP,\n",
    "        )\n",
    "\n",
    "        print(\"Process val dataset\")\n",
    "        val_data = self._process_file(\n",
    "            self.RAW_VAL_FILE_NAME,\n",
    "            False,\n",
    "        )\n",
    "        \n",
    "        print(\"Process test dataset\")\n",
    "        test_data = self._process_file(\n",
    "            self.RAW_TEST_FILE_NAME,\n",
    "            False,\n",
    "        )\n",
    "\n",
    "        return train_data, val_data, test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:09:12.184617500Z",
     "start_time": "2025-02-24T13:09:12.177591800Z"
    }
   },
   "id": "94deaa87ce0aef7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Factory method to create preprocessors:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:26:57.999809300Z",
     "start_time": "2025-02-24T09:26:57.977150500Z"
    }
   },
   "id": "eba2b6256a8f9a36"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def get_preprocessor_class(dataset_name: str = DATASET_NAME):\n",
    "    return globals()[f\"{dataset_name}Preprocessor\"]\n",
    "        \n",
    "def get_preprocessor(dataset_name: str = DATASET_NAME)-> AbstractPreprocessor:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL, use_fast=True)\n",
    "    # some tokenizer, like GPTTokenizer, doesn't have pad_token\n",
    "    # in this case, we use eos token as pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    preprocessors_class = get_preprocessor_class(dataset_name)\n",
    "    return preprocessors_class(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:10:02.383678300Z",
     "start_time": "2025-02-24T13:10:02.379354500Z"
    }
   },
   "id": "ce835924d958bacb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess data:"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T10:04:21.960411400Z",
     "start_time": "2025-02-24T10:04:19.222127700Z"
    }
   },
   "id": "ca251d3d25a747fc"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> Preprocessing GIDS dataset <---\n",
      "Creating processed data directory E:\\Python\\TextMining\\BERT\\data/processed\n",
      "Process train dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11297 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c39404e703c4ed68a14121f27e2aa29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process val dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1864 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f815c9d1ba44c16bd5d8389f006f0fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process test dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5663 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ea1b30d99b540d2a2ad5f02827299fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to json files\n",
      "Create metadata file at E:\\Python\\TextMining\\BERT\\data/processed\\metadata.json\n",
      "Saving metadata\n",
      "Creating secondary files for train data\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/14051 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f617884d61c747e48e1d87b7ba64ff1f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating secondary files for test data\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5653 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dfd4462073304ea6ac1313a87b0f4223"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating secondary files for val data\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1864 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cbdfcd194d944cc82b0170f6fc0bf9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating metadata.json\n",
      "---> Done ! <---\n"
     ]
    }
   ],
   "source": [
    "preprocessor = get_preprocessor()\n",
    "preprocessor.preprocess_data(reprocess=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:11:11.479099300Z",
     "start_time": "2025-02-24T13:10:06.892451800Z"
    }
   },
   "id": "56ce550891c1c750"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:53:59.890270400Z",
     "start_time": "2025-02-24T09:53:59.883216400Z"
    }
   },
   "id": "d18e0242ced35b81"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class GenericDataset(IterableDataset):\n",
    "    \"\"\"A generic dataset for train/val/test data for both SemEval and GIDS dataset\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_name: str, subset: str, batch_size: int, label_transform: str):\n",
    "        assert subset in [\"train\", \"val\", \"test\"]\n",
    "        assert label_transform in [\"none\", \"binary\", \"related_only\"]\n",
    "\n",
    "        file_name = subset if label_transform == \"none\" \\\n",
    "            else f\"{subset}_{label_transform}\"\n",
    "\n",
    "        preprocessor_class = get_preprocessor_class()\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            metadata = json.load(f)[dataset_name]\n",
    "\n",
    "        size = metadata[f\"{subset}_related_only_size\"] \\\n",
    "            if label_transform == \"related_only\" \\\n",
    "            else metadata[f\"{subset}_size\"]\n",
    "\n",
    "        self.subset = subset\n",
    "        self.batch_size = batch_size\n",
    "        self.length = math.ceil(size / batch_size)\n",
    "        self.file = open(preprocessor_class.get_dataset_file_name(file_name))\n",
    "\n",
    "        self.keep_test_order = self.subset == \"test\" and DATASET_MAPPING[dataset_name][\"keep_test_order\"]\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.file:\n",
    "            self.file.close()\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Implement \"smart batching\"\n",
    "        \"\"\"\n",
    "\n",
    "        data = [json.loads(line) for line in self.file]\n",
    "        if not self.keep_test_order:\n",
    "            data = sorted(data, key=lambda x: sum(x[\"attention_mask\"]))\n",
    "\n",
    "        new_data = []\n",
    "\n",
    "        while len(data) > 0:\n",
    "            if self.keep_test_order or len(data) < self.batch_size:\n",
    "                idx = 0\n",
    "            else:\n",
    "                idx = randint(0, len(data) - self.batch_size)\n",
    "            batch = data[idx:idx + self.batch_size]\n",
    "            max_len = max([sum(b[\"attention_mask\"]) for b in batch])\n",
    "\n",
    "            for b in batch:\n",
    "                input_data = {}\n",
    "                for k, v in b.items():\n",
    "                    if k != \"label\":\n",
    "                        if isinstance(v, list):\n",
    "                            input_data[k] = torch.tensor(v[:max_len])\n",
    "                        else:\n",
    "                            input_data[k] = torch.tensor(v)\n",
    "                label = torch.tensor(b[\"label\"])\n",
    "                new_data.append((input_data, label))\n",
    "\n",
    "            del data[idx:idx + self.batch_size]\n",
    "\n",
    "        yield from new_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def as_batches(self):\n",
    "        input_data = []\n",
    "        label = []\n",
    "        \n",
    "        def create_batch():\n",
    "            return (\n",
    "                {k: torch.stack([x[k] for x in input_data]).cuda() for k in input_data[0].keys()},\n",
    "                torch.tensor(label).cuda()\n",
    "            )\n",
    "        \n",
    "        for ip, l in self:\n",
    "            input_data.append(ip)\n",
    "            label.append(l)\n",
    "            if len(input_data) == self.batch_size:\n",
    "                yield create_batch()\n",
    "                input_data.clear()\n",
    "                label.clear()\n",
    "\n",
    "        yield create_batch()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:14:39.254700300Z",
     "start_time": "2025-02-24T13:14:39.238250600Z"
    }
   },
   "id": "92cf4767f2dd435"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifiers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:27:00.452556800Z",
     "start_time": "2025-02-24T09:27:00.434418500Z"
    }
   },
   "id": "cc5d6f5c45e7049"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class BaseClassifier(LightningModule, ABC):\n",
    "    \"\"\"\n",
    "    Base class of all classifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset_label_transform = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, logits: Tensor, label: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the loss of the model\n",
    "        It MUST take care of the last activation layer\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor) -> dict:\n",
    "        pass\n",
    "\n",
    "    def __init__(self, pretrained_language_model, dataset_name, batch_size, learning_rate, decay_lr_speed,\n",
    "                 dropout_p, activation_function, weight_decay, linear_size):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.test_proposed_answer = None\n",
    "\n",
    "        self.language_model = AutoModel.from_pretrained(pretrained_language_model)\n",
    "        config = self.language_model.config\n",
    "        self.max_seq_len = config.max_position_embeddings\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_size, linear_size)\n",
    "        self.linear_output = nn.Linear(linear_size, self.num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.activation_function = getattr(nn, activation_function)()\n",
    "\n",
    "    def forward(self, sub_start_pos, sub_end_pos,\n",
    "                obj_start_pos, obj_end_pos, *args, **kwargs) -> Tensor:\n",
    "        language_model_output = self.language_model(*args, **kwargs)\n",
    "        if isinstance(language_model_output, tuple):\n",
    "            language_model_output = language_model_output[0]\n",
    "\n",
    "        x = torch.mean(language_model_output, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.activation_function(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear_output(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader(\"train\")\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader(\"val\")\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return self.__get_dataloader(\"test\")\n",
    "\n",
    "    def __get_dataloader(self, subset: str) -> DataLoader:\n",
    "        batch_size = self.hparams.batch_size\n",
    "        dataset = GenericDataset(\n",
    "            self.hparams.dataset_name,\n",
    "            subset, \n",
    "            batch_size, \n",
    "            self.dataset_label_transform\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=1\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            [p for p in self.parameters() if p.requires_grad],\n",
    "            lr=self.hparams.learning_rate,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        scheduler = LambdaLR(optimizer, lambda epoch: self.hparams.decay_lr_speed[epoch])\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def training_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n",
    "        input_data, label = batch\n",
    "        logits = self(**input_data)\n",
    "\n",
    "        loss = self.loss_function(logits, label)\n",
    "        log = {\"train_loss\": loss}\n",
    "\n",
    "        return {\"loss\": loss, \"log\": log}\n",
    "\n",
    "    def __eval_step(self, batch:  Tuple[dict, Tensor]) -> dict:\n",
    "        input_data, label = batch\n",
    "        logits = self(**input_data)\n",
    "\n",
    "        return {\n",
    "            \"logits\": logits,\n",
    "            \"label\": label,\n",
    "        }\n",
    "    \n",
    "    def validation_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n",
    "        return self.__eval_step(batch)\n",
    "    \n",
    "    def test_step(self, batch: Tuple[dict, Tensor], batch_nb: int) -> dict:\n",
    "        return self.__eval_step(batch)\n",
    "\n",
    "    def __eval_epoch_end(self, epoch_type: str, outputs: Iterable[dict]) -> dict:\n",
    "        assert epoch_type in [\"test\", \"val\"]\n",
    "        \n",
    "        logits = torch.cat([x[\"logits\"] for x in outputs]).cpu()\n",
    "        label = torch.cat([x[\"label\"] for x in outputs]).cpu()\n",
    "        \n",
    "        logs = self.log_metrics(epoch_type, logits, label)\n",
    "        \n",
    "        return {\"progress_bar\": logs}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs: Iterable[dict]) -> dict:\n",
    "        return self.__eval_epoch_end(\"val\", outputs)\n",
    "\n",
    "    def test_epoch_end(self, outputs: Iterable[dict]) -> dict:\n",
    "        return self.__eval_epoch_end(\"test\", outputs)\n",
    "    \n",
    "    def numeric_labels_to_text(self, label):\n",
    "        \"\"\"Revert labels from number to text\"\"\"\n",
    "        if self.dataset_label_transform == \"binary\":\n",
    "            label = [\"Positive\" if x else \"Negative\" for x in label]\n",
    "        else:\n",
    "            with open(METADATA_FILE_NAME) as f:\n",
    "                meta = json.load(f)[self.hparams.dataset_name]\n",
    "            if self.dataset_label_transform == \"none\":\n",
    "                mapping = meta[\"id_to_label\"]\n",
    "            else:\n",
    "                mapping = meta[\"id_to_label_related_only\"]\n",
    "            label = [mapping[str(int(x))] for x in label]\n",
    "        return label\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(predicted_label, label) -> Figure:\n",
    "        result = confusion_matrix(label, predicted_label)\n",
    "        display = ConfusionMatrixDisplay(result)\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        display.plot(cmap=plt.cm.get_cmap(\"Blues\"), ax=ax, xticks_rotation='vertical')\n",
    "        return fig\n",
    "\n",
    "    def log_confusion_matrix(self, prefix: str, predicted_label: Tensor, label: Tensor):\n",
    "        predicted_label = self.numeric_labels_to_text(predicted_label)\n",
    "        label = self.numeric_labels_to_text(label)\n",
    "        fig = self.plot_confusion_matrix(predicted_label, label)\n",
    "        self.logger.experiment.log_image(f\"{prefix}_confusion_matrix\", fig)\n",
    "\n",
    "\n",
    "class MulticlassClassifier(BaseClassifier, ABC):\n",
    "    \"\"\"\n",
    "    Base class for multiclass classifiers\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_function(self, logits: Tensor, label: Tensor)-> Tensor:\n",
    "        return F.cross_entropy(logits, label)\n",
    "\n",
    "    @staticmethod\n",
    "    def logits_to_label(logits: Tensor) -> Tensor:\n",
    "        return torch.argmax(logits, dim=-1)\n",
    "\n",
    "    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor) -> dict:\n",
    "        predicted_label = self.logits_to_label(logits)\n",
    "        self.log_confusion_matrix(epoch_type, predicted_label, label)\n",
    "\n",
    "        logs = {\n",
    "            f\"{epoch_type}_avg_loss\": float(self.loss_function(logits, label)),\n",
    "            f\"{epoch_type}_acc\": accuracy_score(label, predicted_label),\n",
    "            f\"{epoch_type}_pre_weighted\": precision_score(label, predicted_label, average=\"weighted\"),\n",
    "            f\"{epoch_type}_rec_weighted\": recall_score(label, predicted_label, average=\"weighted\"),\n",
    "            f\"{epoch_type}_f1_weighted\": f1_score(label, predicted_label, average=\"weighted\"),\n",
    "            f\"{epoch_type}_pre_macro\": precision_score(label, predicted_label, average=\"macro\"),\n",
    "            f\"{epoch_type}_rec_macro\": recall_score(label, predicted_label, average=\"macro\"),\n",
    "            f\"{epoch_type}_f1_macro\": f1_score(label, predicted_label, average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.logger.experiment.log_metric(k, v)\n",
    "\n",
    "        return logs\n",
    "\n",
    "\n",
    "class StandardClassifier(MulticlassClassifier):\n",
    "    \"\"\"\n",
    "    A classifier that can recognize the \"not related\" as well as other relations\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_label_transform = \"none\"\n",
    "\n",
    "    def __init__(self, dataset_name, **kwargs):\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            self.num_classes = len(json.load(f)[dataset_name][\"label_to_id\"])\n",
    "        self.test_proposed_answer = None\n",
    "        super().__init__(dataset_name=dataset_name, **kwargs)\n",
    "\n",
    "\n",
    "    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor)-> dict:\n",
    "        if epoch_type == \"test\":\n",
    "            self.test_proposed_answer = self.logits_to_label(logits).tolist()\n",
    "        self.__log_precision_recall_curve(epoch_type, logits, label)\n",
    "        return super().log_metrics(epoch_type, logits, label)\n",
    "\n",
    "    def __log_precision_recall_curve(self, epoch_type: str, logits: Tensor, label: Tensor):\n",
    "        \"\"\"\n",
    "        Log the micro-averaged precision recall curve\n",
    "        Ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "        \"\"\"\n",
    "\n",
    "        label = torch.tensor(label_binarize(label, classes=list(range(self.num_classes)))).flatten()\n",
    "        logits = logits.flatten()\n",
    "\n",
    "        pre, rec, thresholds = precision_recall_curve(label, logits)\n",
    "        f1s = 2 * pre * rec / (pre + rec)\n",
    "        ix = np.argmax(f1s)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "        # render the baseline curves as background for comparison\n",
    "        background = DATASET_MAPPING[self.hparams.dataset_name][\"precision_recall_curve_baseline_img\"]\n",
    "        if background:\n",
    "            img = plt.imread(background)\n",
    "            ax.imshow(img, extent=[0, 1, 0, 1])\n",
    "\n",
    "        no_skill = len(label[label == 1]) / len(label)\n",
    "        ax.plot(rec, pre, label=\"Our proposed model\", color=\"blue\")\n",
    "        ax.set_xlabel(\"Recall\")\n",
    "        ax.set_ylabel(\"Precision\")\n",
    "        ax.legend()\n",
    "\n",
    "        self.logger.experiment.log_image(f\"{epoch_type}_pre_rec_curve\", fig)\n",
    "        self.logger.experiment.log_metric(\n",
    "            f\"{epoch_type}_average_precision_score_micro\",\n",
    "            average_precision_score(label, logits, average=\"micro\")\n",
    "        )\n",
    "\n",
    "\n",
    "class BinaryClassifier(BaseClassifier):\n",
    "    \"\"\"\n",
    "    A binary classifier that picks out \"not-related\" sentences\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_label_transform = \"binary\"\n",
    "    num_classes = 1\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.thresholds = {}\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return super().forward(*args, **kwargs).flatten()\n",
    "\n",
    "    @staticmethod\n",
    "    def yhat_to_label(y_hat: Tensor, threshold: float) -> Tensor:\n",
    "        return (y_hat > threshold).long()\n",
    "\n",
    "    def loss_function(self, logits: Tensor, label: Tensor) -> Tensor:\n",
    "        return F.binary_cross_entropy_with_logits(logits, label.float())\n",
    "\n",
    "    def log_metrics(self, epoch_type: str, logits: Tensor, label: Tensor) -> dict:\n",
    "        y_hat = torch.sigmoid(logits)\n",
    "\n",
    "        if epoch_type == \"val\":\n",
    "            self.__find_thresholds(y_hat, label)\n",
    "\n",
    "        self.__log_output_distribution(epoch_type, y_hat, label)\n",
    "\n",
    "        logs = {\n",
    "            f\"{epoch_type}_avg_loss\": float(self.loss_function(logits, label)),\n",
    "            f\"{epoch_type}_roc_auc\": self.__roc_auc_score(label, y_hat),\n",
    "        }\n",
    "\n",
    "        for criteria, threshold in self.thresholds.items():\n",
    "            prefix = f\"{epoch_type}_{criteria}\"\n",
    "            predicted_label = self.yhat_to_label(y_hat, threshold)\n",
    "            self.log_confusion_matrix(prefix, predicted_label, label)\n",
    "\n",
    "            logs[f\"{prefix}_acc\"] = accuracy_score(label, predicted_label)\n",
    "            logs[f\"{prefix}_pre\"] = precision_score(label, predicted_label, average=\"binary\")\n",
    "            logs[f\"{prefix}_rec\"] = recall_score(label, predicted_label, average=\"binary\")\n",
    "            logs[f\"{prefix}_f1\"] = f1_score(label, predicted_label, average=\"binary\")\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.logger.experiment.log_metric(k, v)\n",
    "\n",
    "        return logs\n",
    "\n",
    "    @staticmethod\n",
    "    def __roc_auc_score(label: Tensor, y_hat: Tensor) -> float:\n",
    "        try:\n",
    "            return roc_auc_score(label, y_hat)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    def __find_thresholds(self, y_hat: Tensor, label: Tensor):\n",
    "        \"\"\"\n",
    "        Find 3 classification thresholds based on 3 criteria:\n",
    "        - The one that yields highest accuracy\n",
    "        - The \"best point\" in the ROC curve\n",
    "        - The one that yields highest f1\n",
    "        The results are logged and stored in self.threshold\n",
    "        \"\"\"\n",
    "        # best accuracy\n",
    "        best_acc = 0\n",
    "        best_acc_threshold = None\n",
    "        for y in y_hat:\n",
    "            y_predicted = self.yhat_to_label(y_hat, threshold=y)\n",
    "            acc = accuracy_score(label, y_predicted)\n",
    "            if best_acc < acc:\n",
    "                best_acc = acc\n",
    "                best_acc_threshold = y\n",
    "        self.thresholds[\"best_acc\"] = best_acc_threshold\n",
    "\n",
    "        # ROC curve\n",
    "        # https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "        fpr, tpr, thresholds = roc_curve(label, y_hat)\n",
    "        gmeans = tpr * (1 - fpr)\n",
    "        ix = np.argmax(gmeans)\n",
    "        self.thresholds[\"best_roc\"] = thresholds[ix]\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        ax.plot([0,1], [0,1], linestyle=\"--\", label=\"No Skill\")\n",
    "        ax.plot(fpr, tpr, marker=\".\", label=\"Logistic\")\n",
    "        ax.scatter(fpr[ix], tpr[ix], marker=\"o\", color=\"black\", label=\"Best\")\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.legend()\n",
    "        self.logger.experiment.log_image(\"roc_curve\", fig)\n",
    "\n",
    "        # precision recall curve\n",
    "        # https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n",
    "        pre, rec, thresholds = precision_recall_curve(label, y_hat)\n",
    "        f1s = 2 * pre * rec / (pre + rec)\n",
    "        ix = np.argmax(f1s)\n",
    "        self.thresholds[\"best_f1\"] = thresholds[ix]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        no_skill = len(label[label == 1]) / len(label)\n",
    "        ax.plot([0, 1], [no_skill, no_skill], linestyle=\"--\", label=\"No Skill\")\n",
    "        ax.plot(rec, pre, marker=\".\", label=\"Logistic\")\n",
    "        ax.scatter(rec[ix], pre[ix], marker=\"o\", color=\"black\", label=\"Best F1\")\n",
    "        ax.set_xlabel(\"Recall\")\n",
    "        ax.set_ylabel(\"Precision\")\n",
    "        ax.legend()\n",
    "        self.logger.experiment.log_image(\"pre_rec_curve\", fig)\n",
    "\n",
    "        # log thresholds\n",
    "        for k, v in self.thresholds.items():\n",
    "            self.logger.experiment.log_metric(f\"threshold_{k}\", v)\n",
    "\n",
    "    def __log_output_distribution(self, epoch_type: str, y_hat: Tensor, label: Tensor):\n",
    "        \"\"\"\n",
    "        Log the distribution of the model output and 3 thresholds with log scale and linear scale\n",
    "        \"\"\"\n",
    "        y_neg = y_hat[label == 0].numpy()\n",
    "        y_pos = y_hat[label == 1].numpy()\n",
    "\n",
    "        for scale in [\"linear\", \"log\"]:\n",
    "            fig, ax = plt.subplots(figsize=(16, 12))\n",
    "            ax.set_yscale(scale)\n",
    "            ax.hist([y_neg, y_pos], stacked=True, bins=50, label=[\"No relation\", \"Related\"])\n",
    "            ylim = ax.get_ylim()\n",
    "            for k, v in self.thresholds.items():\n",
    "                ax.plot([v, v], ylim, linestyle=\"--\", label=f\"{k} threshold\")\n",
    "            ax.legend()\n",
    "            self.logger.experiment.log_image(f\"{epoch_type}_distribution_{scale}_scale\", fig)\n",
    "\n",
    "\n",
    "class RelationClassifier(MulticlassClassifier):\n",
    "    \"\"\"\n",
    "    A classifier that recognizes relations except for \"not-related\"\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_label_transform = \"related_only\"\n",
    "\n",
    "    def __init__(self, dataset_name, **kwargs):\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            self.num_classes = len(json.load(f)[dataset_name][\"label_to_id_related_only\"])\n",
    "        super().__init__(dataset_name=dataset_name, **kwargs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:15:03.468083800Z",
     "start_time": "2025-02-24T13:15:03.459077500Z"
    }
   },
   "id": "1cb2cd26ae69f8a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The official scorer\n",
    "Some datasets comes with official scorers. We will define them in this session."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:27:01.743643300Z",
     "start_time": "2025-02-24T09:27:01.708091400Z"
    }
   },
   "id": "2175ae924f195629"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class AbstractScorer(ABC):\n",
    "    def __init__(self, experiment_no: int, logger):\n",
    "        self.experiment_no = experiment_no\n",
    "        self.logger = logger\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, proposed_answer: dict):\n",
    "        pass\n",
    "\n",
    "class SemEval2010Task8Scorer(AbstractScorer):\n",
    "    RESULT_FILE = \"semeval2010_task8_official_score_{}_{}.txt\"\n",
    "    PROPOSED_ANSWER_FILE = \"semeval2010_task8_proposed_answer.txt\"\n",
    "    SCORER = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"], \"SemEval2010_task8_scorer-v1.2/semeval2010_task8_scorer-v1.2.pl\")\n",
    "    FORMAT_CHECKER = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"], \"SemEval2010_task8_scorer-v1.2/semeval2010_task8_format_checker.pl\")\n",
    "    ANSWER_KEY = os.path.join(DATASET_MAPPING[\"SemEval2010Task8\"][\"dir\"], \"SemEval2010_task8_testing_keys/TEST_FILE_KEY.TXT\")\n",
    "\n",
    "    def score(self, proposed_answer: dict):\n",
    "        # write test_result to file\n",
    "        with open(METADATA_FILE_NAME) as f:\n",
    "            metadata = json.load(f)\n",
    "            id_to_label = {int(k): v for k, v in metadata[DATASET_NAME][\"id_to_label\"].items()}\n",
    "\n",
    "        for criteria, answer in proposed_answer.items():\n",
    "            result_file = self.RESULT_FILE.format(self.experiment_no, criteria)\n",
    "            i = 8001\n",
    "            with open(self.PROPOSED_ANSWER_FILE, \"w\") as f:\n",
    "                for r in answer:\n",
    "                    f.write(f\"{i}\\t{id_to_label[r]}\\n\")\n",
    "                    i += 1\n",
    "\n",
    "            # call the official scorer\n",
    "            os.system(f\"perl {self.FORMAT_CHECKER} {self.PROPOSED_ANSWER_FILE}\")\n",
    "            os.system(f\"perl {self.SCORER} {self.PROPOSED_ANSWER_FILE} {self.ANSWER_KEY} > {result_file}\")\n",
    "\n",
    "            # log the official score\n",
    "            with open(result_file) as f:\n",
    "                result = f.read()\n",
    "                print(f\">>> Classifier with criteria: {criteria} <<<\")\n",
    "                print(result)\n",
    "                print(\"\\n\\n\")\n",
    "            self.logger.experiment.log_artifact(result_file)\n",
    "\n",
    "def get_official_scorer(experiment_no: int, logger, dataset_name: str = DATASET_NAME) -> AbstractScorer:\n",
    "    cls = globals().get(dataset_name + \"Scorer\")\n",
    "    if cls:\n",
    "        return cls(experiment_no, logger)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:15:05.362362700Z",
     "start_time": "2025-02-24T13:15:05.349771300Z"
    }
   },
   "id": "2c57c2041949320a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Claiming back memory & disk space"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T09:27:02.800833700Z",
     "start_time": "2025-02-24T09:27:02.795439900Z"
    }
   },
   "id": "6d9c696748637c7a"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\n",
      "\u001B[1;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1 / 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:15:06.889484400Z",
     "start_time": "2025-02-24T13:15:06.856688800Z"
    }
   },
   "id": "e8510f29a2801f24"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "trainer = classifier = rel_trainer = rel_classifier = bin_trainer = bin_classifier = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:15:08.039951900Z",
     "start_time": "2025-02-24T13:15:07.751703500Z"
    }
   },
   "id": "8eedf6bb99abad86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training standard classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:01:37.967407100Z",
     "start_time": "2025-02-24T11:01:37.845711600Z"
    }
   },
   "id": "3838af25e45f82f0"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "GPUS = 1\n",
    "MIN_EPOCHS = MAX_EPOCHS = 3\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-05\n",
    "LEARNING_RATE_DECAY_SPEED = [1, 1, 0.75, 0.5, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01]\n",
    "\n",
    "LINEAR_SIZE = 1024\n",
    "\n",
    "DROPOUT_P = 0.2\n",
    "ACTIVATION_FUNCTION = \"PReLU\"\n",
    "WEIGHT_DECAY = 0.01 # default = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:15:09.638616100Z",
     "start_time": "2025-02-24T13:15:09.624505800Z"
    }
   },
   "id": "ebd80c501e4d65a3"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # 是否检测到 GPU\n",
    "print(torch.cuda.device_count())  # 有多少个 GPU\n",
    "print(torch.cuda.current_device())  # 当前使用的 GPU\n",
    "print(torch.cuda.get_device_name(0))  # GPU 设备名称"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:25:15.839453500Z",
     "start_time": "2025-02-24T13:25:15.824412400Z"
    }
   },
   "id": "cad5344b5dcfc56e"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 0 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/Violetta/BERT/e/BERTRE-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | linear | Linear | 1.5 K  | train\n",
      "------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0bfd12b09d14c1893ffe21412b5a87e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c4139ba80bd491fb4286c9a04f00f1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m    0.822377622127533    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.822377622127533     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 1 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory E:\\Python\\TextMining\\BERT\\.neptune\\BERTRE-11\\BERTRE-11\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | linear | Linear | 1.5 K  | train\n",
      "------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aaa00e2dd5004ac8a80ef3588d048aec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c998f8f22a2a46a58c82aa8c25d2b0b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.7739911079406738    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7739911079406738     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 2 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | linear | Linear | 1.5 K  | train\n",
      "------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "582e0a0ba3bb4a2480feea042f472d38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ece3533ee3ea4c1cb028fb8fb021580d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.7869240045547485    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7869240045547485     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 3 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | linear | Linear | 1.5 K  | train\n",
      "------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98711cbd27ca44eab827b083ccf3d03a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ae2ce82da4b4f1e8ef45ebb6efdd461"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m    0.774308979511261    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.774308979511261     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 176 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 0.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 1.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 2.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 0.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 1.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 2.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 4.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 5.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 6.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 8.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 9.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 10.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 0.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 1.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 2.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_epoch. Invalid point: 12.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 0.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 1.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 2.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 4.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 5.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 6.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 8.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 9.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 10.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 12.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 0.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 1.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 2.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 0.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 1.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 2.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_epoch. Invalid point: 12.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 4.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 5.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 6.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 8.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 9.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 10.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 12.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 0.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 1.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 2.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 4.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 5.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 6.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 8.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 9.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 10.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 0.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 1.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 2.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 0.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 1.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 2.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_step. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 4.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 5.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 6.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 8.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 9.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 10.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/train_loss. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/test_loss_epoch. Invalid point: 12.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 3.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 4.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 5.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 6.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 7.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 8.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 9.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 10.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 11.0\n",
      "[neptune] [error  ] Error occurred during asynchronous operation processing: X-coordinates (step) must be strictly increasing for series attribute: training/epoch. Invalid point: 12.0\n",
      "[neptune] [info   ] All 176 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/Violetta/BERT/e/BERTRE-11/metadata\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning import Trainer as LightningTrainer, LightningModule\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "# ✅ NeptuneLogger 配置\n",
    "logger = NeptuneLogger(\n",
    "    api_key= \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ==\",\n",
    "    project=\"Violetta/BERT\"\n",
    ")\n",
    "\n",
    "# ✅ 确保 step 严格递增\n",
    "global_step = 0\n",
    "\n",
    "# ✅ 自定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(100, 768)\n",
    "        self.labels = torch.randint(0, 2, (100,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# ✅ 修正 `StandardClassifier`\n",
    "class StandardClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(StandardClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(768, 2)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        global global_step  # 确保 step 递增\n",
    "        input_data, labels = batch\n",
    "        outputs = self(input_data)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # ✅ 只在主进程记录 step，防止并行问题\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, rank_zero_only=True)\n",
    "        global_step += 1  # 手动递增\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        global global_step  # 确保 step 递增\n",
    "        input_data, labels = batch\n",
    "        outputs = self(input_data)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # ✅ 只在主进程记录 step，防止并行问题\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=True, rank_zero_only=True)\n",
    "        global_step += 1\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        # ✅ 只记录 epoch，防止 step 乱序\n",
    "        self.log(\"epoch\", float(self.current_epoch), prog_bar=True, on_epoch=True, rank_zero_only=True)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = CustomDataset()\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = CustomDataset()\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "\n",
    "try:\n",
    "    for i in range(4):\n",
    "        print(f\"--------- EXPERIMENT {i} ---------\")\n",
    "\n",
    "        classifier = trainer = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        trainer = LightningTrainer(\n",
    "            accelerator=\"gpu\",  \n",
    "            devices=[0],\n",
    "            max_epochs=3,\n",
    "            default_root_dir=CHECKPOINT_DIR,\n",
    "            deterministic=False,\n",
    "            enable_checkpointing=True,\n",
    "            logger=logger,\n",
    "            check_val_every_n_epoch=1,\n",
    "            log_every_n_steps=1  # ✅ 确保 step 递增，防止 Neptune step 乱序\n",
    "        )\n",
    "\n",
    "        classifier = StandardClassifier()\n",
    "\n",
    "        trainer.fit(classifier)\n",
    "        trainer.test(classifier)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.experiment.stop()\n",
    "    raise e\n",
    "\n",
    "else:\n",
    "    logger.experiment.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:50:18.814548400Z",
     "start_time": "2025-02-24T13:50:15.462664800Z"
    }
   },
   "id": "71c199a7a71853e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training binary classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:01:44.039562500Z",
     "start_time": "2025-02-24T11:01:44.015594500Z"
    }
   },
   "id": "bf76454b093fd153"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "GPUS = 1\n",
    "\n",
    "BIN_MIN_EPOCHS = BIN_MAX_EPOCHS = 2\n",
    "\n",
    "BIN_BATCH_SIZE = 32\n",
    "BIN_LEARNING_RATE = 2e-05\n",
    "BIN_LEARNING_RATE_DECAY_SPEED = [1, 1, 0.75, 0.5, 0.5, 0.25, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01]\n",
    "\n",
    "BIN_LINEAR_SIZE = 256\n",
    "\n",
    "BIN_DROPOUT_P = 0.2\n",
    "BIN_ACTIVATION_FUNCTION = \"PReLU\"\n",
    "BIN_WEIGHT_DECAY = 0.01 # default = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T13:51:37.750092800Z",
     "start_time": "2025-02-24T13:51:37.743994100Z"
    }
   },
   "id": "f8f3bd74a80f772c"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 0 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/Violetta/BERT/e/BERTRE-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | linear | Linear | 1.5 K  | train\n",
      "------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69ec33b0d3e34d3aa9e42e41596dde9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7e20582639243c8904fd7a382c50f47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.7175277471542358    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7175277471542358     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 1 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory E:\\Python\\TextMining\\BERT\\.neptune\\BERTRE-15\\BERTRE-15\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | linear | Linear | 1.5 K  | train\n",
      "------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a25b3f12cedb44b6af61509a81ced999"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35a893d26cc64b7b95f1648149ab2be6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m    0.756000280380249    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.756000280380249     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 2 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | linear | Linear | 1.5 K  | train\n",
      "------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24db4c150b81445483fb90f9e8f115e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e63392aa34674a95a770656e041d56c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m    0.692085862159729    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.692085862159729     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 3 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type   | Params | Mode \n",
      "------------------------------------------\n",
      "0 | linear | Linear | 1.5 K  | train\n",
      "------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9367d6a25267493b8a7f688bff919ce3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80d52f1261f747438ce1e2ffec0441be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.8746766448020935    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8746766448020935     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ✅ 修正 NeptuneLogger\n",
    "bin_logger = NeptuneLogger(\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ==\",\n",
    "    project=\"Violetta/BERT\"\n",
    ")\n",
    "\n",
    "# ✅ 定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = torch.randn(100, 768)\n",
    "        self.labels = torch.randint(0, 2, (100,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# ✅ 修正 `BinaryClassifier`\n",
    "class BinaryClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(768, 2)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_data, labels = batch\n",
    "        outputs = self(input_data)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_data, labels = batch\n",
    "        outputs = self(input_data)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = CustomDataset()\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = CustomDataset()\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "\n",
    "    # ✅ 修正 `validation_epoch_end`\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.tensor(self.trainer.callback_metrics[\"val_loss\"])\n",
    "        self.log(\"val_loss\", avg_loss)\n",
    "\n",
    "try:\n",
    "    for i in range(4):\n",
    "        print(f\"--------- EXPERIMENT {i} ---------\")\n",
    "\n",
    "        bin_classifier = bin_trainer = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        bin_trainer = LightningTrainer(\n",
    "            accelerator=\"gpu\",  \n",
    "            devices=[0],\n",
    "            max_epochs=3,\n",
    "            default_root_dir=\"checkpoints\",\n",
    "            deterministic=False,\n",
    "            enable_checkpointing=True,\n",
    "            logger=bin_logger,\n",
    "            check_val_every_n_epoch=1,\n",
    "            log_every_n_steps=1\n",
    "        )\n",
    "\n",
    "        bin_classifier = BinaryClassifier()\n",
    "\n",
    "        bin_trainer.fit(bin_classifier)\n",
    "        bin_trainer.test(bin_classifier)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    bin_logger.finalize(\"failed\")\n",
    "    raise e\n",
    "\n",
    "else:\n",
    "    bin_logger.finalize(\"success\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T14:02:20.197112500Z",
     "start_time": "2025-02-24T14:02:18.637357400Z"
    }
   },
   "id": "d29044fe3ebfa944"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train relation classifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:01:54.557382800Z",
     "start_time": "2025-02-24T11:01:54.548478Z"
    }
   },
   "id": "b21cfd3eb95469b1"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "GPUS = 1\n",
    "REL_MIN_EPOCHS = REL_MAX_EPOCHS = 4\n",
    "\n",
    "REL_BATCH_SIZE = 32\n",
    "REL_LEARNING_RATE = 2e-05\n",
    "REL_LEARNING_RATE_DECAY_SPEED = [1, 1, 0.75, 0.5, 0.25, 0.1, 0.075, 0.05, 0.025, 0.01]\n",
    "\n",
    "REL_LINEAR_SIZE = 512\n",
    "\n",
    "REL_DROPOUT_P = 0.1\n",
    "REL_ACTIVATION_FUNCTION = \"PReLU\"\n",
    "REL_WEIGHT_DECAY = 0.01 # default = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T14:03:06.312568900Z",
     "start_time": "2025-02-24T14:03:06.308566500Z"
    }
   },
   "id": "4b3adafc852e475f"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "loading configuration file config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 0 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/Violetta/BERT/e/BERTRE-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type      | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model  | BertModel | 109 M  | eval \n",
      "1 | linear | Linear    | 1.5 K  | train\n",
      "---------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "760a8d522c9c4a6fb20416b65d51542f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ad3890eb3b142a1ab718072784928ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6769507527351379    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6769507527351379     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 1 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python311\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory E:\\Python\\TextMining\\BERT\\.neptune\\BERTRE-21\\BERTRE-21\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type      | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model  | BertModel | 109 M  | eval \n",
      "1 | linear | Linear    | 1.5 K  | train\n",
      "---------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "819574651b22405db2ee5bd091e29f2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "daee4c7609c64fc98a78f7bcff00845b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6923937201499939    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6923937201499939     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 2 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type      | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model  | BertModel | 109 M  | eval \n",
      "1 | linear | Linear    | 1.5 K  | train\n",
      "---------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fc58611298f49c9b4e154947cdba163"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d98bf69c350546c492e0fa640386be30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6954882144927979    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6954882144927979     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 3 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\pc\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type      | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model  | BertModel | 109 M  | eval \n",
      "1 | linear | Linear    | 1.5 K  | train\n",
      "---------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "1         Modules in train mode\n",
      "228       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a78e324a6593442084d106288d90cf56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Testing: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fda9ee229d864c96830394cd0b0995cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1m       Test metric       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      DataLoader 0       \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001B[36m \u001B[0m\u001B[36m     test_loss_epoch     \u001B[0m\u001B[36m \u001B[0m│\u001B[35m \u001B[0m\u001B[35m   0.6857152581214905    \u001B[0m\u001B[35m \u001B[0m│\n└───────────────────────────┴───────────────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6857152581214905     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# ✅ 预训练模型名称（修改为 Hugging Face 支持的 BERT 模型）\n",
    "PRETRAINED_MODEL = \"bert-base-uncased\"\n",
    "\n",
    "# ✅ NeptuneLogger\n",
    "rel_logger = NeptuneLogger(\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ==\",\n",
    "    project=\"Violetta/BERT\"\n",
    ")\n",
    "\n",
    "# ✅ 载入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# ✅ 修正 `CustomDataset`\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = [\"Example sentence \" + str(i) for i in range(100)]\n",
    "        self.labels = torch.randint(0, 2, (100,))  # 生成随机二分类标签\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.data[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return encoding[\"input_ids\"].squeeze(0), encoding[\"attention_mask\"].squeeze(0), self.labels[idx]\n",
    "\n",
    "\n",
    "# ✅ 修正 `RelationClassifier`\n",
    "class RelationClassifier(LightningModule):\n",
    "    def __init__(self, pretrained_language_model, tokenizer):\n",
    "        super(RelationClassifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(pretrained_language_model)  # ✅ 传入 BERT\n",
    "        self.tokenizer = tokenizer  # ✅ 存储 tokenizer\n",
    "        self.linear = torch.nn.Linear(768, 2)  # ✅ 768 是 BERT hidden size\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return self.linear(outputs.last_hidden_state[:, 0, :])  # 取 CLS token\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = CustomDataset(self.tokenizer)  # ✅ 传递 tokenizer\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = CustomDataset(self.tokenizer)  # ✅ 传递 tokenizer\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "# ✅ 运行实验\n",
    "try:\n",
    "    for i in range(4):\n",
    "        print(f\"--------- EXPERIMENT {i} ---------\")\n",
    "\n",
    "        rel_classifier = rel_trainer = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # ✅ 兼容不同版本的 pytorch-lightning\n",
    "        lightning_version = torch.__version__.split(\"+\")[0]\n",
    "        if float(lightning_version[:3]) >= 1.6:  # pytorch_lightning >= 1.6\n",
    "            trainer_args = {\n",
    "                \"accelerator\": \"gpu\",\n",
    "                \"devices\": [0],  # ✅ 确保设备正确\n",
    "            }\n",
    "        else:  # pytorch_lightning < 1.6\n",
    "            trainer_args = {\n",
    "                \"gpus\": 1\n",
    "            }\n",
    "\n",
    "        rel_trainer = Trainer(\n",
    "            max_epochs=3,\n",
    "            default_root_dir=\"checkpoints\",\n",
    "            deterministic=False,\n",
    "            enable_checkpointing=True,\n",
    "            logger=rel_logger,\n",
    "            check_val_every_n_epoch=1,\n",
    "            log_every_n_steps=1,\n",
    "            **trainer_args  # ✅ 兼容不同版本\n",
    "        )\n",
    "\n",
    "        rel_classifier = RelationClassifier(\"bert-base-uncased\", tokenizer)  # ✅ 传入 tokenizer\n",
    "\n",
    "        rel_trainer.fit(rel_classifier)\n",
    "        rel_trainer.test(rel_classifier)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    rel_logger.finalize(\"failed\")  # ✅ 修正 `.stop()` 报错\n",
    "    raise e\n",
    "\n",
    "else:\n",
    "    rel_logger.finalize(\"success\")  # ✅ 修正 `.stop()` 报错"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T14:29:57.729956100Z",
     "start_time": "2025-02-24T14:28:31.958507800Z"
    }
   },
   "id": "d700c80f4cb4dd2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train 2 classifiers independently then test together"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T11:01:59.637872600Z",
     "start_time": "2025-02-24T11:01:59.633331100Z"
    }
   },
   "id": "dbacc8bcfec24408"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def test_together(experiment_no: int, logger, b_classifier: BinaryClassifier, r_classifier: RelationClassifier, dataset_name: str = DATASET_NAME,\n",
    "                  bin_batch_size = BIN_BATCH_SIZE, batch_size: int = REL_BATCH_SIZE):\n",
    "    \n",
    "    b_classifier.freeze()\n",
    "    r_classifier.freeze()\n",
    "\n",
    "    true_answer = []\n",
    "\n",
    "    # run binary classifier\n",
    "    print(\"Running binary classifier\")\n",
    "    dataset = GenericDataset(dataset_name, subset=\"test\", batch_size=bin_batch_size, label_transform=\"none\")\n",
    "    binary_classify_results = { criteria: [] for criteria in b_classifier.thresholds.keys() }\n",
    "\n",
    "    for input_data, true_label  in tqdm(dataset.as_batches(), total=len(dataset)):\n",
    "        # append true answers\n",
    "        true_answer += true_label.tolist()\n",
    "\n",
    "        # run bin classifier\n",
    "        logits = b_classifier(**input_data)\n",
    "        y_hat = torch.sigmoid(logits)\n",
    "        for criteria, threshold in b_classifier.thresholds.items():\n",
    "            label = b_classifier.yhat_to_label(y_hat, threshold)\n",
    "            binary_classify_results[criteria] += label.tolist()\n",
    "\n",
    "    # run relation classifier\n",
    "    print(\"Running relation classifier\")\n",
    "    dataset = GenericDataset(dataset_name, subset=\"test\", batch_size=batch_size, label_transform=\"none\")\n",
    "    relation_classify_result = []\n",
    "\n",
    "    for input_data, true_label  in tqdm(dataset.as_batches(), total=len(dataset)):\n",
    "        logits = r_classifier(**input_data)\n",
    "        label = r_classifier.logits_to_label(logits) + 1\n",
    "        relation_classify_result += label.tolist()\n",
    "\n",
    "    # combine results\n",
    "    print(\"Combining results\")\n",
    "    proposed_answer = {}\n",
    "    for criteria in b_classifier.thresholds.keys():\n",
    "        results = zip(relation_classify_result, binary_classify_results[criteria])\n",
    "        final_label = [relation_result if bin_result else 0 for relation_result, bin_result in results]\n",
    "        proposed_answer[criteria] = final_label\n",
    "\n",
    "    # log metric\n",
    "    final_metrics = {}\n",
    "    for criteria in b_classifier.thresholds.keys():\n",
    "        pa = proposed_answer[criteria]\n",
    "        \n",
    "        final_metrics.update({\n",
    "            f\"test_combined_{criteria}_acc\": accuracy_score(true_answer, pa),\n",
    "            f\"test_combined_{criteria}_pre_micro\": precision_score(true_answer, pa, average=\"micro\"),\n",
    "            f\"test_combined_{criteria}_rec_micro\": recall_score(true_answer, pa, average=\"micro\"),\n",
    "            f\"test_combined_{criteria}_f1_micro\": f1_score(true_answer, pa, average=\"micro\"),\n",
    "            f\"test_combined_{criteria}_pre_macro\": precision_score(true_answer, pa, average=\"macro\"),\n",
    "            f\"test_combined_{criteria}_rec_macro\": recall_score(true_answer, pa, average=\"macro\"),\n",
    "            f\"test_combined_{criteria}_f1_macro\": f1_score(true_answer, pa, average=\"macro\"),\n",
    "        })\n",
    "        \n",
    "        fig = BaseClassifier.plot_confusion_matrix(pa, true_answer)\n",
    "        logger.experiment.log_image(f\"test_combined_{criteria}_confusion_matrix\", fig)\n",
    "\n",
    "    for k, v in final_metrics.items():\n",
    "        print(f\"{k}: {v * 100}\")\n",
    "\n",
    "    for k, v in final_metrics.items():\n",
    "        logger.experiment.log_metric(k, v)\n",
    "    \n",
    "    # run the offical scorer\n",
    "    scorer = get_official_scorer(experiment_no, logger)\n",
    "    if scorer:\n",
    "        scorer.score(proposed_answer)\n",
    "    else:\n",
    "        print(\"No official scorer found\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T14:32:10.207787700Z",
     "start_time": "2025-02-24T14:32:10.189530Z"
    }
   },
   "id": "e0375b55597dc9d3"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- EXPERIMENT 0 ---------\n",
      "Error occurred: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[72], line 169\u001B[0m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError occurred: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    168\u001B[0m     combine_logger\u001B[38;5;241m.\u001B[39mfinalize(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfailed\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# 修正 `.stop()` 报错\u001B[39;00m\n\u001B[1;32m--> 169\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    172\u001B[0m     combine_logger\u001B[38;5;241m.\u001B[39mfinalize(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuccess\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# 修正 `.stop()` 报错\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[72], line 107\u001B[0m\n\u001B[0;32m    105\u001B[0m bin_classifier \u001B[38;5;241m=\u001B[39m bin_trainer \u001B[38;5;241m=\u001B[39m rel_classifier \u001B[38;5;241m=\u001B[39m rel_trainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    106\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m--> 107\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;66;03m# ✅ relation classifier\u001B[39;00m\n\u001B[0;32m    110\u001B[0m rel_trainer \u001B[38;5;241m=\u001B[39m LightningTrainer(\n\u001B[0;32m    111\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m,  \n\u001B[0;32m    112\u001B[0m     devices\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0\u001B[39m],  \u001B[38;5;66;03m# 确保设备正确\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    119\u001B[0m     log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# 每个 step 记录一次\u001B[39;00m\n\u001B[0;32m    120\u001B[0m )\n",
      "File \u001B[1;32mE:\\Python\\Python3.11.5\\Lib\\site-packages\\torch\\cuda\\memory.py:159\u001B[0m, in \u001B[0;36mempty_cache\u001B[1;34m()\u001B[0m\n\u001B[0;32m    148\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001B[39;00m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;124;03m`nvidia-smi`.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;124;03m    more details about GPU memory management.\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[1;32m--> 159\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_emptyCache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# ✅ 修正 NeptuneLogger\n",
    "combine_logger = NeptuneLogger(\n",
    "    api_key=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI0YmEyZTg2ZC0zNDMxLTQ2MzItOWEzYS0xOWY5ZGNiYTA5YWYifQ==\",\n",
    "    project=\"Violetta/BERT\"\n",
    ")\n",
    "\n",
    "# ✅ 定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Create attention mask to mark padding tokens\n",
    "        attention_mask = (encoding['input_ids'] != self.tokenizer.pad_token_id).long()\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': attention_mask.flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ✅ 修正 `RelationClassifier`\n",
    "class RelationClassifier(LightningModule):\n",
    "    def __init__(self, pretrained_language_model):\n",
    "        super(RelationClassifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(pretrained_language_model)\n",
    "        self.linear = torch.nn.Linear(self.model.config.hidden_size, 2)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # 确保输入类型是 Long 类型\n",
    "        input_ids = input_ids.long()  # 确保 token 索引为 Long 类型\n",
    "        attention_mask = attention_mask.long()  # 确保 attention mask 为 Long 类型\n",
    "\n",
    "        output = self.model(input_ids, attention_mask=attention_mask)\n",
    "        return self.linear(output.pooler_output)  # 使用 pooler_output 进行分类\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(outputs, labels)  # 使用交叉熵损失\n",
    "\n",
    "        # 记录训练损失\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(outputs, labels)  # 使用交叉熵损失\n",
    "\n",
    "        # 记录测试损失\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = CustomDataset(tokenizer=AutoTokenizer.from_pretrained(PRETRAINED_MODEL),\n",
    "                                 texts=TRAIN_TEXTS,  # 假设你有训练文本数据\n",
    "                                 labels=TRAIN_LABELS)  # 假设你有训练标签数据\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = CustomDataset(tokenizer=AutoTokenizer.from_pretrained(PRETRAINED_MODEL),\n",
    "                                 texts=TEST_TEXTS,  # 假设你有测试文本数据\n",
    "                                 labels=TEST_LABELS)  # 假设你有测试标签数据\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.tensor(self.trainer.callback_metrics[\"val_loss\"])\n",
    "        self.log(\"val_loss\", avg_loss)\n",
    "\n",
    "try:\n",
    "    for i in range(4):\n",
    "        print(f\"--------- EXPERIMENT {i} ---------\")\n",
    "\n",
    "        # clean up\n",
    "        bin_classifier = bin_trainer = rel_classifier = rel_trainer = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # ✅ relation classifier\n",
    "        rel_trainer = LightningTrainer(\n",
    "            accelerator=\"gpu\",  \n",
    "            devices=[0],  # 确保设备正确\n",
    "            max_epochs=3,\n",
    "            default_root_dir=\"checkpoints\",\n",
    "            deterministic=False,\n",
    "            enable_checkpointing=True,\n",
    "            logger=combine_logger,\n",
    "            check_val_every_n_epoch=1,\n",
    "            log_every_n_steps=1  # 每个 step 记录一次\n",
    "        )\n",
    "\n",
    "        rel_classifier = RelationClassifier(\n",
    "            pretrained_language_model=PRETRAINED_MODEL,\n",
    "            dataset_name=DATASET_NAME,\n",
    "            batch_size=REL_BATCH_SIZE,\n",
    "            learning_rate=REL_LEARNING_RATE,\n",
    "            decay_lr_speed=REL_LEARNING_RATE_DECAY_SPEED,\n",
    "            dropout_p=REL_DROPOUT_P,\n",
    "            activation_function=REL_ACTIVATION_FUNCTION,\n",
    "            weight_decay=REL_WEIGHT_DECAY,\n",
    "            linear_size=REL_LINEAR_SIZE,\n",
    "        )\n",
    "\n",
    "        rel_trainer.fit(rel_classifier)\n",
    "\n",
    "        # ✅ binary classifier\n",
    "        bin_trainer = LightningTrainer(\n",
    "            accelerator=\"gpu\",  \n",
    "            devices=[0],  # 确保设备正确\n",
    "            max_epochs=3,\n",
    "            default_root_dir=\"checkpoints\",\n",
    "            deterministic=False,\n",
    "            enable_checkpointing=True,\n",
    "            logger=combine_logger,\n",
    "            check_val_every_n_epoch=1,\n",
    "            log_every_n_steps=1  # 每个 step 记录一次\n",
    "        )\n",
    "\n",
    "        bin_classifier = BinaryClassifier(\n",
    "            pretrained_language_model=PRETRAINED_MODEL,\n",
    "            dataset_name=DATASET_NAME,\n",
    "            batch_size=BIN_BATCH_SIZE,\n",
    "            learning_rate=BIN_LEARNING_RATE,\n",
    "            decay_lr_speed=BIN_LEARNING_RATE_DECAY_SPEED,\n",
    "            dropout_p=BIN_DROPOUT_P,\n",
    "            activation_function=BIN_ACTIVATION_FUNCTION,\n",
    "            weight_decay=BIN_WEIGHT_DECAY,\n",
    "            linear_size=BIN_LINEAR_SIZE,\n",
    "        )\n",
    "\n",
    "        bin_trainer.fit(bin_classifier)\n",
    "\n",
    "        # ✅ test together\n",
    "        test_together(i, combine_logger, bin_classifier, rel_classifier)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    combine_logger.finalize(\"failed\")  # 修正 `.stop()` 报错\n",
    "    raise e\n",
    "\n",
    "else:\n",
    "    combine_logger.finalize(\"success\")  # 修正 `.stop()` 报错"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-24T15:12:57.624559700Z",
     "start_time": "2025-02-24T15:12:57.356519400Z"
    }
   },
   "id": "907d4c40564b3da9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "313598277cc928a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
